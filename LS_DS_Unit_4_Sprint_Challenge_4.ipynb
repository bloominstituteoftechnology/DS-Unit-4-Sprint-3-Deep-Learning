{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trista-paul/DS-Unit-4-Sprint-4-Deep-Learning/blob/master/LS_DS_Unit_4_Sprint_Challenge_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvrgvIz_UXq4",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 4*\n",
        "\n",
        "# Sprint Challenge\n",
        "### RNNs, CNNs, GANS, and AutoML\n",
        "\n",
        "In this Sprint Challenge, you'll explore some of the cutting edge of Data Science. *Caution* - these approaches can be pretty heavy computationally. All problems are designed to completed with 5-10 minutes of run time on most machines. If you approach takes longer, please double check your work. \n",
        "\n",
        "## Part 1 - RNNs\n",
        "\n",
        "Use an RNN to fit a classification model on tweets to distinguish from tweets from any two accounts. The following code sample illustrates how to access data from an account (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayL4i7l6UXq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "93ff0b84-8bf9-421d-a2ae-244d1bd8eeca"
      },
      "source": [
        "!pip install twitterscraper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: twitterscraper in /usr/local/lib/python3.6/dist-packages (0.9.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (4.2.6)\n",
            "Requirement already satisfied: coala-utils~=0.5.0 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.5.1)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (2.21.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->twitterscraper) (4.6.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2019.3.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6v1Fd-tUXrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1078
        },
        "outputId": "782c4868-34e3-4b02-d954-2181d22fa46f"
      },
      "source": [
        "from twitterscraper import query_tweets\n",
        "\n",
        "austen_tweets = query_tweets('from:austen',1000)\n",
        "len(austen_tweets)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-16', 'from:austen since:2006-11-16 until:2007-07-14', 'from:austen since:2007-07-14 until:2008-03-10', 'from:austen since:2008-03-10 until:2008-11-06', 'from:austen since:2008-11-06 until:2009-07-04', 'from:austen since:2009-07-04 until:2010-03-01', 'from:austen since:2010-03-01 until:2010-10-27', 'from:austen since:2010-10-27 until:2011-06-25', 'from:austen since:2011-06-25 until:2012-02-20', 'from:austen since:2012-02-20 until:2012-10-17', 'from:austen since:2012-10-17 until:2013-06-14', 'from:austen since:2013-06-14 until:2014-02-10', 'from:austen since:2014-02-10 until:2014-10-08', 'from:austen since:2014-10-08 until:2015-06-05', 'from:austen since:2015-06-05 until:2016-01-31', 'from:austen since:2016-01-31 until:2016-09-28', 'from:austen since:2016-09-28 until:2017-05-26', 'from:austen since:2017-05-26 until:2018-01-21', 'from:austen since:2018-01-21 until:2018-09-18', 'from:austen since:2018-09-18 until:2019-05-17']\n",
            "INFO: Querying from:austen since:2006-11-16 until:2007-07-14\n",
            "INFO: Querying from:austen since:2006-03-21 until:2006-11-16\n",
            "INFO: Querying from:austen since:2007-07-14 until:2008-03-10\n",
            "INFO: Querying from:austen since:2014-02-10 until:2014-10-08\n",
            "INFO: Querying from:austen since:2012-02-20 until:2012-10-17\n",
            "INFO: Querying from:austen since:2011-06-25 until:2012-02-20\n",
            "INFO: Querying from:austen since:2010-10-27 until:2011-06-25\n",
            "INFO: Querying from:austen since:2018-01-21 until:2018-09-18\n",
            "INFO: Querying from:austen since:2016-09-28 until:2017-05-26\n",
            "INFO: Querying from:austen since:2008-11-06 until:2009-07-04\n",
            "INFO: Querying from:austen since:2016-01-31 until:2016-09-28\n",
            "INFO: Querying from:austen since:2015-06-05 until:2016-01-31\n",
            "INFO: Querying from:austen since:2012-10-17 until:2013-06-14\n",
            "INFO: Querying from:austen since:2008-03-10 until:2008-11-06\n",
            "INFO: Querying from:austen since:2017-05-26 until:2018-01-21\n",
            "INFO: Querying from:austen since:2009-07-04 until:2010-03-01\n",
            "INFO: Querying from:austen since:2013-06-14 until:2014-02-10\n",
            "INFO: Querying from:austen since:2010-03-01 until:2010-10-27\n",
            "INFO: Querying from:austen since:2018-09-18 until:2019-05-17\n",
            "INFO: Querying from:austen since:2014-10-08 until:2015-06-05\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-03-21%20until%3A2006-11-16.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2007-07-14%20until%3A2008-03-10.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-11-16%20until%3A2007-07-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2015-06-05%20until%3A2016-01-31.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-01-31%20until%3A2016-09-28.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-03-01%20until%3A2010-10-27.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2009-07-04%20until%3A2010-03-01.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-02-10%20until%3A2014-10-08.\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2012-10-17%20until%3A2013-06-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2013-06-14%20until%3A2014-02-10.\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-09-28%20until%3A2017-05-26.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2011-06-25%20until%3A2012-02-20.\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-10-27%20until%3A2011-06-25.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-11-06%20until%3A2009-07-04.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-10-08%20until%3A2015-06-05.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-03-10%20until%3A2008-11-06.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 1 tweets for from%3Aausten%20since%3A2012-02-20%20until%3A2012-10-17.\n",
            "INFO: Got 1 tweets (1 new).\n",
            "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-05-26%20until%3A2018-01-21.\n",
            "INFO: Got 61 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-09-18%20until%3A2019-05-17.\n",
            "INFO: Got 121 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-01-21%20until%3A2018-09-18.\n",
            "INFO: Got 181 tweets (60 new).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttHuWLxBUXrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77cb19c3-d2da-4191-b943-f859f44b7908"
      },
      "source": [
        "austen_tweets[0].text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8K2_ZKfUXrP",
        "colab_type": "text"
      },
      "source": [
        "Your Tasks:\n",
        "* Select two twitter accounts to gather data from\n",
        "* Use twitterscraper to get ~1,000 tweets from each account\n",
        "* Encode the characters to a sequence of integers for the model\n",
        "* Get the data into the appropriate shape/format, including labels and a train/test split\n",
        "* Use Keras to fit a predictive model, classying tweets as being from one acount or the other\n",
        "* Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well as the RNN code we used in class.\n",
        "\n",
        "Note - focus on getting a running model, not on making accuracy with extreme data size or epoch numbers. Fit a baseline model based on tweet text. Only revisit and push accuracy or incorporate additional features if you get everything else done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "503AV-57U6r6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2086
        },
        "outputId": "1dc824c3-57e9-45ca-c193-f1826827cb10"
      },
      "source": [
        "tommy_tweets = query_tweets('from:tommycollison',1000)\n",
        "\n",
        "patrick_tweets = query_tweets('from:patrickc', 1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: queries: ['from:tommycollison since:2006-03-21 until:2006-11-16', 'from:tommycollison since:2006-11-16 until:2007-07-14', 'from:tommycollison since:2007-07-14 until:2008-03-10', 'from:tommycollison since:2008-03-10 until:2008-11-06', 'from:tommycollison since:2008-11-06 until:2009-07-04', 'from:tommycollison since:2009-07-04 until:2010-03-01', 'from:tommycollison since:2010-03-01 until:2010-10-27', 'from:tommycollison since:2010-10-27 until:2011-06-25', 'from:tommycollison since:2011-06-25 until:2012-02-20', 'from:tommycollison since:2012-02-20 until:2012-10-17', 'from:tommycollison since:2012-10-17 until:2013-06-14', 'from:tommycollison since:2013-06-14 until:2014-02-10', 'from:tommycollison since:2014-02-10 until:2014-10-08', 'from:tommycollison since:2014-10-08 until:2015-06-05', 'from:tommycollison since:2015-06-05 until:2016-01-31', 'from:tommycollison since:2016-01-31 until:2016-09-28', 'from:tommycollison since:2016-09-28 until:2017-05-26', 'from:tommycollison since:2017-05-26 until:2018-01-21', 'from:tommycollison since:2018-01-21 until:2018-09-18', 'from:tommycollison since:2018-09-18 until:2019-05-17']\n",
            "INFO: Querying from:tommycollison since:2006-03-21 until:2006-11-16\n",
            "INFO: Querying from:tommycollison since:2006-11-16 until:2007-07-14\n",
            "INFO: Querying from:tommycollison since:2011-06-25 until:2012-02-20\n",
            "INFO: Querying from:tommycollison since:2018-01-21 until:2018-09-18\n",
            "INFO: Querying from:tommycollison since:2016-09-28 until:2017-05-26\n",
            "INFO: Querying from:tommycollison since:2012-02-20 until:2012-10-17\n",
            "INFO: Querying from:tommycollison since:2016-01-31 until:2016-09-28\n",
            "INFO: Querying from:tommycollison since:2015-06-05 until:2016-01-31\n",
            "INFO: Querying from:tommycollison since:2018-09-18 until:2019-05-17\n",
            "INFO: Querying from:tommycollison since:2010-10-27 until:2011-06-25\n",
            "INFO: Querying from:tommycollison since:2012-10-17 until:2013-06-14\n",
            "INFO: Querying from:tommycollison since:2014-02-10 until:2014-10-08\n",
            "INFO: Querying from:tommycollison since:2009-07-04 until:2010-03-01\n",
            "INFO: Querying from:tommycollison since:2013-06-14 until:2014-02-10\n",
            "INFO: Querying from:tommycollison since:2008-03-10 until:2008-11-06\n",
            "INFO: Querying from:tommycollison since:2008-11-06 until:2009-07-04\n",
            "INFO: Querying from:tommycollison since:2017-05-26 until:2018-01-21\n",
            "INFO: Querying from:tommycollison since:2010-03-01 until:2010-10-27\n",
            "INFO: Querying from:tommycollison since:2007-07-14 until:2008-03-10\n",
            "INFO: Querying from:tommycollison since:2014-10-08 until:2015-06-05\n",
            "INFO: Got 0 tweets for from%3Atommycollison%20since%3A2006-03-21%20until%3A2006-11-16.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Atommycollison%20since%3A2006-11-16%20until%3A2007-07-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 28 tweets for from%3Atommycollison%20since%3A2007-07-14%20until%3A2008-03-10.\n",
            "INFO: Got 28 tweets (28 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2008-03-10%20until%3A2008-11-06.\n",
            "INFO: Got 88 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2010-03-01%20until%3A2010-10-27.\n",
            "INFO: Got 148 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2009-07-04%20until%3A2010-03-01.\n",
            "INFO: Got 208 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2012-10-17%20until%3A2013-06-14.\n",
            "INFO: Got 268 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2016-01-31%20until%3A2016-09-28.\n",
            "INFO: Got 328 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2014-02-10%20until%3A2014-10-08.\n",
            "INFO: Got 388 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2014-10-08%20until%3A2015-06-05.\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2008-11-06%20until%3A2009-07-04.\n",
            "INFO: Got 448 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2012-02-20%20until%3A2012-10-17.\n",
            "INFO: Got 508 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2011-06-25%20until%3A2012-02-20.\n",
            "INFO: Got 568 tweets (60 new).\n",
            "INFO: Got 628 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2017-05-26%20until%3A2018-01-21.\n",
            "INFO: Got 688 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2016-09-28%20until%3A2017-05-26.\n",
            "INFO: Got 748 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2015-06-05%20until%3A2016-01-31.\n",
            "INFO: Got 808 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2010-10-27%20until%3A2011-06-25.\n",
            "INFO: Got 868 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2013-06-14%20until%3A2014-02-10.\n",
            "INFO: Got 928 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2018-09-18%20until%3A2019-05-17.\n",
            "INFO: Got 988 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Atommycollison%20since%3A2018-01-21%20until%3A2018-09-18.\n",
            "INFO: Got 1048 tweets (60 new).\n",
            "INFO: queries: ['from:patrickc since:2006-03-21 until:2006-11-16', 'from:patrickc since:2006-11-16 until:2007-07-14', 'from:patrickc since:2007-07-14 until:2008-03-10', 'from:patrickc since:2008-03-10 until:2008-11-06', 'from:patrickc since:2008-11-06 until:2009-07-04', 'from:patrickc since:2009-07-04 until:2010-03-01', 'from:patrickc since:2010-03-01 until:2010-10-27', 'from:patrickc since:2010-10-27 until:2011-06-25', 'from:patrickc since:2011-06-25 until:2012-02-20', 'from:patrickc since:2012-02-20 until:2012-10-17', 'from:patrickc since:2012-10-17 until:2013-06-14', 'from:patrickc since:2013-06-14 until:2014-02-10', 'from:patrickc since:2014-02-10 until:2014-10-08', 'from:patrickc since:2014-10-08 until:2015-06-05', 'from:patrickc since:2015-06-05 until:2016-01-31', 'from:patrickc since:2016-01-31 until:2016-09-28', 'from:patrickc since:2016-09-28 until:2017-05-26', 'from:patrickc since:2017-05-26 until:2018-01-21', 'from:patrickc since:2018-01-21 until:2018-09-18', 'from:patrickc since:2018-09-18 until:2019-05-17']\n",
            "INFO: Querying from:patrickc since:2008-03-10 until:2008-11-06\n",
            "INFO: Querying from:patrickc since:2010-03-01 until:2010-10-27\n",
            "INFO: Querying from:patrickc since:2008-11-06 until:2009-07-04\n",
            "INFO: Querying from:patrickc since:2009-07-04 until:2010-03-01\n",
            "INFO: Querying from:patrickc since:2006-11-16 until:2007-07-14\n",
            "INFO: Querying from:patrickc since:2007-07-14 until:2008-03-10\n",
            "INFO: Querying from:patrickc since:2006-03-21 until:2006-11-16\n",
            "INFO: Querying from:patrickc since:2014-02-10 until:2014-10-08\n",
            "INFO: Querying from:patrickc since:2015-06-05 until:2016-01-31\n",
            "INFO: Querying from:patrickc since:2018-01-21 until:2018-09-18\n",
            "INFO: Querying from:patrickc since:2011-06-25 until:2012-02-20\n",
            "INFO: Querying from:patrickc since:2010-10-27 until:2011-06-25\n",
            "INFO: Querying from:patrickc since:2013-06-14 until:2014-02-10\n",
            "INFO: Querying from:patrickc since:2016-01-31 until:2016-09-28\n",
            "INFO: Querying from:patrickc since:2018-09-18 until:2019-05-17\n",
            "INFO: Querying from:patrickc since:2016-09-28 until:2017-05-26\n",
            "INFO: Querying from:patrickc since:2012-02-20 until:2012-10-17\n",
            "INFO: Querying from:patrickc since:2017-05-26 until:2018-01-21\n",
            "INFO: Querying from:patrickc since:2014-10-08 until:2015-06-05\n",
            "INFO: Querying from:patrickc since:2012-10-17 until:2013-06-14\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2008-11-06%20until%3A2009-07-04.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2010-03-01%20until%3A2010-10-27.\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2009-07-04%20until%3A2010-03-01.\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2010-10-27%20until%3A2011-06-25.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2012-02-20%20until%3A2012-10-17.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2006-11-16%20until%3A2007-07-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2006-03-21%20until%3A2006-11-16.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2007-07-14%20until%3A2008-03-10.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2012-10-17%20until%3A2013-06-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Apatrickc%20since%3A2008-03-10%20until%3A2008-11-06.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 1 tweets for from%3Apatrickc%20since%3A2011-06-25%20until%3A2012-02-20.\n",
            "INFO: Got 1 tweets (1 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2014-02-10%20until%3A2014-10-08.\n",
            "INFO: Got 61 tweets (60 new).\n",
            "INFO: Got 59 tweets for from%3Apatrickc%20since%3A2013-06-14%20until%3A2014-02-10.\n",
            "INFO: Got 120 tweets (59 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2014-10-08%20until%3A2015-06-05.\n",
            "INFO: Got 180 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2015-06-05%20until%3A2016-01-31.\n",
            "INFO: Got 240 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2016-09-28%20until%3A2017-05-26.\n",
            "INFO: Got 300 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2016-01-31%20until%3A2016-09-28.\n",
            "INFO: Got 360 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2018-01-21%20until%3A2018-09-18.\n",
            "INFO: Got 420 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2017-05-26%20until%3A2018-01-21.\n",
            "INFO: Got 480 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Apatrickc%20since%3A2018-09-18%20until%3A2019-05-17.\n",
            "INFO: Got 540 tweets (60 new).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXygjYcwUXrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a56d5d26-1078-4430-d64f-60a7d2aba5da"
      },
      "source": [
        "len(tommy_tweets)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwj7BGoVYt3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "583f614b-deaa-4475-9fca-68c7a3a72257"
      },
      "source": [
        "len(patrick_tweets)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "540"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgd5qu7umcji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eed44b3f-3b25-4a6f-aac2-3c9ddaf980b5"
      },
      "source": [
        "type(tommy_tweets[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "twitterscraper.tweet.Tweet"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2AD2wPmh9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0a8736d-ee35-4162-8fcb-0605b2493eeb"
      },
      "source": [
        "tommy_tweets[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<twitterscraper.tweet.Tweet at 0x7f58cddd1c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DTmCEX2mnJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b5b0dbb-c0cc-4b1c-f9d5-dfe4c4882bfe"
      },
      "source": [
        "type(patrick_tweets[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "twitterscraper.tweet.Tweet"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8go1sXkmxdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24c98876-498d-4a34-c13a-4f2e71ac79db"
      },
      "source": [
        "patrick_tweets[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<twitterscraper.tweet.Tweet at 0x7f58cdf41588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTtPI5U1aS9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d7840485-574b-44a6-83f8-852d13de0c7c"
      },
      "source": [
        "#convert tweet object to string\n",
        "index = 0\n",
        "\n",
        "for tweet in tommy_tweets:\n",
        "  tweet = tommy_tweets[index].text\n",
        "  tommy_tweets[index] = tweet\n",
        "  index = index + 1\n",
        "  \n",
        "tommy_tweets[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good lan, 2 games and la Cuichina pizza. Looking forward to tomorrow,  Monday for the first time...well, ever!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISfCI2ePlfwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a304218b-b23c-4e52-d123-04d24703162d"
      },
      "source": [
        "#convert tweet object to string\n",
        "index = 0\n",
        "\n",
        "for tweet in patrick_tweets:\n",
        "  tweet = patrick_tweets[index].text\n",
        "  patrick_tweets[index] = tweet\n",
        "  index = index + 1\n",
        "  \n",
        "patrick_tweets[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@enneff not specifically, but we'd love to link to a third-party library if someone wrote one :)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtU7gO-fjeA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get a string with all text\n",
        "tommy_tweets_string = \" \".join(tommy_tweets)\n",
        "patrick_tweets_string = \" \".join(patrick_tweets)\n",
        "all_text = \" \".join([tommy_tweets_string, patrick_tweets_string])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xkt1aQ7kRMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make a dictionary with all text\n",
        "characters = list(set(all_text))\n",
        "num_characters = len(characters)\n",
        "text_length = len(all_text)\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(characters))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(characters))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxA8Oz0Fvn1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42ed1ee4-8a7e-48f7-b22e-d9579aad70de"
      },
      "source": [
        "num_characters"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpgIhHltk8at",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1865
        },
        "outputId": "0a6fc9aa-2773-405a-de10-337461ad1dfc"
      },
      "source": [
        "#apply per tweet\n",
        "index = 0\n",
        "tommy_tweets_encoded = []\n",
        "\n",
        "for tweet in tommy_tweets:\n",
        "  integer_encoded = [char_to_int[i] for i in tommy_tweets[index]]\n",
        "  tommy_tweets_encoded.append(integer_encoded)\n",
        "  index = index + 1\n",
        "  \n",
        "tommy_tweets_encoded[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[48,\n",
              " 116,\n",
              " 116,\n",
              " 122,\n",
              " 90,\n",
              " 50,\n",
              " 18,\n",
              " 17,\n",
              " 26,\n",
              " 90,\n",
              " 37,\n",
              " 90,\n",
              " 59,\n",
              " 18,\n",
              " 33,\n",
              " 55,\n",
              " 54,\n",
              " 90,\n",
              " 18,\n",
              " 17,\n",
              " 122,\n",
              " 90,\n",
              " 50,\n",
              " 18,\n",
              " 90,\n",
              " 138,\n",
              " 66,\n",
              " 41,\n",
              " 29,\n",
              " 128,\n",
              " 41,\n",
              " 17,\n",
              " 18,\n",
              " 90,\n",
              " 70,\n",
              " 41,\n",
              " 76,\n",
              " 76,\n",
              " 18,\n",
              " 11,\n",
              " 90,\n",
              " 10,\n",
              " 116,\n",
              " 116,\n",
              " 30,\n",
              " 41,\n",
              " 17,\n",
              " 59,\n",
              " 90,\n",
              " 38,\n",
              " 116,\n",
              " 40,\n",
              " 47,\n",
              " 18,\n",
              " 40,\n",
              " 122,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 33,\n",
              " 116,\n",
              " 40,\n",
              " 40,\n",
              " 116,\n",
              " 47,\n",
              " 26,\n",
              " 90,\n",
              " 90,\n",
              " 98,\n",
              " 116,\n",
              " 17,\n",
              " 122,\n",
              " 18,\n",
              " 2,\n",
              " 90,\n",
              " 38,\n",
              " 116,\n",
              " 40,\n",
              " 90,\n",
              " 5,\n",
              " 128,\n",
              " 55,\n",
              " 90,\n",
              " 38,\n",
              " 41,\n",
              " 40,\n",
              " 54,\n",
              " 5,\n",
              " 90,\n",
              " 5,\n",
              " 41,\n",
              " 33,\n",
              " 55,\n",
              " 11,\n",
              " 11,\n",
              " 11,\n",
              " 47,\n",
              " 55,\n",
              " 50,\n",
              " 50,\n",
              " 26,\n",
              " 90,\n",
              " 55,\n",
              " 99,\n",
              " 55,\n",
              " 40,\n",
              " 21]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw5Ehm99iefG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1630
        },
        "outputId": "7923c549-7e8f-4349-9bf0-f44199b0a8d9"
      },
      "source": [
        "#apply per tweet\n",
        "index = 0\n",
        "patrick_tweets_encoded = []\n",
        "\n",
        "for tweet in patrick_tweets:\n",
        "  \n",
        "  integer_encoded = [char_to_int[i] for i in patrick_tweets[index]]\n",
        "  \n",
        "  patrick_tweets_encoded.append(integer_encoded)\n",
        "    \n",
        "  index = index + 1\n",
        "  \n",
        "patrick_tweets_encoded[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[92,\n",
              " 55,\n",
              " 17,\n",
              " 17,\n",
              " 55,\n",
              " 38,\n",
              " 38,\n",
              " 90,\n",
              " 17,\n",
              " 116,\n",
              " 5,\n",
              " 90,\n",
              " 54,\n",
              " 70,\n",
              " 55,\n",
              " 29,\n",
              " 41,\n",
              " 38,\n",
              " 41,\n",
              " 29,\n",
              " 18,\n",
              " 50,\n",
              " 50,\n",
              " 2,\n",
              " 26,\n",
              " 90,\n",
              " 145,\n",
              " 66,\n",
              " 5,\n",
              " 90,\n",
              " 47,\n",
              " 55,\n",
              " 71,\n",
              " 122,\n",
              " 90,\n",
              " 50,\n",
              " 116,\n",
              " 99,\n",
              " 55,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 90,\n",
              " 50,\n",
              " 41,\n",
              " 17,\n",
              " 30,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 90,\n",
              " 18,\n",
              " 90,\n",
              " 5,\n",
              " 128,\n",
              " 41,\n",
              " 40,\n",
              " 122,\n",
              " 83,\n",
              " 70,\n",
              " 18,\n",
              " 40,\n",
              " 5,\n",
              " 2,\n",
              " 90,\n",
              " 50,\n",
              " 41,\n",
              " 145,\n",
              " 40,\n",
              " 18,\n",
              " 40,\n",
              " 2,\n",
              " 90,\n",
              " 41,\n",
              " 38,\n",
              " 90,\n",
              " 54,\n",
              " 116,\n",
              " 33,\n",
              " 55,\n",
              " 116,\n",
              " 17,\n",
              " 55,\n",
              " 90,\n",
              " 47,\n",
              " 40,\n",
              " 116,\n",
              " 5,\n",
              " 55,\n",
              " 90,\n",
              " 116,\n",
              " 17,\n",
              " 55,\n",
              " 90,\n",
              " 4,\n",
              " 72]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYAi3oLoipr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3401966c-b34b-4ddf-fb63-812d316eb293"
      },
      "source": [
        "len(tommy_tweets_encoded)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1049"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx8e3aIFc_v7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6631ff6-679b-4317-9cd1-b72cda6b5acd"
      },
      "source": [
        "len(patrick_tweets_encoded)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "540"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t6BoxPYntex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8eacbd7-a13d-459a-d8d9-030664f192d1"
      },
      "source": [
        "#label tweets\n",
        "import numpy as np\n",
        "tommy_label = np.ones(len(tommy_tweets_encoded)).tolist()\n",
        "patrick_label = np.zeros(len(patrick_tweets_encoded)).tolist()\n",
        "print(len(tommy_label), len(patrick_label))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1049 540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrorGe0JqWyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine the arrays\n",
        "encoded_tweets = tommy_tweets_encoded + patrick_tweets_encoded\n",
        "labels = tommy_label + patrick_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12WVhrKNrTbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38061e8b-4bb2-428b-a279-029eba919252"
      },
      "source": [
        "print(len(encoded_tweets), len(labels))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1590 1590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bqrQmgn7oxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "bdfd58a2-5f37-4e9e-d0a6-3abea1b9e022"
      },
      "source": [
        "import pandas as pd\n",
        "data = {'label': labels, 'tweet': encoded_tweets}\n",
        "df = pd.DataFrame(data = data)\n",
        "df.head(5)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[48, 116, 116, 122, 90, 50, 18, 17, 26, 90, 37...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[121, 66, 54, 5, 90, 38, 41, 17, 41, 54, 128, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>[10, 116, 116, 30, 41, 17, 59, 90, 38, 116, 40...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>[140, 128, 116, 71, 122, 90, 128, 18, 99, 55, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>[10, 18, 70, 5, 116, 70, 21]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                              tweet\n",
              "0     1  [48, 116, 116, 122, 90, 50, 18, 17, 26, 90, 37...\n",
              "1     1  [121, 66, 54, 5, 90, 38, 41, 17, 41, 54, 128, ...\n",
              "2     1  [10, 116, 116, 30, 41, 17, 59, 90, 38, 116, 40...\n",
              "3     1  [140, 128, 116, 71, 122, 90, 128, 18, 99, 55, ...\n",
              "4     1                       [10, 18, 70, 5, 116, 70, 21]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhn3R9oGAKnO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94e1031a-a51b-41ff-cf86-fad69394a39f"
      },
      "source": [
        "X = df['tweet'].values\n",
        "y = df['label'].values\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1590,) (1590,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvJ_j0SYsXZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cm8KbGwtGxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
        "                                                test_size = 0.2,\n",
        "                                                random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXGfnnZk63Nz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1828250-41c0-4b19-bf69-763aab16e1e7"
      },
      "source": [
        "print(ytest[0]) #shape of a y value"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7roA6Fi66sI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4637
        },
        "outputId": "d1df34d4-4d98-4b8e-fe4c-6a9d253bfedc"
      },
      "source": [
        "Xtest[0] #shape of an X value"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37,\n",
              " 108,\n",
              " 90,\n",
              " 142,\n",
              " 99,\n",
              " 55,\n",
              " 40,\n",
              " 2,\n",
              " 116,\n",
              " 17,\n",
              " 55,\n",
              " 90,\n",
              " 47,\n",
              " 128,\n",
              " 116,\n",
              " 90,\n",
              " 54,\n",
              " 41,\n",
              " 59,\n",
              " 17,\n",
              " 54,\n",
              " 90,\n",
              " 66,\n",
              " 70,\n",
              " 90,\n",
              " 38,\n",
              " 116,\n",
              " 40,\n",
              " 90,\n",
              " 10,\n",
              " 18,\n",
              " 33,\n",
              " 145,\n",
              " 122,\n",
              " 18,\n",
              " 90,\n",
              " 41,\n",
              " 54,\n",
              " 90,\n",
              " 41,\n",
              " 33,\n",
              " 70,\n",
              " 50,\n",
              " 41,\n",
              " 29,\n",
              " 41,\n",
              " 5,\n",
              " 50,\n",
              " 2,\n",
              " 90,\n",
              " 18,\n",
              " 59,\n",
              " 40,\n",
              " 55,\n",
              " 55,\n",
              " 41,\n",
              " 17,\n",
              " 59,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 90,\n",
              " 54,\n",
              " 70,\n",
              " 55,\n",
              " 17,\n",
              " 122,\n",
              " 90,\n",
              " 128,\n",
              " 116,\n",
              " 66,\n",
              " 40,\n",
              " 54,\n",
              " 90,\n",
              " 55,\n",
              " 99,\n",
              " 55,\n",
              " 40,\n",
              " 2,\n",
              " 90,\n",
              " 47,\n",
              " 55,\n",
              " 55,\n",
              " 30,\n",
              " 90,\n",
              " 47,\n",
              " 18,\n",
              " 2,\n",
              " 26,\n",
              " 90,\n",
              " 47,\n",
              " 18,\n",
              " 2,\n",
              " 90,\n",
              " 116,\n",
              " 66,\n",
              " 5,\n",
              " 54,\n",
              " 41,\n",
              " 122,\n",
              " 55,\n",
              " 90,\n",
              " 5,\n",
              " 128,\n",
              " 55,\n",
              " 41,\n",
              " 40,\n",
              " 90,\n",
              " 29,\n",
              " 116,\n",
              " 33,\n",
              " 38,\n",
              " 116,\n",
              " 40,\n",
              " 5,\n",
              " 90,\n",
              " 76,\n",
              " 116,\n",
              " 17,\n",
              " 55,\n",
              " 90,\n",
              " 80,\n",
              " 128,\n",
              " 116,\n",
              " 47,\n",
              " 90,\n",
              " 116,\n",
              " 38,\n",
              " 5,\n",
              " 55,\n",
              " 17,\n",
              " 90,\n",
              " 122,\n",
              " 116,\n",
              " 90,\n",
              " 70,\n",
              " 55,\n",
              " 116,\n",
              " 70,\n",
              " 50,\n",
              " 55,\n",
              " 90,\n",
              " 29,\n",
              " 128,\n",
              " 116,\n",
              " 116,\n",
              " 54,\n",
              " 55,\n",
              " 90,\n",
              " 5,\n",
              " 128,\n",
              " 18,\n",
              " 5,\n",
              " 25,\n",
              " 72,\n",
              " 90,\n",
              " 54,\n",
              " 116,\n",
              " 90,\n",
              " 5,\n",
              " 128,\n",
              " 55,\n",
              " 2,\n",
              " 90,\n",
              " 29,\n",
              " 18,\n",
              " 17,\n",
              " 90,\n",
              " 50,\n",
              " 55,\n",
              " 18,\n",
              " 40,\n",
              " 17,\n",
              " 90,\n",
              " 17,\n",
              " 55,\n",
              " 47,\n",
              " 90,\n",
              " 54,\n",
              " 30,\n",
              " 41,\n",
              " 50,\n",
              " 50,\n",
              " 54,\n",
              " 11,\n",
              " 90,\n",
              " 148,\n",
              " 90,\n",
              " 54,\n",
              " 66,\n",
              " 54,\n",
              " 70,\n",
              " 55,\n",
              " 29,\n",
              " 5,\n",
              " 90,\n",
              " 5,\n",
              " 128,\n",
              " 18,\n",
              " 5,\n",
              " 90,\n",
              " 47,\n",
              " 41,\n",
              " 50,\n",
              " 50,\n",
              " 41,\n",
              " 17,\n",
              " 59,\n",
              " 17,\n",
              " 55,\n",
              " 54,\n",
              " 54,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 90,\n",
              " 145,\n",
              " 55,\n",
              " 90,\n",
              " 66,\n",
              " 17,\n",
              " 29,\n",
              " 116,\n",
              " 33,\n",
              " 38,\n",
              " 116,\n",
              " 40,\n",
              " 5,\n",
              " 18,\n",
              " 145,\n",
              " 50,\n",
              " 55,\n",
              " 90,\n",
              " 135,\n",
              " 40,\n",
              " 55,\n",
              " 18,\n",
              " 50,\n",
              " 50,\n",
              " 2,\n",
              " 135,\n",
              " 90,\n",
              " 54,\n",
              " 5,\n",
              " 18,\n",
              " 17,\n",
              " 122,\n",
              " 54,\n",
              " 90,\n",
              " 5,\n",
              " 116,\n",
              " 90,\n",
              " 54,\n",
              " 5,\n",
              " 66,\n",
              " 122,\n",
              " 55,\n",
              " 17,\n",
              " 5,\n",
              " 54,\n",
              " 11,\n",
              " 90,\n",
              " 80,\n",
              " 148,\n",
              " 90,\n",
              " 128,\n",
              " 116,\n",
              " 70,\n",
              " 55,\n",
              " 90,\n",
              " 54,\n",
              " 116,\n",
              " 11,\n",
              " 90,\n",
              " 72]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgS39iebtyNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "4c3cf741-9e61-4cb0-e354-df78a062c0f8"
      },
      "source": [
        "max_features = num_characters #number of character mappings in dict\n",
        "maxlen = 240 #number of characters in tweet\n",
        "batch_size = 32\n",
        "\n",
        "Xtrain = sequence.pad_sequences(Xtrain, maxlen=maxlen)\n",
        "Xtest = sequence.pad_sequences(Xtest, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(Xtrain, ytrain, batch_size=32, epochs=15,\n",
        "          validation_data=(Xtest, ytest))\n",
        "\n",
        "score, acc = model.evaluate(Xtest, ytest,\n",
        "                            batch_size=32)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-58ef8612a5b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUIoKqc-UXrd",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - CNNs\n",
        "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW93Usv9UXrf",
        "colab_type": "code",
        "colab": {},
        "outputId": "3deb4e21-620a-4b3e-e448-e217da8408c5"
      },
      "source": [
        "!pip install google_images_download"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_images_download\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/a7/1dc0de31c24d6b93469f46d743f36a7f51406d4e452690706ac5be2a5eab/google_images_download-2.7.1.tar.gz\n",
            "Collecting selenium (from google_images_download)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K    100% || 911kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /Users/jonathansokoll/anaconda3/lib/python3.7/site-packages (from selenium->google_images_download) (1.24.1)\n",
            "Building wheels for collected packages: google-images-download\n",
            "  Building wheel for google-images-download (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /Users/jonathansokoll/Library/Caches/pip/wheels/22/f3/7b/d1d7a18d9784458622ef3f9702c0bdbc179b431adde169c1a0\n",
            "Successfully built google-images-download\n",
            "Installing collected packages: selenium, google-images-download\n",
            "Successfully installed google-images-download-2.7.1 selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkH0JzyjUXrl",
        "colab_type": "code",
        "colab": {},
        "outputId": "587e2e82-b4c5-4b10-8035-e597b3d3b255"
      },
      "source": [
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "arguments = {'keywords': \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
        "absolute_image_paths = response.download(arguments)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Item no.: 1 --> Item name = animal pond\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
            "Completed Image ====> 1.Pondanimals.GIF\n",
            "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
            "Completed Image ====> 2.hqdefault.jpg\n",
            "Image URL: https://get.pxhere.com/photo/water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
            "Completed Image ====> 3.water-animal-pond-wildlife-mammal-fish-eat-fauna-whiskers-vertebrate-otter-mink-marmot-sea-otter-mustelidae-1383482.jpg\n",
            "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
            "Completed Image ====> 4.PKLS4116_inline.png\n",
            "Image URL: http://images.animalpicturesociety.com/images/5d/alligator_animal_on_pond.jpg\n",
            "Completed Image ====> 5.alligator_animal_on_pond.jpg\n",
            "\n",
            "Errors: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhFMF2ZCUXrr",
        "colab_type": "text"
      },
      "source": [
        "At the time of writing at least a few do, but since the internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is validly run ResNet50 on the input images - don't worry about tuning or improving the model. \n",
        "\n",
        "*Hint:* ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are bullfrog, tree frog, and tailed frog.\n",
        "\n",
        "Stretch goal - also check for fish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GKD7s-SUXrs",
        "colab_type": "code",
        "colab": {},
        "outputId": "dbb319bd-9231-4145-d108-a940ae9cf999"
      },
      "source": [
        "# TODO - your code!\n",
        "raise Exception(\"This task is incomplete. \\nReplace this line with your code.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "This task is incomplete. \nReplace this line with your code.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9ede0649e9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO - your code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This task is incomplete. \\nReplace this line with your code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: This task is incomplete. \nReplace this line with your code."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6U_v6GqUzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as5pNr_xUXr0",
        "colab_type": "text"
      },
      "source": [
        "## Part 3 - AutoML\n",
        "\n",
        "Use [TPOT](https://epistasislab.github.io/tpot/) to fit a predictive model for the King County housing data, with `price` as the target output variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L797BYThUXr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tpot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1b8uBMJUXr8",
        "colab_type": "code",
        "colab": {},
        "outputId": "ecf63993-9908-49a2-85dd-6220f609f188"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>...</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7129300520</td>\n",
              "      <td>20141013T000000</td>\n",
              "      <td>221900.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1180</td>\n",
              "      <td>5650</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1180</td>\n",
              "      <td>0</td>\n",
              "      <td>1955</td>\n",
              "      <td>0</td>\n",
              "      <td>98178</td>\n",
              "      <td>47.5112</td>\n",
              "      <td>-122.257</td>\n",
              "      <td>1340</td>\n",
              "      <td>5650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6414100192</td>\n",
              "      <td>20141209T000000</td>\n",
              "      <td>538000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2570</td>\n",
              "      <td>7242</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>2170</td>\n",
              "      <td>400</td>\n",
              "      <td>1951</td>\n",
              "      <td>1991</td>\n",
              "      <td>98125</td>\n",
              "      <td>47.7210</td>\n",
              "      <td>-122.319</td>\n",
              "      <td>1690</td>\n",
              "      <td>7639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5631500400</td>\n",
              "      <td>20150225T000000</td>\n",
              "      <td>180000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>770</td>\n",
              "      <td>10000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>770</td>\n",
              "      <td>0</td>\n",
              "      <td>1933</td>\n",
              "      <td>0</td>\n",
              "      <td>98028</td>\n",
              "      <td>47.7379</td>\n",
              "      <td>-122.233</td>\n",
              "      <td>2720</td>\n",
              "      <td>8062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2487200875</td>\n",
              "      <td>20141209T000000</td>\n",
              "      <td>604000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>3.00</td>\n",
              "      <td>1960</td>\n",
              "      <td>5000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1050</td>\n",
              "      <td>910</td>\n",
              "      <td>1965</td>\n",
              "      <td>0</td>\n",
              "      <td>98136</td>\n",
              "      <td>47.5208</td>\n",
              "      <td>-122.393</td>\n",
              "      <td>1360</td>\n",
              "      <td>5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1954400510</td>\n",
              "      <td>20150218T000000</td>\n",
              "      <td>510000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1680</td>\n",
              "      <td>8080</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>1680</td>\n",
              "      <td>0</td>\n",
              "      <td>1987</td>\n",
              "      <td>0</td>\n",
              "      <td>98074</td>\n",
              "      <td>47.6168</td>\n",
              "      <td>-122.045</td>\n",
              "      <td>1800</td>\n",
              "      <td>7503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
              "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
              "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
              "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
              "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
              "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
              "\n",
              "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
              "0      5650     1.0           0     0  ...      7        1180              0   \n",
              "1      7242     2.0           0     0  ...      7        2170            400   \n",
              "2     10000     1.0           0     0  ...      6         770              0   \n",
              "3      5000     1.0           0     0  ...      7        1050            910   \n",
              "4      8080     1.0           0     0  ...      8        1680              0   \n",
              "\n",
              "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
              "0      1955             0    98178  47.5112 -122.257           1340   \n",
              "1      1951          1991    98125  47.7210 -122.319           1690   \n",
              "2      1933             0    98028  47.7379 -122.233           2720   \n",
              "3      1965             0    98136  47.5208 -122.393           1360   \n",
              "4      1987             0    98074  47.6168 -122.045           1800   \n",
              "\n",
              "   sqft_lot15  \n",
              "0        5650  \n",
              "1        7639  \n",
              "2        8062  \n",
              "3        5000  \n",
              "4        7503  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZCyvvfbUXsB",
        "colab_type": "text"
      },
      "source": [
        "As with previous questions, your goal is to run TPOT and successfully run and report error at the end. Also, in the interest of time, feel free to choose small `generation=1`and `population_size=10` parameters, so your pipeline runs efficiently. You will want to be able to iterate and test. \n",
        "\n",
        "*Hint:* You will have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running - as long as you still get a valid model with reasonable predictive power. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hKZYRGFUXsC",
        "colab_type": "code",
        "colab": {},
        "outputId": "8834c8b5-2dbc-4328-978b-bec236b16348"
      },
      "source": [
        "# TODO - your code!\n",
        "raise Exception(\"This task is incomplete. \\nReplace this line with your code.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "This task is incomplete. \nReplace this line with your code.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9ede0649e9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO - your code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This task is incomplete. \\nReplace this line with your code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: This task is incomplete. \nReplace this line with your code."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SYNOvxIUXsI",
        "colab_type": "text"
      },
      "source": [
        "## Part 4 - More... \n",
        "\n",
        "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
        "* What do you consider your strongest area as a Data Scientist? \n",
        "* What area of Data Science would you most like to learn more about and why? \n",
        "* Where do you think Data Science will be in 5 years? \n",
        "\n",
        "A few sentences per answer is fine. Only elaborate if time allows. Use markdown to format your answers.\n",
        "\n",
        "Thank you for your hard, and congratulations!! You've learned a lot, and you should proudly call yourself a Data Scientist. "
      ]
    }
  ]
}