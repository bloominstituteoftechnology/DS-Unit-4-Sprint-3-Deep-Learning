{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhU3R-8dzk5z"
   },
   "source": [
    "# Lambda School Data Science Unit 4 Sprint Challenge 4\n",
    "\n",
    "## RNNs, CNNs, AutoML, and more...\n",
    "\n",
    "In this sprint challenge, you'll explore some of the cutting edge of Data Science.\n",
    "\n",
    "*Caution* - these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5UwGRnJOmD4"
   },
   "source": [
    "## Part 1 - RNNs\n",
    "\n",
    "Use an RNN to fit a simple classification model on tweets to distinguish from tweets from Austen Allred and tweets from Weird Al Yankovic.\n",
    "\n",
    "Following is code to scrape the needed data (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "3if1yTMUoG3U",
    "outputId": "6075f571-d8b1-4e91-bfb9-26ad7a467261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twitterscraper in c:\\users\\jhump\\anaconda3\\lib\\site-packages (0.9.3)\n",
      "Requirement already satisfied: coala-utils~=0.5.0 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from twitterscraper) (0.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from twitterscraper) (2.21.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from twitterscraper) (4.3.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from twitterscraper) (0.0.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests->twitterscraper) (2019.3.9)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from bs4->twitterscraper) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->twitterscraper) (1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install twitterscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1114
    },
    "colab_type": "code",
    "id": "DS-9ksWjoJit",
    "outputId": "0c3512e4-5cd4-4dc6-9cda-baf00c835f59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-14', 'from:austen since:2006-11-14 until:2007-07-11', 'from:austen since:2007-07-11 until:2008-03-05', 'from:austen since:2008-03-05 until:2008-10-30', 'from:austen since:2008-10-30 until:2009-06-25', 'from:austen since:2009-06-25 until:2010-02-19', 'from:austen since:2010-02-19 until:2010-10-15', 'from:austen since:2010-10-15 until:2011-06-11', 'from:austen since:2011-06-11 until:2012-02-04', 'from:austen since:2012-02-04 until:2012-09-30', 'from:austen since:2012-09-30 until:2013-05-26', 'from:austen since:2013-05-26 until:2014-01-20', 'from:austen since:2014-01-20 until:2014-09-15', 'from:austen since:2014-09-15 until:2015-05-12', 'from:austen since:2015-05-12 until:2016-01-05', 'from:austen since:2016-01-05 until:2016-08-31', 'from:austen since:2016-08-31 until:2017-04-26', 'from:austen since:2017-04-26 until:2017-12-21', 'from:austen since:2017-12-21 until:2018-08-16', 'from:austen since:2018-08-16 until:2019-04-12']\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 1 tweets (1 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 1 tweets (0 new).\n",
      "INFO: Got 61 tweets (60 new).\n",
      "INFO: Got 121 tweets (60 new).\n",
      "INFO: Got 181 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitterscraper import query_tweets\n",
    "\n",
    "austen_tweets = query_tweets('from:austen', 1000)\n",
    "len(austen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fLKqFh8DovaN",
    "outputId": "64b0d621-7e74-4181-9116-406e8c518465"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1114
    },
    "colab_type": "code",
    "id": "MRQeIIf1orCS",
    "outputId": "44b57b5e-2a0e-4656-ca06-d77637caf593"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['from:AlYankovic since:2006-03-21 until:2006-11-14', 'from:AlYankovic since:2006-11-14 until:2007-07-11', 'from:AlYankovic since:2007-07-11 until:2008-03-05', 'from:AlYankovic since:2008-03-05 until:2008-10-30', 'from:AlYankovic since:2008-10-30 until:2009-06-25', 'from:AlYankovic since:2009-06-25 until:2010-02-19', 'from:AlYankovic since:2010-02-19 until:2010-10-15', 'from:AlYankovic since:2010-10-15 until:2011-06-11', 'from:AlYankovic since:2011-06-11 until:2012-02-04', 'from:AlYankovic since:2012-02-04 until:2012-09-30', 'from:AlYankovic since:2012-09-30 until:2013-05-26', 'from:AlYankovic since:2013-05-26 until:2014-01-20', 'from:AlYankovic since:2014-01-20 until:2014-09-15', 'from:AlYankovic since:2014-09-15 until:2015-05-12', 'from:AlYankovic since:2015-05-12 until:2016-01-05', 'from:AlYankovic since:2016-01-05 until:2016-08-31', 'from:AlYankovic since:2016-08-31 until:2017-04-26', 'from:AlYankovic since:2017-04-26 until:2017-12-21', 'from:AlYankovic since:2017-12-21 until:2018-08-16', 'from:AlYankovic since:2018-08-16 until:2019-04-12']\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 60 tweets (60 new).\n",
      "INFO: Got 120 tweets (60 new).\n",
      "INFO: Got 180 tweets (60 new).\n",
      "INFO: Got 240 tweets (60 new).\n",
      "INFO: Got 300 tweets (60 new).\n",
      "INFO: Got 360 tweets (60 new).\n",
      "INFO: Got 420 tweets (60 new).\n",
      "INFO: Got 480 tweets (60 new).\n",
      "INFO: Got 540 tweets (60 new).\n",
      "INFO: Got 600 tweets (60 new).\n",
      "INFO: Got 660 tweets (60 new).\n",
      "INFO: Got 720 tweets (60 new).\n",
      "INFO: Got 780 tweets (60 new).\n",
      "INFO: Got 840 tweets (60 new).\n",
      "INFO: Got 900 tweets (60 new).\n",
      "INFO: Got 960 tweets (60 new).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_tweets = query_tweets('from:AlYankovic', 1000)\n",
    "len(al_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_dB7I87ty8f1",
    "outputId": "38e7dffb-92bb-4a4c-8bbd-f1ddd951cc85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey @suzanneyankovic, where'd I leave my shoes?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_tweets[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0mrcjEu_zRl4",
    "outputId": "cdea5ac9-bf26-434f-d2aa-fbc251c8ceef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(austen_tweets + al_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYCVJX6ep8iO"
   },
   "source": [
    "Your tasks:\n",
    "\n",
    "- Encode the characters to a sequence of integers for the model\n",
    "- Get the data into the appropriate shape/format, including labels and a train/test split\n",
    "- Use Keras to fit a predictive model, classifying tweets as being from Austen versus Weird Al\n",
    "- Report your overall score and accuracy\n",
    "\n",
    "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
    "\n",
    "*Note* - focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QVSlFEAqWJM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "# from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data: encoding, shaping, train/test split\n",
    "# encoding\n",
    "austen_text = ''\n",
    "al_text = ''\n",
    "\n",
    "\n",
    "def process_tweets(text, tweets):\n",
    "    for tweet in tweets:  # austen_tweets, al_tweets\n",
    "        try:\n",
    "            text += '\\n\\n' + tweet.text  # austen_text, al_text\n",
    "        except:\n",
    "            print('Failed: ' + tweet.text)\n",
    "\n",
    "    text = text.split('\\n\\n')\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_tweets(text):    \n",
    "    chars = list(set(text)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "    num_chars = len(chars) # the number of unique characters\n",
    "    txt_data_size = len(text)\n",
    "    print(\"unique characters : \", num_chars)\n",
    "    print(\"txt_data_size : \", txt_data_size)\n",
    "    # one hot encode\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    # print(char_to_int)\n",
    "    # print(\"----------------------------------------------------\")\n",
    "    # print(int_to_char)\n",
    "    # print(\"----------------------------------------------------\")\n",
    "    # integer encode input data\n",
    "    integer_encoded = [char_to_int[i] for i in text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "    print(integer_encoded)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"data length : \", len(integer_encoded))\n",
    "    return integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  224\n",
      "txt_data_size :  227\n",
      "[0, 185, 151, 13, 124, 30, 63, 86, 108, 69, 82, 61, 163, 65, 32, 27, 61, 159, 70, 216, 20, 84, 1, 18, 167, 186, 62, 23, 103, 131, 132, 40, 39, 178, 177, 64, 137, 127, 36, 49, 198, 194, 160, 207, 140, 72, 52, 126, 125, 31, 19, 21, 138, 206, 99, 181, 142, 109, 35, 11, 111, 50, 107, 75, 195, 44, 29, 10, 143, 4, 67, 105, 211, 200, 183, 8, 121, 98, 154, 90, 6, 60, 132, 205, 156, 93, 74, 7, 155, 78, 91, 115, 148, 203, 187, 94, 144, 199, 15, 76, 172, 87, 175, 96, 68, 214, 223, 141, 85, 123, 189, 150, 149, 73, 147, 2, 33, 92, 24, 208, 174, 79, 128, 66, 201, 71, 89, 165, 153, 139, 217, 114, 161, 97, 3, 80, 162, 72, 81, 168, 38, 190, 17, 9, 119, 182, 55, 188, 170, 209, 213, 221, 222, 45, 77, 204, 171, 210, 22, 53, 51, 129, 164, 34, 176, 101, 192, 116, 202, 83, 113, 37, 212, 102, 54, 122, 130, 152, 59, 5, 117, 46, 41, 57, 56, 100, 104, 118, 179, 106, 12, 193, 136, 215, 169, 58, 166, 184, 47, 95, 145, 220, 173, 157, 110, 191, 158, 218, 25, 43, 26, 196, 48, 16, 219, 42, 133, 197, 14, 112, 28, 134, 120, 88, 146, 135, 180]\n",
      "----------------------------------------------------\n",
      "data length :  227\n"
     ]
    }
   ],
   "source": [
    "austen_text_procd = encode_tweets(process_tweets(austen_text, austen_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  965\n",
      "txt_data_size :  965\n",
      "[0, 561, 527, 870, 875, 253, 287, 453, 104, 948, 788, 210, 221, 143, 419, 738, 69, 66, 350, 594, 9, 299, 951, 676, 722, 954, 97, 256, 82, 912, 175, 399, 665, 715, 697, 853, 212, 958, 891, 501, 243, 699, 356, 347, 482, 306, 296, 719, 375, 447, 774, 409, 334, 421, 59, 251, 765, 959, 789, 442, 631, 150, 925, 752, 55, 369, 859, 928, 274, 578, 110, 135, 844, 292, 111, 475, 286, 124, 354, 321, 247, 690, 489, 342, 600, 916, 423, 93, 691, 12, 663, 208, 263, 480, 327, 105, 898, 726, 748, 492, 268, 205, 498, 588, 41, 770, 54, 776, 679, 60, 240, 389, 881, 433, 359, 825, 365, 4, 427, 74, 619, 851, 170, 509, 705, 599, 326, 56, 605, 345, 328, 434, 805, 655, 629, 400, 504, 574, 817, 922, 876, 23, 44, 19, 165, 200, 759, 678, 29, 541, 325, 5, 961, 346, 962, 829, 64, 841, 24, 408, 68, 270, 545, 317, 382, 903, 861, 183, 219, 687, 653, 209, 737, 436, 546, 348, 157, 535, 257, 706, 203, 311, 945, 768, 874, 573, 675, 531, 777, 565, 390, 792, 87, 106, 85, 543, 162, 302, 231, 20, 761, 178, 198, 587, 762, 682, 952, 151, 669, 260, 799, 264, 217, 901, 963, 355, 116, 160, 229, 620, 507, 554, 784, 905, 913, 275, 601, 437, 239, 577, 753, 708, 939, 168, 840, 81, 918, 88, 727, 285, 908, 22, 412, 289, 935, 714, 732, 340, 612, 833, 534, 230, 315, 464, 71, 830, 624, 49, 290, 388, 949, 180, 222, 142, 634, 810, 138, 237, 648, 367, 818, 58, 426, 553, 564, 259, 813, 548, 25, 769, 869, 177, 892, 385, 394, 21, 417, 654, 843, 950, 766, 932, 904, 579, 416, 310, 279, 372, 872, 77, 471, 542, 854, 101, 78, 33, 450, 228, 152, 95, 929, 790, 267, 549, 802, 53, 91, 144, 10, 195, 522, 462, 329, 889, 395, 234, 560, 893, 194, 125, 396, 736, 750, 65, 775, 223, 232, 164, 873, 117, 241, 320, 361, 700, 169, 90, 404, 583, 884, 430, 604, 608, 403, 444, 960, 293, 821, 834, 609, 139, 666, 14, 242, 656, 276, 70, 637, 618, 131, 92, 647, 366, 644, 650, 757, 883, 133, 262, 575, 946, 215, 936, 349, 694, 146, 855, 36, 284, 721, 207, 454, 393, 410, 689, 623, 335, 728, 435, 674, 483, 440, 557, 249, 734, 61, 308, 37, 351, 523, 374, 506, 98, 487, 502, 485, 134, 362, 455, 265, 405, 281, 190, 595, 711, 692, 758, 610, 686, 709, 297, 652, 826, 528, 731, 303, 322, 649, 906, 47, 915, 484, 670, 443, 113, 46, 720, 798, 413, 712, 261, 606, 701, 52, 964, 472, 73, 128, 702, 461, 862, 30, 42, 491, 628, 103, 849, 850, 645, 161, 909, 540, 62, 271, 858, 899, 291, 341, 754, 282, 312, 496, 227, 469, 804, 617, 927, 827, 75, 716, 121, 755, 625, 550, 524, 319, 957, 661, 836, 273, 407, 532, 452, 269, 613, 767, 581, 933, 486, 677, 179, 592, 795, 571, 40, 114, 226, 517, 148, 503, 84, 580, 680, 277, 567, 352, 333, 582, 953, 368, 724, 667, 181, 185, 357, 584, 930, 294, 488, 877, 72, 479, 923, 867, 568, 885, 547, 363, 614, 31, 947, 140, 621, 172, 122, 896, 931, 704, 816, 107, 391, 910, 831, 785, 847, 304, 848, 278, 318, 235, 500, 632, 244, 336, 814, 815, 401, 630, 740, 428, 112, 525, 555, 822, 566, 536, 941, 902, 163, 339, 438, 199, 283, 868, 458, 137, 860, 154, 13, 856, 611, 245, 189, 456, 845, 26, 149, 192, 153, 640, 481, 778, 314, 490, 38, 668, 842, 99, 254, 343, 224, 280, 272, 360, 397, 250, 713, 607, 18, 6, 747, 1, 956, 646, 86, 756, 298, 499, 218, 943, 109, 300, 593, 763, 797, 147, 539, 301, 585, 751, 158, 43, 182, 824, 449, 2, 937, 445, 266, 739, 819, 806, 411, 94, 882, 16, 323, 733, 791, 878, 402, 220, 533, 429, 807, 662, 184, 823, 730, 166, 508, 337, 457, 832, 696, 493, 141, 431, 330, 556, 510, 422, 530, 942, 772, 919, 173, 887, 174, 782, 771, 439, 695, 418, 155, 156, 911, 313, 664, 779, 188, 15, 424, 246, 186, 288, 132, 864, 35, 478, 529, 252, 783, 331, 96, 558, 467, 707, 383, 384, 780, 57, 920, 914, 871, 476, 477, 563, 627, 586, 511, 764, 176, 895, 258, 907, 838, 638, 338, 917, 865, 741, 633, 494, 406, 8, 703, 886, 233, 225, 793, 569, 206, 639, 386, 803, 572, 214, 512, 723, 597, 596, 460, 463, 857, 672, 773, 171, 710, 474, 544, 940, 201, 67, 392, 693, 808, 598, 451, 316, 76, 497, 955, 39, 659, 353, 34, 126, 518, 894, 3, 473, 537, 505, 414, 513, 735, 926, 521, 786, 642, 514, 468, 576, 516, 743, 643, 683, 934, 193, 839, 79, 515, 837, 863, 236, 526, 749, 671, 127, 136, 211, 305, 852, 28, 63, 27, 800, 248, 32, 729, 415, 559, 238, 196, 921, 660, 373, 811, 379, 846, 45, 7, 787, 159, 879, 657, 213, 378, 432, 828, 820, 519, 381, 255, 358, 129, 658, 717, 602, 809, 615, 641, 197, 725, 89, 344, 651, 398, 370, 425, 538, 83, 309, 866, 938, 688, 745, 380, 698, 115, 685, 589, 332, 744, 781, 80, 123, 551, 812, 364, 760, 718, 446, 466, 202, 622, 108, 590, 376, 742, 441, 684, 17, 191, 448, 459, 377, 888, 387, 880, 495, 900, 216, 100, 944, 307, 635, 51, 796, 295, 636, 145, 465, 371, 120, 187, 167, 835, 102, 616, 570, 562, 119, 794, 746, 890, 681, 130, 420, 118, 324, 11, 48, 603, 591, 204, 897, 924, 50, 552, 470, 626, 673, 801, 520]\n",
      "----------------------------------------------------\n",
      "data length :  965\n"
     ]
    }
   ],
   "source": [
    "al_text_procd = encode_tweets(process_tweets(al_text, al_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(austen_text_procd, sample(al_text_procd, 227),\n",
    "                                                    test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train...\n",
      "WARNING:tensorflow:From C:\\Users\\jhump\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 170 samples, validate on 57 samples\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 3s 17ms/step - loss: -3.8416 - acc: 0.0059 - val_loss: -11.7103 - val_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 0s 515us/step - loss: -19.5642 - acc: 0.0059 - val_loss: -25.4821 - val_acc: 0.0000e+00\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 0s 474us/step - loss: -38.6183 - acc: 0.0059 - val_loss: -41.3300 - val_acc: 0.0000e+00\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 0s 597us/step - loss: -63.3961 - acc: 0.0059 - val_loss: -59.2663 - val_acc: 0.0000e+00\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 0s 526us/step - loss: -96.7799 - acc: 0.0059 - val_loss: -80.1463 - val_acc: 0.0000e+00\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 0s 589us/step - loss: -144.0230 - acc: 0.0059 - val_loss: -104.1273 - val_acc: 0.0000e+00\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 0s 418us/step - loss: -214.1653 - acc: 0.0118 - val_loss: -132.1498 - val_acc: 0.0000e+00\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 0s 500us/step - loss: -300.0669 - acc: 0.0118 - val_loss: -165.8483 - val_acc: 0.0000e+00\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 0s 653us/step - loss: -430.4064 - acc: 0.0118 - val_loss: -206.0182 - val_acc: 0.0000e+00\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 0s 515us/step - loss: -611.5220 - acc: 0.0118 - val_loss: -253.8431 - val_acc: 0.0000e+00\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 0s 506us/step - loss: -854.1509 - acc: 0.0118 - val_loss: -311.8501 - val_acc: 0.0000e+00\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 0s 1ms/step - loss: -1173.5253 - acc: 0.0118 - val_loss: -380.8584 - val_acc: 0.0000e+00\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 0s 577us/step - loss: -1605.3596 - acc: 0.0118 - val_loss: -461.4919 - val_acc: 0.0000e+00\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 0s 500us/step - loss: -2133.9971 - acc: 0.0118 - val_loss: -557.1449 - val_acc: 0.0000e+00\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 0s 444us/step - loss: -2808.6257 - acc: 0.0118 - val_loss: -665.2370 - val_acc: 0.0000e+00\n",
      "57/57 [==============================] - 0s 140us/step\n",
      "Test score: -665.2370369894463\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fit keras model and report score, accuracy\n",
    "'''\n",
    "Proviso: The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- Choice of batch size is important; choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "\n",
    "max_features = 2000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "# print('Loading data...')\n",
    "\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPn6c0x21gu1"
   },
   "source": [
    "Conclusion - RNN runs, and gives pretty decent improvement over a naive \"It's Al!\" model. To *really* improve the model, more playing with parameters, and just getting more data (particularly Austen tweets), would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz0LCZd_O4IG"
   },
   "source": [
    "## Part 2- CNNs\n",
    "\n",
    "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "whIqEWR236Af",
    "outputId": "7a74e30d-310d-4a3a-9ae4-5bf52d137bda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google_images_download in c:\\users\\jhump\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from google_images_download) (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (1.24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "EKnnnM8k38sN",
    "outputId": "59f477e9-0b25-4a38-9678-af24e0176535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = animal pond\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
      "Completed Image ====> 1. pondanimals.gif\n",
      "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
      "Completed Image ====> 2. hqdefault.jpg\n",
      "Image URL: https://vetstreet-brightspot.s3.amazonaws.com/8d/ac/377fecad46d8820697c26efacc32/koi-pond-thinkstock-153560141-335sm61313.jpg\n",
      "Completed Image ====> 3. koi-pond-thinkstock-153560141-335sm61313.jpg\n",
      "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
      "Completed Image ====> 4. pkls4116_inline.png\n",
      "Image URL: https://pixnio.com/free-images/fauna-animals/reptiles-and-amphibians/alligators-and-crocodiles-pictures/alligator-animal-on-pond.jpg\n",
      "Completed Image ====> 5. alligator-animal-on-pond.jpg\n",
      "\n",
      "Errors: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {\"keywords\": \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
    "absolute_image_paths = response.download(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "si5YfNqS50QU"
   },
   "source": [
    "At time of writing at least a few do, but since the Internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is to validly run ResNet50 on the input images - don't worry about tuning or improving the model.\n",
    "\n",
    "*Hint* - ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
    "\n",
    "*Stretch goal* - also check for fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaT07ddW3nHz"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "def process_img_path(img_path):\n",
    "    return image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "\n",
    "def img_contains_frog(img):\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    model = ResNet50(weights='imagenet')\n",
    "    features = model.predict(x)\n",
    "    results = decode_predictions(features, top=3)[0]\n",
    "    print(results)\n",
    "    for entry in results:\n",
    "        if entry[1] == 'bullfrog' or 'tree frog' or 'tailed frog':\n",
    "            return entry[2]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'animal pond': ['C:\\\\git\\\\LSDS\\\\DS-Unit-4-Sprint-4-Deep-Learning\\\\downloads\\\\animal pond\\\\1. pondanimals.gif',\n",
       "  'C:\\\\git\\\\LSDS\\\\DS-Unit-4-Sprint-4-Deep-Learning\\\\downloads\\\\animal pond\\\\2. hqdefault.jpg',\n",
       "  'C:\\\\git\\\\LSDS\\\\DS-Unit-4-Sprint-4-Deep-Learning\\\\downloads\\\\animal pond\\\\3. koi-pond-thinkstock-153560141-335sm61313.jpg',\n",
       "  'C:\\\\git\\\\LSDS\\\\DS-Unit-4-Sprint-4-Deep-Learning\\\\downloads\\\\animal pond\\\\4. pkls4116_inline.png',\n",
       "  'C:\\\\git\\\\LSDS\\\\DS-Unit-4-Sprint-4-Deep-Learning\\\\downloads\\\\animal pond\\\\5. alligator-animal-on-pond.jpg']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'img_to_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1fd44ef3177d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mabsolute_image_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animal pond'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_img_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mimg_contains_frog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'has a frog in it.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-49170963bbac>\u001b[0m in \u001b[0;36mimg_contains_frog\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimg_contains_frog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'img_to_array'"
     ]
    }
   ],
   "source": [
    "procd_images = []\n",
    "for path in absolute_image_paths['animal pond']:\n",
    "    image = process_img_path(path)\n",
    "    if img_contains_frog(image):\n",
    "        print(image, 'has a frog in it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('downloads\\\\animal pond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='1. pondanimals.gif', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in procd_images:\n",
    "    print(image, img_contains_frog(process_img_path(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEuhvSu7O5Rf"
   },
   "source": [
    "## Part 3 - AutoML\n",
    "\n",
    "Use [TPOT](https://github.com/EpistasisLab/tpot) to fit a predictive model for the King County housing data, with `price` as the target output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "pz0e_7Ve60pM",
    "outputId": "7d7f644d-2edc-417b-bd7e-14d7044ef032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in c:\\users\\jhump\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (0.20.3)\n",
      "Requirement already satisfied: deap>=1.0 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (1.2.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (1.2.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (1.1.2)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (4.31.1)\n",
      "Requirement already satisfied: pandas>=0.20.2 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (1.16.2)\n",
      "Requirement already satisfied: update-checker>=0.16 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from tpot) (0.16)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from pandas>=0.20.2->tpot) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from pandas>=0.20.2->tpot) (2.8.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from update-checker>=0.16->tpot) (2.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.20.2->tpot) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\jhump\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "GflK25wp7jnf",
    "outputId": "a3e65568-5bfa-4b33-ca10-da515d8b418d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\jhump\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "!wget https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "7G5-Gqyg9A-V",
    "outputId": "1140fe22-952d-4745-d628-d91766af65b8"
   },
   "outputs": [],
   "source": [
    "!head kc_house_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KynXZjOY8hBL"
   },
   "source": [
    "As with previous questions, your goal is to run TPOT and successfully run and report error at the end.  Also, in the interest of time, feel free to choose small `generation=1` and `population_size=10` parameters so your pipeline runs efficiently and you are able to iterate and test.\n",
    "\n",
    "*Hint* - you'll have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running, as long as you still get a valid model with reasonable predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOREO8VJO7MZ"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_house_prices = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv')\n",
    "df = kc_house_prices.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living',\n",
       "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
       "       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
       "       'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape is (21613, 21)\n",
    "kc_house_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_house_prices = kc_house_prices.drop(['id', 'date', 'grade', 'lat', 'long'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price            0\n",
       "bedrooms         0\n",
       "bathrooms        0\n",
       "sqft_living      0\n",
       "sqft_lot         0\n",
       "floors           0\n",
       "waterfront       0\n",
       "view             0\n",
       "condition        0\n",
       "sqft_above       0\n",
       "sqft_basement    0\n",
       "yr_built         0\n",
       "yr_renovated     0\n",
       "zipcode          0\n",
       "sqft_living15    0\n",
       "sqft_lot15       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_house_prices.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 16 columns):\n",
      "price            21613 non-null float64\n",
      "bedrooms         21613 non-null int64\n",
      "bathrooms        21613 non-null float64\n",
      "sqft_living      21613 non-null int64\n",
      "sqft_lot         21613 non-null int64\n",
      "floors           21613 non-null float64\n",
      "waterfront       21613 non-null int64\n",
      "view             21613 non-null int64\n",
      "condition        21613 non-null int64\n",
      "sqft_above       21613 non-null int64\n",
      "sqft_basement    21613 non-null int64\n",
      "yr_built         21613 non-null int64\n",
      "yr_renovated     21613 non-null int64\n",
      "zipcode          21613 non-null int64\n",
      "sqft_living15    21613 non-null int64\n",
      "sqft_lot15       21613 non-null int64\n",
      "dtypes: float64(3), int64(13)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "kc_house_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = kc_house_prices.drop('price', axis=1)\n",
    "y = kc_house_prices.price\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b790cf270943d6bbe2e3ea121428fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=120, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -33709012362.469643\n",
      "Generation 2 - Current best internal CV score: -31000854418.422516\n",
      "Generation 3 - Current best internal CV score: -27986122886.222393\n",
      "Generation 4 - Current best internal CV score: -27986122886.222393\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "626zYgjkO7Vq"
   },
   "source": [
    "## Part 4 - More..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__lDWfcUO8oo"
   },
   "source": [
    "Answer the following questions, with a target audience of a fellow Data Scientist.  \n",
    "A few sentences per answer is fine - only elaborate if time allows.  \n",
    "- What do you consider your strongest area, as a Data Scientist?  \n",
    "A:  Understanding elementary concepts in machine learning and neural networks is my strongest area in data science. I enjoy using machine computing to create electronic classification models that draw on bagged and/or boosted decision tree ensembles. While I need to revisit my calculus studies, I believe I have a good\n",
    "initial grasp on backpropagation, and I love to consider activation flow.  \n",
    " \n",
    " \n",
    "- What area of Data Science would you most like to learn more about, and why?  \n",
    "A:  Logistic regression is the area. This is because I see so much of natural phenomena (including human\n",
    "societies) waxing and waning on a sigmoid function.  \n",
    "  \n",
    "  \n",
    "- Where do you think Data Science will be in 5 years?  \n",
    "A:  I listened closely when the LSDS Program Director told the DS01 cohort about one top SV rideshare company's\n",
    "take on predictive modeling--that it would be solved in the next 5 years or so. What I wonder about, more\n",
    "broadly, is if/when humans will enable machines to mechanistically evolve heuristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Hoqe3mM_Mtc"
   },
   "source": [
    "Thank you for your hard work, and congratulations! You've learned a lot, and should proudly call yourself a Data Scientist."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
