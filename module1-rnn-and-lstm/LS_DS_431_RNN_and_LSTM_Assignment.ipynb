{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [**infinite monkeys typing for an infinite amount of time**](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of William Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "We will focus specifically on Shakespeare's Sonnets in order to improve our model's ability to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-15T18:18:20.453Z",
     "iopub.status.busy": "2020-06-15T18:18:20.442Z",
     "iopub.status.idle": "2020-06-15T18:18:20.513Z",
     "shell.execute_reply": "2020-06-15T18:18:20.523Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# a custom data prep class that we'll be using \n",
    "from data_cleaning_toolkit_class import data_cleaning_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use request to pull data from a URL\n",
    "\n",
    "[**Read through the request documentation**](https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request) in order to learn how to download the Shakespeare Sonnets from the Gutenberg website. \n",
    "\n",
    "**Protip:** Do not over think it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ac79c2e9a53d747ebf8fb41f4b39340",
     "grade": false,
     "grade_id": "cell-b8ececfad1f60557",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# download all of Shakespears Sonnets from the Project Gutenberg website\n",
    "\n",
    "# here's the link for the sonnets\n",
    "url_shakespeare_sonnets = \"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
    "\n",
    "# use request and the url to download all of the sonnets - save the result to `r`\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab4f4f14188a9f3703d43d223bfa150",
     "grade": false,
     "grade_id": "cell-0cd0c8509bc8e8cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# move the downloaded text out of the request object - save the result to `raw_text_data`\n",
    "# hint: take at look at the attributes of `r`\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of `raw_text_data`\n",
    "type(raw_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, we are tasked with cleaning up messy data\n",
    "# Question: Do you see any characters that we could use to split up the text?\n",
    "raw_text_data[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13b66e41cc64459f0757f6f53a78e08f",
     "grade": false,
     "grade_id": "cell-916f742d2cea299a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# split the text into lines and save the result to `split_data`\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to drop all the boilder plate text (i.e. titles and descriptions) as well as white spaces\n",
    "# so that we are left with only the sonnets themselves \n",
    "split_data[:20] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use list index slicing in order to remove the titles and descriptions so we are only left with the sonnets.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00ead0a1024ff72116c24f6b473c1aac",
     "grade": false,
     "grade_id": "cell-1f388b88b0eec24a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sonnets exists between these indicies \n",
    "# titles and descriptions exist outside of these indicies\n",
    "\n",
    "# use index slicing to isolate the sonnet lines - save the result to `sonnets`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how all non-sonnet lines have far less characters than the actual sonnet lines?\n",
    "# well, let's use that observation to filter out all the non-sonnet lines\n",
    "sonnets[200:240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "649cf52260448a5faf539ad6b6e8e6e8",
     "grade": false,
     "grade_id": "cell-84c4b3cf1f3c032a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# any string with less than n_chars characters will be filtered out - save results to `filtered_sonnets`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok - much better!\n",
    "# but we still need to remove all the punctuation and case normalize the text\n",
    "filtered_sonnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom data cleaning tool \n",
    "\n",
    "Use one of the methods in `data_cleaning_toolkit` to clean your data.\n",
    "\n",
    "There is an example of this in the guided project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a722083a29139936744ff9a341e1c9a3",
     "grade": false,
     "grade_id": "cell-775c14b456d8a724",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the data_cleaning_toolkit class - save result to `dctk`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab91e612cd08068f3a36172979157d5d",
     "grade": false,
     "grade_id": "cell-684010b6a7360876",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use data_cleaning_toolkit to remove punctuation and to case normalize - save results to `clean_sonnets`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# much better!\n",
    "clean_sonnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your data tool to create character sequences \n",
    "\n",
    "We'll need the `create_char_sequenes` method for this task. However this method requires a parameter call `maxlen` which is responsible for setting the maximum sequence length. \n",
    "\n",
    "So what would be a good sequence length, exactly? \n",
    "\n",
    "In order to answer that question, let's do some statistics! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1deebea2ada0a7dc7d2eb08295ee1e2b",
     "grade": false,
     "grade_id": "cell-9ebdaa2654dd29ab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_stats(corpus):\n",
    "    \"\"\"\n",
    "    Calculates statisics on the length of every line in the sonnets\n",
    "    \"\"\"\n",
    "    \n",
    "    # write a list comprehension that calculates each sonnets line length - save the results to `doc_lens` \n",
    "\n",
    "    # use numpy to calcualte and return the mean, median, std, max, min of the doc lens - all in one line of code\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sonnet line length statistics \n",
    "mean ,med, std, max_, min_ = calc_stats(clean_sonnets)\n",
    "mean, med, std, max_, min_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "690957e46b6f2f32c1f17756d8ceab5b",
     "grade": false,
     "grade_id": "cell-35185e26897aad7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# using the results of the sonnet line length statistics, use your judgement and select a value for maxlen\n",
    "# use .create_char_sequences() to create sequences\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `data_cleaning_toolkit_class.py` file. \n",
    "\n",
    "In the first 4 lines of code in the `create_char_sequences` method, class attributes `n_features` and `unique_chars` are created. Let's call them in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input features for our LSTM model\n",
    "dctk.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique charactes that appear in our sonnets \n",
    "dctk.unique_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for Questions \n",
    "\n",
    "----\n",
    "**Question 1:** \n",
    "\n",
    "Why are the `number of unique characters` (i.e. **dctk.unique_chars**) and the `number of model input features` (i.e. **dctk.n_features**) the same?\n",
    "\n",
    "**Hint:** The model that we will shortly be building here is very similar to the text generation model that we built in the guided project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:**\n",
    "\n",
    "Our model is a classification model that, when given a sequence of characters, it will predict what character should come next. We need to ask ourselves, what are all the possible options? Well, every unique character that shows up in the sonnets is a possible option. \n",
    "\n",
    "Therefore our model is a multi-class classification model that outputs a probability score (recall that we are using the Softmax activation function in the output layer) for each of the unique character that it can predict as the character that should show up next in the sonnet line. \n",
    "\n",
    "`Number of unique characters` (i.e. **dctk.unique_chars**) and the `number of model input features` (i.e. **dctk.n_features**) are the same because all unique characters are a possible outcome for our model to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 2:**\n",
    "\n",
    "Take a look at the print out of `dctk.unique_chars` one more time. Notice that there is a white space. \n",
    "\n",
    "Why is it desirable to have a white space as a possible character to predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:**\n",
    "\n",
    "The model that we will be building shortly, will be given a a sequence of characters that make up a portion of a sonnet. It's task is to predict what characters should come next order to write sonnets of its own. In order for the model to be able to generate words, it needs to be able to include white spaces between non-white space characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our data tool to create X and Y splits\n",
    "\n",
    "You'll need the `create_X_and_Y` method for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: provide a walk through of data_cleaning_toolkit with unit tests that check for understanding \n",
    "X, y = dctk.create_X_and_Y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that our input matrix isn't actually a matrix - it's a rank 3 tensor\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $X$.shape we see three numbers (*n1*, *n2*, *n3*). What do these numbers mean?\n",
    "\n",
    "Well, *n1* tells us the number of samples that we have. But what about the other two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first index returns a signle sample, which we can see is a sequence \n",
    "first_sample_index = 0 \n",
    "X[first_sample_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each sequence (i.e. $X[i]$ where $i$ is some index value) is `maxlen` long and has `dctk.n_features` number of features. Let's try to better understand this shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each sequence is maxlen long and has dctk.n_features number of features\n",
    "X[first_sample_index].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each row corresponds to a character vector** and there are `maxlen` number of character vectors. \n",
    "\n",
    "**Each column corresponds to a unique character** and there are `dctk.n_features` number of features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's index for a single character vector \n",
    "first_char_vect_index = 0\n",
    "X[first_sample_index][first_char_vect_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is a single `TRUE` value and all the rest of the values are `FALSE`. \n",
    "\n",
    "This is a one-hot encoding for which character appears at each index within a sequence. Specifically, the cell above is looking at the first character in the sequence.\n",
    "\n",
    "Only a single character can appear as the first character in a sequence, so there will necessarily be a single `TRUE` value and the rest will be `FALSE`. \n",
    "\n",
    "Let's say that `TRUE` appears in the $ith$ index; by  $ith$ index we simply mean some index in the general case. How can we find out which character that actually corresponds to?\n",
    "\n",
    "To answer this question, we need to use the character-to-integer look up dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the index to character dictionary\n",
    "# if a TRUE appears in the 0th index of a character vector,\n",
    "# then we know that whatever char you see below next to the 0th key \n",
    "# is the character that that character vector is endcoding for\n",
    "dctk.int_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at an example to tie it all together\n",
    "\n",
    "seq_len_counter = 0\n",
    "\n",
    "# index for a single sample \n",
    "for seq_of_char_vects in X[first_sample_index]:\n",
    "    \n",
    "    # get index with max value, which will be the one TRUE value \n",
    "    index_with_TRUE_val = np.argmax(seq_of_char_vects)\n",
    "    \n",
    "    print (dctk.int_char[index_with_TRUE_val])\n",
    "    \n",
    "    seq_len_counter+=1\n",
    "    \n",
    "print (\"Sequence length: {}\".format(seq_len_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for Questions \n",
    "\n",
    "----\n",
    "**Question 1:** \n",
    "\n",
    "In your own words, how would you describe the numbers from the shape print out of `X.shape` to a fellow classmate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:**\n",
    "\n",
    "`X.shape` is a rank 3 tensor. \n",
    "\n",
    "Each of the numbers in this shape can be summerized as follows: \n",
    "\n",
    "`X.shape = (num of seqs, len of seqs, num of features)`\n",
    "\n",
    "`num of seqs`: number of sequence samples that we have created\n",
    "\n",
    "`len of seqs`: length of the sequences (i.e. number of characters in sequence)\n",
    "\n",
    "`num of features`: number of features for our model to predict (i.e. number of unique characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Text Generation model\n",
    "\n",
    "Now that we have prepped our data (and understood that process) let's finally build out our character generation model, similar to what we did in the guided project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Helper function to sample an index from a probability array\n",
    "    \"\"\"\n",
    "    # convert preds to array \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    # scale values \n",
    "    preds = np.log(preds) / temperature\n",
    "    # exponentiate values\n",
    "    exp_preds = np.exp(preds)\n",
    "    # this equation should look familar to you (hint: it's an activation function)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    # Draw samples from a multinomial distribution\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    # return the index that corresponds to the max probability \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"\"\n",
    "    Function invoked at end of each epoch. Prints the text generated by our model.\n",
    "    \"\"\"\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "\n",
    "    # randomly pick a starting index \n",
    "    # will be used to take a random sequence of chars from `text`\n",
    "    start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
    "    \n",
    "    # this is our seed string (i.e. input seqeunce into the model)\n",
    "    generated = ''\n",
    "\n",
    "    # start the sentence at index `start_index` and include the next` dctk.maxlen` number of chars\n",
    "    sentence = text[start_index: start_index + dctk.maxlen]\n",
    "\n",
    "    # add to generated\n",
    "    generated += sentence\n",
    "\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    # use model to predict what the next 40 chars should be that follow the seed string\n",
    "    for i in range(40):\n",
    "\n",
    "        # shape of a single sample in a rank 3 tensor \n",
    "        x_dims = (1, dctk.maxlen, dctk.n_features)\n",
    "        # create an array of zeros with shape x_dims\n",
    "        # recall that python considers zeros and boolean FALSE as the same\n",
    "        x_pred = np.zeros(x_dims)\n",
    "\n",
    "        # create a seq vector for our randomly select sequence \n",
    "        # i.e. create a numerical encoding for each char in the sequence \n",
    "        for t, char in enumerate(sentence):\n",
    "            # for sample 0 in seq index t and character `char` encode a 1 (which is the same as a TRUE)\n",
    "            x_pred[0, t, dctk.char_int[char]] = 1\n",
    "\n",
    "        # next, take the seq vector and pass into model to get a prediction of what the next char should be \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        # use the sample helper function to get index for next char \n",
    "        next_index = sample(preds)\n",
    "        # use look up dict to get next char \n",
    "        next_char = dctk.int_char[next_index]\n",
    "\n",
    "        # append next char to sequence \n",
    "        sentence = sentence[1:] + next_char \n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this for on_epoch_end()\n",
    "text = \" \".join(clean_sonnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create callback object that will print out text generation at the end of each epoch \n",
    "# use for real-time monitoring of model performance\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Train Model\n",
    "\n",
    "Build a text generation model using LSTMs. Feel free to reference the model used in the guided project. \n",
    "\n",
    "It is recommended that you train this model to at least 50 epochs (but more if you're computer can handle it). \n",
    "\n",
    "You are free to change up the architecture as you wish. \n",
    "\n",
    "Just in case you have difficultly training a model, there is a pre-trained model saved to a file called `trained_text_gen_model.h5` that you can load in (the same way that you learned how to load in Keras models in Sprint 2 Module 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e17312b57e17284124ce562dff81b00d",
     "grade": false,
     "grade_id": "cell-f34be90367fd9071",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# build text generation model layer by layer \n",
    "# fit model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model to file \n",
    "model.save(\"trained_text_gen_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with our trained model \n",
    "\n",
    "Now that we have a trained model that, though far from perfect, is able to generate actual English words, we can take a look at the predictions to continue to learn more about how a text generation model works. \n",
    "\n",
    "We can also take this as an opportunity to unpack the `def on_epoch_end` function to better understand how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our joinned clean sonnet data\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick a starting index \n",
    "# will be used to take a random sequence of chars from `text`\n",
    "# run this cell a few times and you'll see `start_index` is random\n",
    "start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next use the randomly selected starting index to sample a sequence from the `text`\n",
    "\n",
    "# this is our seed string (i.e. input seqeunce into the model)\n",
    "generated = ''\n",
    "\n",
    "# start the sentence at index `start_index` and include the next` dctk.maxlen` number of chars\n",
    "sentence = text[start_index: start_index + dctk.maxlen]\n",
    "\n",
    "# add to generated\n",
    "generated += sentence\n",
    "\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block of code let's us know what the seed string is \n",
    "# i.e. the input seqeunce into the model\n",
    "print('----- Generating with seed: \"' + sentence + '\"')\n",
    "sys.stdout.write(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model to predict what the next 40 chars should be that follow the seed string\n",
    "for i in range(40):\n",
    "\n",
    "    # shape of a single sample in a rank 3 tensor \n",
    "    x_dims = (1, dctk.maxlen, dctk.n_features)\n",
    "    # create an array of zeros with shape x_dims\n",
    "    # recall that python considers zeros and boolean FALSE as the same\n",
    "    x_pred = np.zeros(x_dims)\n",
    "\n",
    "    # create a seq vector for our randomly select sequence \n",
    "    # i.e. create a numerical encoding for each char in the sequence \n",
    "    for t, char in enumerate(sentence):\n",
    "        # for sample 0 in seq index t and character `char` encode a 1 (which is the same as a TRUE)\n",
    "        x_pred[0, t, dctk.char_int[char]] = 1\n",
    "\n",
    "    # next, take the seq vector and pass into model to get a prediction of what the next char should be \n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    # use the sample helper function to get index for next char \n",
    "    next_index = sample(preds)\n",
    "    # use look up dict to get next char \n",
    "    next_char = dctk.int_char[next_index]\n",
    "\n",
    "    # append next char to sequence \n",
    "    sentence = sentence[1:] + next_char "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the seed string\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the 40 chars that the model thinks should come after the seed stirng\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how put it all together\n",
    "generated + sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37  (Python3)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
