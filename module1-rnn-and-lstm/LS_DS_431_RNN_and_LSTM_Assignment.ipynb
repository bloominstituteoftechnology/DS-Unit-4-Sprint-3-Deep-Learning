{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lxml import html, etree\n",
    "import requests\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# scrapes webpage and creates html tree with lxml\n",
    "page = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
    "tree = html.fromstring(page.content)\n",
    "\n",
    "# sets all text to a variable\n",
    "text_big = tree.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes shorter shakespear collection for testing\n",
    "page_small = requests.get('https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt')\n",
    "tree_small = html.fromstring(page_small.content)\n",
    "\n",
    "# sets all text to a variable\n",
    "text_small = tree_small.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks text to use in model\n",
    "text = text_big\n",
    "\n",
    "# creates orderedlist of charecters in text\n",
    "chars = ' '.join(text)\n",
    "\n",
    "# gets list of unique charecters\n",
    "unique_chars = list(set(chars))\n",
    "\n",
    "# creates a dictionary to map number to charecter\n",
    "num_to_char = dict(enumerate(unique_chars))\n",
    "\n",
    "# creates dictonary maping char to num\n",
    "char_to_num = {value: key for key, value in num_to_char.items()}\n",
    "\n",
    "# encodes text\n",
    "text_encoded = [char_to_num[i] for i in text_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "iters = 3\n",
    "sequence_length = 40 # how far back to look\n",
    "batch_size = round((len(text_encoded) / sequence_length) + 0.5) # gets batch size based on sequence length\n",
    "hidden_size = 100  # number of hidden nodes\n",
    "learning_rate = 0.01\n",
    "num_chars = len(unique_chars)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "W_xh = np.random.randn(hidden_size, num_chars) * 0.01     # input to hidden weight\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01   # hidden to hidden weight\n",
    "W_hy = np.random.randn(num_chars, hidden_size) * 0.01     # hidden to output weight\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size, 1)) # not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes forward/backprop from lecture\n",
    "\n",
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # sets values equal to empty dict\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code     \n",
    "\n",
    "    return loss, ps, hs, xs\n",
    "\n",
    "\n",
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 56.560689\n",
      "iter 1, loss: 55.672149\n",
      "iter 2, loss: 52.398502\n",
      "CPU times: user 1min 37s, sys: 5min 50s, total: 7min 27s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iters):\n",
    "    h_prev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_num[ch] for ch in chars[data_pointer: data_pointer + sequence_length]]\n",
    "        targets = [char_to_num[ch] for ch in chars[data_pointer + 1: data_pointer + sequence_length + 1]] # t+1        \n",
    "            \n",
    "        if (data_pointer + sequence_length + 1 >= len(chars) and b == batch_size - 1): # processing of the last part of the input data. \n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "        # forward prop\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "\n",
    "        # backward prop\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 1 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c t   G u t e n b e r g â€™ s   T h e   C \n"
     ]
    }
   ],
   "source": [
    "# checking the value of input and target\n",
    "\n",
    "data_pointer=0\n",
    "\n",
    "test = [char_to_num[ch] for ch in chars[data_pointer+10: data_pointer + sequence_length +10]]\n",
    "\n",
    "words = []\n",
    "for i in test:\n",
    "    word = num_to_char[i]\n",
    "    words.append(word)\n",
    "    \n",
    "string = ''\n",
    "for i in words:\n",
    "    string += i\n",
    "    \n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_num[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(num_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " a n h S n f   a c e g p   e e   \n",
      " i p o \n",
      " o s a \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('a', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks text to use in model\n",
    "text = text_small\n",
    "# creates orderedlist of charecters in text\n",
    "chars = ' '.join(text_small)\n",
    "\n",
    "# gets list of unique charecters\n",
    "unique_chars = list(set(chars))\n",
    "\n",
    "# creates a dictionary to map number to charecter\n",
    "num_to_char = dict(enumerate(unique_chars))\n",
    "\n",
    "# creates dictonary maping char to num\n",
    "char_to_num = {value: key for key, value in num_to_char.items()}\n",
    "\n",
    "# encodes text\n",
    "text_encoded = [char_to_num[i] for i in text_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 33318\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"That, poor contempt, or claim'd thou sle\",\n",
       " \"t, poor contempt, or claim'd thou slept \",\n",
       " \"poor contempt, or claim'd thou slept so \",\n",
       " \"r contempt, or claim'd thou slept so fai\",\n",
       " \"ontempt, or claim'd thou slept so faithf\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks at sentences\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 's', 'f', 't', 'u']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks at next chars\n",
    "next_chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "x = np.zeros((len(sentences), maxlen, len(unique_chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(unique_chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_num[char]] = 1\n",
    "    y[i, char_to_num[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 62)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x[0][0].shape)\n",
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0] # just realized we are using last 40 words (x) as input to predict next letter (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(unique_chars))))\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33318 samples\n",
      "Epoch 1/10\n",
      "33318/33318 [==============================] - 5s 164us/sample - loss: 2.5773\n",
      "Epoch 2/10\n",
      "33318/33318 [==============================] - 2s 74us/sample - loss: 2.0644\n",
      "Epoch 3/10\n",
      "33318/33318 [==============================] - 2s 72us/sample - loss: 1.8842\n",
      "Epoch 4/10\n",
      "33318/33318 [==============================] - 2s 73us/sample - loss: 1.7514\n",
      "Epoch 5/10\n",
      "33318/33318 [==============================] - 2s 73us/sample - loss: 1.6428\n",
      "Epoch 6/10\n",
      "33318/33318 [==============================] - 2s 73us/sample - loss: 1.5451\n",
      "Epoch 7/10\n",
      "33318/33318 [==============================] - 2s 73us/sample - loss: 1.4587\n",
      "Epoch 8/10\n",
      "33318/33318 [==============================] - 2s 74us/sample - loss: 1.3908\n",
      "Epoch 9/10\n",
      "33318/33318 [==============================] - 2s 73us/sample - loss: 1.3205\n",
      "Epoch 10/10\n",
      "33318/33318 [==============================] - 2s 74us/sample - loss: 1.2557\n"
     ]
    }
   ],
   "source": [
    "# fit model and set output to history var\n",
    "history = model.fit(x, \n",
    "                    y,\n",
    "                    batch_size=128,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' sweet good stocks, and marry with her w'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = text[start_index: start_index + maxlen]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.zeros((1, maxlen, len(unique_chars)))\n",
    "for t, i in enumerate(sentence):\n",
    "    x_pred[0, t, char_to_num[i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40, 62)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_prediction = model.predict(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the probabilities for each charecter being the next char given x pred as the start\n",
    "sm_prediction[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.amax(sm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.64090338e-05, 2.11246149e-08, 3.02720146e-05, 8.64725189e-06,\n",
       "        1.22183735e-07, 3.47687751e-06, 2.44201624e-06, 1.47042971e-03,\n",
       "        6.42986242e-06, 5.63208487e-05, 2.20692164e-05, 5.37290752e-01,\n",
       "        4.49101748e-07, 3.75856034e-04, 1.80732549e-08, 4.33581328e-04,\n",
       "        6.69973160e-05, 2.66016570e-10, 2.66223597e-05, 4.39468586e-08,\n",
       "        1.20934684e-08, 5.80301332e-07, 1.47466994e-06, 4.26553470e-07,\n",
       "        1.69343313e-08, 3.88917059e-08, 4.06579602e-06, 2.22693441e-09,\n",
       "        1.54655045e-05, 3.26412510e-06, 1.28146223e-06, 2.60664383e-03,\n",
       "        5.12294901e-05, 3.83335390e-08, 7.40756789e-08, 4.47359198e-04,\n",
       "        1.52123945e-07, 2.96765083e-05, 1.29370292e-05, 6.91521564e-06,\n",
       "        1.92814216e-04, 1.90886418e-09, 3.97080324e-09, 4.52901393e-01,\n",
       "        4.21837103e-06, 9.61817932e-07, 8.10102210e-06, 3.60662307e-06,\n",
       "        1.00000000e+00, 1.23759073e-05, 1.31652132e-01, 8.24770936e-08,\n",
       "        5.42856753e-07, 1.21815000e-02, 4.03077956e-06, 1.33594112e-05,\n",
       "        3.24429711e-04, 9.62623048e-08, 6.33794480e-06, 6.57838484e-07,\n",
       "        2.84777911e-07, 1.81492624e-07]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_prediction/best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
