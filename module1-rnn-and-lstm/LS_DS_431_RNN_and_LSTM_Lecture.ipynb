{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 95s 4ms/sample - loss: 0.4470 - accuracy: 0.7926 - val_loss: 0.3753 - val_accuracy: 0.8342\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 101s 4ms/sample - loss: 0.2924 - accuracy: 0.8832 - val_loss: 0.3767 - val_accuracy: 0.8346\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 104s 4ms/sample - loss: 0.2024 - accuracy: 0.9212 - val_loss: 0.4327 - val_accuracy: 0.8259\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.1454 - accuracy: 0.9460 - val_loss: 0.4677 - val_accuracy: 0.8238\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 106s 4ms/sample - loss: 0.1011 - accuracy: 0.9642 - val_loss: 0.6101 - val_accuracy: 0.8271\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-916e4f82f497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot training & validation loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some recent headlines from schools around the country: In Indiana, officials played a segment of a 911 call of a teacher in a panic during the Columbine High School shooting to students. In Ohio, officers fired blank shots during an active-shooter drill. In South Carolina, an officer dressed in black posed as an intruder on an unannounced drill. In Michigan, a school is spending $48 million on a renovation that includes curved hallways and hiding niches, in hopes of protecting students from a mass shooting. In Florida, a police officer arrested two 6-year-old students for misdemeanor battery. In Colorado, teachers received buckets and kitty litter for students to use as toilets in case of a prolonged school lockdown.\\n\\nMass shootings, meaning incidents with at least two deaths, in schools are horrifying. But it is highly unlikely that a child would ever witness one. Research indicates that some security measures brought in to make schools safer — like realistic shooter trainings — may be causing children more harm than good.\\n\\nIt is 10 times more likely that a student will die on the way to school.\\n\\nOur chances of dying in a fire are also much greater — 1 in 1,500. But we don’t overreact.\\n\\nMore children have died from lightning strikes than from mass shootings in schools in the past 20 years. Still, we don’t obsess about them.\\n\\nExactly how common are school shootings?\\n\\nIn the two decades since Columbine, there have been 10 mass shootings in schools according to a recent analysis by James Alan Fox, a professor of criminology at Northeastern University who has been studying school violence for several decades. In total, 81 people have been killed, 64 of them students. That’s an average of four deaths per year, three of them students.\\n\\nEven one death is too many. But for perspective, 729 children committed suicide with a firearm in 2017, and 863 were victims of homicides by guns that year.\\n\\nSchool-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides in 2017 Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University.\\n\\nNearly every public school in the country now conducts lockdown drills, and even the youngest students participate (last year, one school adapted a lullaby to prepare kindergartners). But very few studies have looked into the efficiency of these drills. One of them concluded that the practice can be helpful to teach students basic safety procedures. But to the author of the study, Jaclyn Schildkraut, an associate professor at the State University of New York at Oswego, there is no point in dramatizing the drills. “All that causes is fear,” she said.\\n\\nRestaurants have 10 times as many homicides as schools. Why do we want to arm teachers and not wait staffs?\\n\\n“There’s a misunderstanding in where the dangers are,” said Dewey G. Cornell, a psychologist and professor at the University of Virginia. “Kids are at far greater danger going to and from school, than they are in the classroom,” he said. “School counseling, academic support, that’s gonna do far more to keep our communities safe.”\\n\\nUnlike the United States, the other wealthy countries in the Group of Seven don’t do lockdown drills and rarely have school shootings. What is the United States doing that is so different from them?\\n\\nGun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017.\\n\\nMany researchers think easy access to guns is an important part of the problem. “Violence in schools is just a small part of the larger problem of gun violence in our society,” Cornell wrote in a statement about prevention of violence in schools and communities.\\n\\nMisguided safety measures, such as dramatized lockdown drills, may give us the impression that we are protecting children, when, in fact, we are handing them a burden that adults are failing to address.\\n\\nRead more:\\n\\n‘What if someone was shooting?’\\n\\nThey grew up practicing lockdown drills. Now they’re steering the conversation on gun violence.\\n\\nSchool shootings are extraordinarily rare. Why is fear of them driving policy?\\n\\nPutting more cops in schools won’t make schools safer, and it will likely inflict a lot of harm'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 34,\n",
       " 111,\n",
       " 47,\n",
       " 80,\n",
       " 73,\n",
       " 108,\n",
       " 94,\n",
       " 47,\n",
       " 73,\n",
       " 111,\n",
       " 6,\n",
       " 15,\n",
       " 24,\n",
       " 34,\n",
       " 95,\n",
       " 94,\n",
       " 1,\n",
       " 111,\n",
       " 73,\n",
       " 28,\n",
       " 47,\n",
       " 57,\n",
       " 57,\n",
       " 56,\n",
       " 120,\n",
       " 65,\n",
       " 15,\n",
       " 82,\n",
       " 34,\n",
       " 94,\n",
       " 28,\n",
       " 65,\n",
       " 15,\n",
       " 73,\n",
       " 28,\n",
       " 15,\n",
       " 34,\n",
       " 111,\n",
       " 15]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.5915\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"n such changes are made, we will make a \"\n",
      "n such changes are made, we will make a deens enthint, rave and in megee s.. Ohe. AuH follis aso9 tuwhimg he erssing pounen arcendardass cfacents (used: prethirvand V ondey wiss afdiad bave siopeenS hes at,9e ptro ans ron anirB Connrmseadpar tion to thouth-ones bamavy andenof lyon unt resits seatior a. Ne thard aldeos’s o. Chins, Shemrace share ste, ina. “ITtrummer yom hig in wevire man rathonn.\n",
      "\n",
      "AA Ceally gorluiex, caotren. Vof hali: w\n",
      "178374/178374 [==============================] - 153s 860us/sample - loss: 2.5915\n",
      "Epoch 2/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 2.2482\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \" rivals. That’s when Pompeo lost his coo\"\n",
      " rivals. That’s when Pompeo lost his coottal oss of ay thes a With extaincaded wharkats, lomiticy tiem more helle seiny and pingntertan, loa covents, to whangenter, at pand hes toy hove stwar meat in underiondy wo morwventer the getren ffit Nty th Thil hind courd motighs couptidn ley has — wred miple zof leAd. “I “ohis a ptavested Hourg. Ho leston. On wathen it the hame, thach for a amica, appsaded bue momponced exean, Poneat.\n",
      "\n",
      "Bis the \n",
      "178374/178374 [==============================] - 165s 926us/sample - loss: 2.2481\n",
      "Epoch 3/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 2.1148\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"controlled with U.S. protection.\n",
      "\n",
      "For ma\"\n",
      "controlled with U.S. protection.\n",
      "\n",
      "For matice ay Agroed, ane praties buthergs orel asterstri, “Sy. She areend D. Anp Wash fort-Dbach: Thes sead himm,” inchibeds searment Greandverrens sureena” Trounap hass'; [walkne inflapt thas Saund. Gean’s hoou, had recested and Schist its bart dos, Gkronga Traters ffonmem, hask and ittont soud “Bith Onered millow, we?d’ cand lack enerthing, by his miss, .throped the the mocrail:\n",
      "\n",
      "He’s ored bece ssanr\n",
      "178374/178374 [==============================] - 161s 902us/sample - loss: 2.1148\n",
      "Epoch 4/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 2.0203\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"sis of previously unpublished Russian Ai\"\n",
      "sis of previously unpublished Russian Aimans and onono, the sork the 104-paib the warted jorbame ray villowsee and whine enting ofly aconger linchy appelowe. Oclicudan-witn abord be Wi1e Lare.\n",
      "\n",
      "Ensplitice to we him in moch, “He Porker.”\n",
      "\n",
      "Ond mord in a oldmerations. Nof so hes lis for its to to geinging there pabcent on cringhrrwise to as deable was aush from Hinging CTimpos’s’s not Arrund, Buth Dacht’s inllapthers fer well an (ampy for \n",
      "178374/178374 [==============================] - 145s 812us/sample - loss: 2.0203\n",
      "Epoch 5/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 1.9509\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"ow to Catch a Mole,” could be used as a \"\n",
      "ow to Catch a Mole,” could be used as a bumides wore right rotand of cloppers. Aised the sime, the Tunniftering, the theaggf cortifild to Eniarays Retsome for armpation, and on of meres confort. Shindosbanr vy exaratil are hout theyo, duttorcy properts ans for is to have “meke and goted has simention. 1920 manyther. “Eliter comelices” that to retornse gold and a fooplating a fext to readtia and detal toper modiinste thad mphougl,” Bull,\n",
      "178374/178374 [==============================] - 140s 785us/sample - loss: 1.9508\n",
      "Epoch 6/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.8922\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"ery where Parnas and Fruman got their mo\"\n",
      "ery where Parnas and Fruman got their moted a plafe for year-in the presfrnqures a acreasering “Warral deficiched “Fle reporterd murtaresines. Ale pease reare that fial was tome in at tre idso an ucher itstaching to gem timed.”\n",
      "\n",
      "It bepationsing shen protes to phare deforiticall elage, collom, in Pustelpag’s Beaush finter,” fromberviof.\n",
      "\n",
      "Pombriath. Areet amooters — want al gemina dunt of the tagneling the Dive among net-undiral anizy wea\n",
      "178374/178374 [==============================] - 124s 696us/sample - loss: 1.8921\n",
      "Epoch 7/10\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8437\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \" Franken asked Perry during his confirma\"\n",
      " Franken asked Perry during his confirmanined in prezair CchingD\n",
      "\n",
      "Hade looking hitthervionals doom. he rast the mits expecties the Kanded.\n",
      "\n",
      "Fo yort to coues on wepted bitsseed, by: hermakings, orle, and plesent cast, the Eversione, Trump dizing the sais, ham oner. Averyres reiarks aroinmently know has is fall-lass. Dephodem of status, alunt a term $r. holding, foveraur esolows up takes team and ferm. Retain stacks and may nolding laticl\n",
      "178374/178374 [==============================] - 116s 653us/sample - loss: 1.8436\n",
      "Epoch 8/10\n",
      "178368/178374 [============================>.] - ETA: 0s - loss: 1.8024\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"n Oct. 23.\n",
      "\n",
      "EXTRA MILEAGE\n",
      "\n",
      "— After a str\"\n",
      "n Oct. 23.\n",
      "\n",
      "EXTRA MILEAGE\n",
      "\n",
      "— After a streat cailon’ the syic inarcestion of and Lezance: “And Rulligns was id oor topk bread to next the sell country and knolbore oney wit call allack stace oone as agnoupril from to 20 perceminisive of eney fon itch ot mened invescensent that “Hilpine Trump Heratement forluge. We Bose” Hornex do ksigg be,”’ mated ferching in Grit in Sun mode of calles for drempine My, (mench/, to me polity to the propte\n",
      "178374/178374 [==============================] - 118s 659us/sample - loss: 1.8024\n",
      "Epoch 9/10\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7651\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"s Day rather than Columbus Day has been \"\n",
      "s Day rather than Columbus Day has been into solver. Aolless A2, Morogn 2019, 63 remerancy to Amber millies at jo eventy from hombay’ frum new theid my laws chouder “Sourne’s bakesive and by firmn to abrial quered Jyo, bush people with fally, lovis and drimined on prone’t Inventine Rujortable\n",
      "\n",
      "THE AD— Y PFY AdES AMPEP CO BE USERENE SISESS IN ALIRLE TEESTOS ATIES CO ORRARTINT IRAS ANPS OR ANWANCTONTOMTCOSITET.LADICThINITITS OR THE SERESR\n",
      "178374/178374 [==============================] - 115s 647us/sample - loss: 1.7652\n",
      "Epoch 10/10\n",
      "178336/178374 [============================>.] - ETA: 0s - loss: 1.7305\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"n a manner that violates any laws. For e\"\n",
      "n a manner that violates any laws. For evences the wollowizg the lowed to Indayman than’s sufficual cleagended a dorish boed the sodient. Ondanalt, docher, theerge arnaws hebds to now, in but scured his a merilf cames. “Holl. Reballt fanerated may newher that my his I rece impeoded here.]\n",
      "\n",
      "“Hat Dail, the 45 last it’s “Essocks,, his deplan hempaned lifensing treading townized rut we’s downging, with at-rements pocigity tinune.\n",
      "\n",
      "6. The tw\n",
      "178374/178374 [==============================] - 116s 648us/sample - loss: 1.7304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9312a8fe10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NNF-DS10",
   "language": "python",
   "name": "u4-s2-nnf-ds10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
