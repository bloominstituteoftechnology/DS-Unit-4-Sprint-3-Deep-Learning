{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 4 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_m0hJ4uCzHz"
   },
   "source": [
    "## Time series with plain old regression\n",
    "\n",
    "Recurrences are fancy, and we'll get to those later - let's start with something simple. Regression can handle time series just fine if you just set them up correctly - let's try some made-up stock data. And to make it, let's use a few list comprehensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkJUFfsgnqr_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "days = np.array((range(28)))\n",
    "stock_quotes = np.array([random() + day * random() for day in days]) #autocorrelating time and stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "y-ORgKGNBOcb",
    "outputId": "133809e1-8588-4acb-f07e-20dfefcd03ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.53770288,  0.86336839,  0.73517059,  3.1922686 ,  4.17708386,\n",
       "        0.25692901,  2.58155067,  2.75102524,  0.72352409,  8.03285774,\n",
       "        1.15896662,  5.35135391,  9.00617763, 10.07242455, 13.55582162,\n",
       "        1.07495833, 12.00260954, 16.42329937,  6.61118778,  1.34368721,\n",
       "       20.1701102 , 19.95614621, 14.018627  , 21.77836078,  7.09015898,\n",
       "        8.52873082, 17.86113645, 24.38526919])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3lR2wGvBx3a"
   },
   "source": [
    "Let's take a look with a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "pVUTC2tmBSIq",
    "outputId": "75664a71-713d-4815-d4cc-2055f485784a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd64853a9b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import scatter\n",
    "scatter(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgD4q-T_B0jd"
   },
   "source": [
    "Looks pretty linear, let's try a simple OLS regression.\n",
    "\n",
    "First, these need to be NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3Q0MrnUBXAl"
   },
   "outputs": [],
   "source": [
    "days = days.reshape(-1, 1)  # X needs to be column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIhsbC4o9fBb",
    "outputId": "6e06e1fd-50ec-4dc6-d4f8-b49b1aba19b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19],\n",
       "       [20],\n",
       "       [21],\n",
       "       [22],\n",
       "       [23],\n",
       "       [24],\n",
       "       [25],\n",
       "       [26],\n",
       "       [27]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqr0SHOnB5yR"
   },
   "source": [
    "Now let's use good old `scikit-learn` and linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PqyHxgFvBYl5",
    "outputId": "0d4a183e-fdb3-4e97-c8be-ab31a82b07ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5629179757086855"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ols_stocks = LinearRegression()\n",
    "ols_stocks.fit(days, stock_quotes)\n",
    "ols_stocks.score(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlU0mr-KB_Yk"
   },
   "source": [
    "That seems to work pretty well, but real stocks don't work like this.\n",
    "\n",
    "Let's make *slightly* more realistic data that depends on more than just time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FV1Emb2BuLz"
   },
   "outputs": [],
   "source": [
    "# Not everything is best as a comprehension\n",
    "stock_data = np.empty([len(days), 4])\n",
    "for day in days:\n",
    "  asset = random()\n",
    "  liability = random()\n",
    "  quote = random() + ((day * random()) + (20 * asset) - (15 * liability))\n",
    "  quote = max(quote, 0.01)  # Want positive quotes\n",
    "  stock_data[day] = np.array([quote, day, asset, liability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "6Qe2zzN1CESe",
    "outputId": "5b0cafbf-3a47-40cf-8bc3-38f2dc084e91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.67439053e+00, 0.00000000e+00, 5.34875620e-01, 5.86616876e-01],\n",
       "       [1.25681520e+01, 1.00000000e+00, 6.78720146e-01, 1.41501098e-01],\n",
       "       [1.16998056e+01, 2.00000000e+00, 5.95429304e-01, 3.29680218e-02],\n",
       "       [9.95074324e+00, 3.00000000e+00, 5.62328518e-01, 1.97504565e-01],\n",
       "       [9.40010447e-02, 4.00000000e+00, 3.07725275e-01, 6.91854742e-01],\n",
       "       [7.99280665e+00, 5.00000000e+00, 4.50402992e-01, 3.65218833e-01],\n",
       "       [1.00000000e-02, 6.00000000e+00, 1.59304651e-01, 5.96986498e-01],\n",
       "       [1.22719078e+01, 7.00000000e+00, 8.24836632e-01, 4.11131643e-01],\n",
       "       [1.04421213e+01, 8.00000000e+00, 9.38226678e-01, 9.80541136e-01],\n",
       "       [2.08508154e+00, 9.00000000e+00, 1.91901871e-01, 4.26912208e-01],\n",
       "       [1.00000000e-02, 1.00000000e+01, 5.45471422e-02, 7.53401316e-01],\n",
       "       [4.54133234e+00, 1.10000000e+01, 1.70183958e-01, 2.70223220e-01],\n",
       "       [2.12519504e+01, 1.20000000e+01, 5.84000513e-01, 1.57443012e-01],\n",
       "       [1.06073773e+01, 1.30000000e+01, 9.58667424e-01, 7.09188048e-01],\n",
       "       [1.01422061e+01, 1.40000000e+01, 3.51241787e-02, 2.03379877e-01],\n",
       "       [2.12666110e+00, 1.50000000e+01, 2.72764276e-01, 8.87969845e-01],\n",
       "       [1.77979189e+01, 1.60000000e+01, 8.19396127e-01, 8.27890072e-01],\n",
       "       [1.00000000e-02, 1.70000000e+01, 2.58979716e-02, 6.38358064e-01],\n",
       "       [1.13824669e+01, 1.80000000e+01, 1.77212616e-01, 7.15664369e-01],\n",
       "       [4.81969073e+00, 1.90000000e+01, 2.53691880e-01, 2.88329872e-01],\n",
       "       [1.20339250e+01, 2.00000000e+01, 1.48014178e-01, 4.92935501e-03],\n",
       "       [2.79005318e+01, 2.10000000e+01, 9.73719082e-01, 5.54442828e-01],\n",
       "       [1.43517783e+01, 2.20000000e+01, 7.18076481e-01, 8.41411203e-02],\n",
       "       [8.10097331e+00, 2.30000000e+01, 4.05463859e-01, 6.48652325e-01],\n",
       "       [2.21274217e+01, 2.40000000e+01, 1.44275577e-01, 2.44576770e-01],\n",
       "       [2.42863607e+01, 2.50000000e+01, 7.87347213e-01, 6.72939677e-01],\n",
       "       [1.00000000e-02, 2.60000000e+01, 1.00077711e-01, 9.36401463e-01],\n",
       "       [1.45413362e+01, 2.70000000e+01, 7.20771395e-01, 9.97915100e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzYy4Pb2CLCh"
   },
   "source": [
    "Let's look again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "qdBcScz4CIXr",
    "outputId": "ad3ec81f-8fba-4355-d251-630ba5fc333c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAQ+ElEQVR4nO3db2hldX7H8c+3aQoXFaJMlJnU6ViRUFmpWS5SmLLMdulGpWAUXDqF7RQWZh8oaJFQs0/0SXHYrG77oAhjHXYK1lYwG6VKs6KC9YndjHHNuEO6yzJrvTPMRNygCxc6xm8f5MRMMjfJ/XPOPed7zvsFQ5Jfbrzfcw/34zm/87vfY+4uAEA8v5N3AQCA7hDgABAUAQ4AQRHgABAUAQ4AQf1uP59sz549fuDAgX4+JQCEd+rUqY/dfXjreF8D/MCBA5qfn+/nUwJAeGb261bjTKEAQFAEOAAERYADQFAEOAAERYADQFB9XYUCoPxmFxqanlvSuZWm9g3VNDk+qomxkbzLKiUCHEBqZhcamppZVPPSqiSpsdLU1MyiJBHiGWAKBUBqpueWvgzvdc1Lq5qeW8qponIjwAGk5txKs6Nx9IYAB5CafUO1jsbRGwIcQGomx0dVGxzYNFYbHNDk+GhOFZUbFzEBpGb9QiWrUPqDAAeQqomxEQK7T5hCAYCgCHAACIoAB4CgCHAACIoAB4CgCHAACIoAB4CgCHAACIoAB4CgCHAACIoAB4CgCHAACIoAB4Cgdg1wM7vRzN40szNm9oGZPZSMP25mDTN7L/l3d/blAgDWtdNO9nNJj7j7u2Z2jaRTZvZa8rsfuvsPsisPALCdXQPc3c9LOp98/5mZnZFEs18AyFlHc+BmdkDSmKR3kqEHzex9MzthZtdu8zdHzWzezOaXl5d7KhYAsKHtADezqyW9KOlhd/9U0tOSbpZ0u9aO0J9s9Xfuftzd6+5eHx4eTqFkAIDU5i3VzGxQa+H9nLvPSJK7X7js989I+o9MKgSAjM0uNELex3PXADczk/SspDPu/tRl43uT+XFJulfS6WxKBIDszC40NDWzqOalVUlSY6WpqZlFSSp8iLczhXJQ0rcl/dmWJYPfN7NFM3tf0tcl/W2WhQJAFqbnlr4M73XNS6uanlvKqaL2tbMK5W1J1uJXr6ZfDgD017mVZkfjRcInMQFU2r6hWkfjRUKAA6i0yfFR1QYHNo3VBgc0OT6aU0Xta2sVCgCU1fqFylKuQgGAspsYGwkR2FsxhQIAQRHgABAUAQ4AQRHgABAUAQ4AQRHgABAUAQ4AQbEOHEApRW0R2wkCHEDpRG4R2wmmUACUTuQWsZ0gwAGUTuQWsZ0gwAGUTuQWsZ0gwAGUTuQWsZ3gIibCqsIqA3QncovYThDgCKkqqwzQvagtYjvBFApCqsoqA2AnBDhCqsoqA2AnBDhCqsoqA2AnBDhCqsoqA2AnXMRESFVZZQDshABHWFVYZQDshCkUAAhq1wA3sxvN7E0zO2NmH5jZQ8n4dWb2mpn9Ivl6bfblAgDWtXME/rmkR9z9jyT9iaQHzOxWSY9Ket3db5H0evIzAKBPdg1wdz/v7u8m338m6YykEUn3SDqZPOykpImsigQAXKmjOXAzOyBpTNI7km5w9/PSWshLun6bvzlqZvNmNr+8vNxbtQCAL7Ud4GZ2taQXJT3s7p+2+3fuftzd6+5eHx4e7qZGAEALbQW4mQ1qLbyfc/eZZPiCme1Nfr9X0sVsSgQAtNLOKhST9KykM+7+1GW/elnSkeT7I5JeSr88AMB22vkgz0FJ35a0aGbvJWPfk3RM0gtm9h1JH0q6P5sSAQCt7Brg7v62JNvm199ItxwAQLv4JCYABEWAA0BQBDgABEWAA0BQBDgABEWAA0BQBDgABMUdeYA+m11ocCs4pIIAB/podqGhqZlFNS+tSpIaK01NzSxKEiGOjjGFAvTR9NzSl+G9rnlpVdNzSzlVhMgIcKCPzq00OxoHdkKAA320b6jW0TiwEwIc6KPJ8VHVBgc2jdUGBzQ5PppTRYiMi5hAH61fqGQVCtJAgAN9NjE2QmAjFUyhAEBQBDgABEWAA0BQBDgABEWAA0BQBDgABEWAA0BQBDgABEWAA0BQBDgABEWAA0BQBDgABLVrgJvZCTO7aGanLxt73MwaZvZe8u/ubMsEAGzVzhH4jyTd2WL8h+5+e/Lv1XTLAgDsZtcAd/e3JH3Sh1oAAB3opR/4g2b215LmJT3i7r9p9SAzOyrpqCTt37+/h6cDgFhmFxqZ3ryj24uYT0u6WdLtks5LenK7B7r7cXevu3t9eHi4y6cDUEazCw0dPPaGbnr0FR089oZmFxp5l5Sa2YWGpmYW1VhpyiU1VpqamllMdRu7CnB3v+Duq+7+haRnJN2RWkUAKqEfAZen6bklNS+tbhprXlrV9NxSas/RVYCb2d7LfrxX0untHgsArfQj4PJ0bqXZ0Xg3dp0DN7PnJR2StMfMPpL0mKRDZna7JJd0VtJ3U6sIQCX0I+DytG+opkaLbdk3VEvtOXYNcHc/3GL42dQqAFBJ/Qi4PE2Oj2pqZnHTWUZtcECT46OpPQefxASQi8nxUdUGBzaNpR1weZoYG9ET992mkaGaTNLIUE1P3HdbqqtQellGCABdWw+yLJfZ5W1ibCTT7SHAAeQm64ArO6ZQACAoAhwAgiLAASAoAhwAgiLAASCo0q1Cybr7FwAURakCfL05zvonn9ab40gixAGUTqkCfKfmOFUMcM5GgHIrVYCXvTlOJzgbAcqvVBcxt2uCU5bmOJ0oe6tO9FeZb7wQWakCvOzNcTrB2QjSUvYbL0RWqgDvtPtXmY8qOBtBWjibK65SzYFL7TfHKfsccT96EaMaOJsrrlIdgXei7EcV/ehFjGLJ6oySs7niKt0ReLuqcFRBq87qyPKMkrO54qrsEThHFSiTLM8oOZsrrsoegXNUgTLJ+oySs7liquwROEcVKBPOKKupskfgEkcVKA/OKKup0gGOailzb5gq3CAYVyLAIanc4SaVf92/xBllFVV2DhwbqvBR6bKv+0c1cQSOSrTh7XSVRtnPSFAOuwa4mZ2Q9BeSLrr7V5Kx6yT9u6QDks5K+pa7/ya7MvNX5jd0FT7UtG+opkaL7Wm1SqMK0y0oh3amUH4k6c4tY49Ket3db5H0evJzaZV9iqEKS9A66VTJdAui2DXA3f0tSZ9sGb5H0snk+5OSJlKuq1DK/oauQhveTtb9V+GMBOXQ7Rz4De5+XpLc/byZXb/dA83sqKSjkrR///4uny5fZX9DV2UJWrurNDqZbgHylPlFTHc/Lum4JNXrdc/6+bJQhTc0S9A28KEYRNHtMsILZrZXkpKvF9MrqXiqMMWADbRZQBTdHoG/LOmIpGPJ15dSq6iAqjLFgA2ckSCCdpYRPi/pkKQ9ZvaRpMe0FtwvmNl3JH0o6f4siywC3tAAimbXAHf3w9v86hsp1wIA6AAfpQeAoAhwAAiKAAeAoAhwAAiKAAeAoGgnWwBl7nQIpIX3yZUI8JzRuhTYHe+T1phCyVnZOx0CaeB90hoBnrOydzoE0sD7pDUCPGdVuJkC0CveJ60R4Dmj0yGwO94nrXERM2d0OgR2x/ukNXPv3z0W6vW6z8/P9+35AKAMzOyUu9e3jjOFAgBBEeAAEBQBDgBBEeAAEBQBDgBBEeAAEBQBDgBBEeAAEBQBDgBBEeAAEBQBDgBBEeAAEBQBDgBB9dRO1szOSvpM0qqkz1t1ywIAZCONfuBfd/ePU/jvAAA6wA0dgAqaXWhwc4QS6HUO3CX9xMxOmdnRVg8ws6NmNm9m88vLyz0+HYBezS40NDWzqMZKUy6psdLU1MyiZhcaeZeGDvUa4Afd/auS7pL0gJl9besD3P24u9fdvT48PNzj06HsZhcaOnjsDd306Cs6eOwNQiUD03NLal5a3TTWvLSq6bmlnCpCt3oKcHc/l3y9KOnHku5IoyhUE0eG/XFupdnROIqr6wA3s6vM7Jr17yV9U9LptApD9XBk2B/7hmodjaO4ejkCv0HS22b2M0n/LekVd//PdMpCFXFk2B+T46OqDQ5sGqsNDmhyfDSnitCtrlehuPuvJP1xirWg4vYN1dRoEdYcGaZrfbUJq1DiYxkhCmNyfFRTM4ubplE4MszGxNgIgV0CBDgKgyNDoDMEOAqFI0OgfTSzAoCgCHAACIoAB4CgCHAACIoAB4CgCr8KhbaXANBaoQN8vbnR+gc71psbSSLEAVReoadQaG4EANsrdIDT3AgAtlfoKRSaGxUX1yY28FogL4U+AqftZTFx44UNvBbIU6EDfGJsRE/cd5tGhmoySSNDNT1x320c3eSMaxMbeC2Qp0JPoUg0Nyoirk1s4LVAngp9BI5i4pZcG3gtkCcCHB3j2sQGXgvkqfBTKCgebrywgdcCeTJ379uT1et1n5+f79vzAUAZmNkpd69vHWcKBQCCIsABICgCHACCIsABICgCHACCIsABICjWgWeA7nQA+qGnADezOyX9o6QBSf/s7sdSqSqwrO8iFPF/DhFrLgpeu+Ip0j7pegrFzAYk/ZOkuyTdKumwmd2aVmFRZdmdLmLr0og1FwWvXfEUbZ/0Mgd+h6Rfuvuv3P3/JP2bpHvSKSuuLLvTRWxdGrHmouC1K56i7ZNeAnxE0v9e9vNHydgmZnbUzObNbH55ebmHp4shy+50EVuXRqy5KHjtiqdo+6SXALcWY1c0VnH34+5ed/f68PBwD08XQ5bd6SK2Lo1Yc1Hw2hVP0fZJLwH+kaQbL/v59yWd662c+LK8i1DE1qURay4KXrviKdo+6WUVyk8l3WJmN0lqSPpLSX+VSlXBZXUXoYitSyPWXBS8dsVTtH3SUztZM7tb0j9obRnhCXf/+50eTztZAOjcdu1ke1oH7u6vSnq1l/8GAKA7fJQeAIIiwAEgKAIcAIIiwAEgqL7e1NjMliX9uss/3yPp4xTLKaKybyPbF1/Zt7Go2/cH7n7FJyH7GuC9MLP5VstoyqTs28j2xVf2bYy2fUyhAEBQBDgABBUpwI/nXUAflH0b2b74yr6NobYvzBw4AGCzSEfgAIDLEOAAEFSIADezO81sycx+aWaP5l1P2szsrJktmtl7ZlaKdo1mdsLMLprZ6cvGrjOz18zsF8nXa/OssRfbbN/jZtZI9uN7SbfOkMzsRjN708zOmNkHZvZQMl6KfbjD9oXah4WfA09unvw/kv5cazeR+Kmkw+7+81wLS5GZnZVUd/cifoCgK2b2NUm/lfQv7v6VZOz7kj5x92PJ/4ivdfe/y7PObm2zfY9L+q27/yDP2tJgZnsl7XX3d83sGkmnJE1I+huVYB/usH3fUqB9GOEInJsnB+Tub0n6ZMvwPZJOJt+f1NobJqRttq803P28u7+bfP+ZpDNau+dtKfbhDtsXSoQAb+vmycG5pJ+Y2SkzO5p3MRm6wd3PS2tvIEnX51xPFh40s/eTKZaQ0wtbmdkBSWOS3lEJ9+GW7ZMC7cMIAd7WzZODO+juX5V0l6QHktNzxPO0pJsl3S7pvKQn8y2nd2Z2taQXJT3s7p/mXU/aWmxfqH0YIcBLf/Nkdz+XfL0o6cdamzYqowvJ3OP6HOTFnOtJlbtfcPdVd/9C0jMKvh/NbFBr4facu88kw6XZh622L9o+jBDgX9482cx+T2s3T34555pSY2ZXJRdRZGZXSfqmpNM7/1VYL0s6knx/RNJLOdaSuvVgS9yrwPvRzEzSs5LOuPtTl/2qFPtwu+2Ltg8LvwpF6vzmyZGY2R9q7ahbWrtH6b+WYfvM7HlJh7TWnvOCpMckzUp6QdJ+SR9Kut/dQ14I3Gb7Dmnt1NslnZX03fX54mjM7E8l/ZekRUlfJMPf09o8cfh9uMP2HVagfRgiwAEAV4owhQIAaIEAB4CgCHAACIoAB4CgCHAACIoAB4CgCHAACOr/ARYDj3ZXQg8OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_quotes = stock_data[:,0]\n",
    "scatter(days, stock_quotes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBXb7dieCO5h"
   },
   "source": [
    "How does our old model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7gAxCgy1COnX",
    "outputId": "ab82d066-952b-4719-e252-f679badb7b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1304018805642806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = np.array(days).reshape(-1, 1)\n",
    "ols_stocks.fit(days, stock_quotes)\n",
    "ols_stocks.score(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E94vTFUCax_"
   },
   "source": [
    "Not bad, but can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mCR5GImZCbGz",
    "outputId": "9c0ef176-cb6e-4727-bdab-02f22a663b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7096344785385665"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
    "ols_stocks.score(stock_data[:,1:], stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Qk-jlBCCiKB"
   },
   "source": [
    "Yep - unsurprisingly, the other covariates (assets and liabilities) have info.\n",
    "\n",
    "But, they do worse without the day data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dDcZl7I5Cf5D",
    "outputId": "3cbd0a6e-2aa6-4f21-dbb3-66ef266f782b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4548418085645855"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,2:], stock_quotes)\n",
    "ols_stocks.score(stock_data[:,2:], stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnLXlrK8ENjb"
   },
   "source": [
    "## Time series jargon\n",
    "\n",
    "There's a lot of semi-standard language and tricks to talk about this sort of data. [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) has an excellent guidebook, but here are some highlights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWUyhnTbcq55"
   },
   "source": [
    "### Moving average\n",
    "\n",
    "Moving average aka rolling average aka running average.\n",
    "\n",
    "Convert a series of data to a series of averages of continguous subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "47bHhBSCcvw-",
    "outputId": "ee6bc38c-43d1-4d12-def3-fa409c9dcac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.980782701207195,\n",
       " 11.406233604516785,\n",
       " 7.248183290341171,\n",
       " 6.01251697941414,\n",
       " 2.6989358986455128,\n",
       " 6.758238157784441,\n",
       " 7.5746763818165475,\n",
       " 8.266370227403556,\n",
       " 4.179067620026775,\n",
       " 2.2121379578806906,\n",
       " 8.601094256599755,\n",
       " 12.133553367575013,\n",
       " 14.000511302407952,\n",
       " 7.625414859260315,\n",
       " 10.022262053035277,\n",
       " 6.644860005908659,\n",
       " 9.73012860228899,\n",
       " 5.4040525408031,\n",
       " 9.412027546373134,\n",
       " 14.918049182602369,\n",
       " 18.09541171402879,\n",
       " 16.784427813078228,\n",
       " 14.860057784575266,\n",
       " 18.171585234865393,\n",
       " 15.47459413024592,\n",
       " 12.945898974690214,\n",
       " 4.850445416376003,\n",
       " 4.84711208304267]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_quotes_rolling = [sum(stock_quotes[i:i+3]) / 3\n",
    "                        for i in range(len(stock_quotes - 2))]\n",
    "stock_quotes_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36XvbGhoc186"
   },
   "source": [
    "Pandas has nice series related functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "id": "nTNatxtycys_",
    "outputId": "65cf7f83-a475-435f-97cc-b0fc72d85f19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.980783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.406234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.248183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.012517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.698936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.758238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.574676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.266370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.179068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.212138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.601094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12.133553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.625415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.022262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.644860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.730129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.404053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.412028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.918049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18.095412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16.784428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.860058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.171585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.474594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.945899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0         NaN\n",
       "1         NaN\n",
       "2    8.980783\n",
       "3   11.406234\n",
       "4    7.248183\n",
       "5    6.012517\n",
       "6    2.698936\n",
       "7    6.758238\n",
       "8    7.574676\n",
       "9    8.266370\n",
       "10   4.179068\n",
       "11   2.212138\n",
       "12   8.601094\n",
       "13  12.133553\n",
       "14  14.000511\n",
       "15   7.625415\n",
       "16  10.022262\n",
       "17   6.644860\n",
       "18   9.730129\n",
       "19   5.404053\n",
       "20   9.412028\n",
       "21  14.918049\n",
       "22  18.095412\n",
       "23  16.784428\n",
       "24  14.860058\n",
       "25  18.171585\n",
       "26  15.474594\n",
       "27  12.945899"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(stock_quotes)\n",
    "df.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "os-szg47dgwf"
   },
   "source": [
    "### Forecasting\n",
    "\n",
    "Forecasting - at it's simplest, it just means \"predict the future\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D_qtt6irdj0x",
    "outputId": "899d4e2b-1c00-4018-c9d5-3451d75a4e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.48753373])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
    "ols_stocks.predict([[29, 0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjnQY0trdnHp"
   },
   "source": [
    "One way to predict if you just have the series data is to use the prior observation. This can be pretty good (if you had to pick one feature to model the temperature for tomorrow, the temperature today is a good choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bzC4DV9Hdupp",
    "outputId": "72adf4bd-a00a-411d-88cb-2f67078ba44c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14666134425697552"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = np.array([30 + random() * day\n",
    "                        for day in np.array(range(365)).reshape(-1, 1)])\n",
    "temperature_next = temperature[1:].reshape(-1, 1)\n",
    "temperature_ols = LinearRegression()\n",
    "temperature_ols.fit(temperature[:-1], temperature_next)\n",
    "temperature_ols.score(temperature[:-1], temperature_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFdssXQbdxbE"
   },
   "source": [
    "But you can often make it better by considering more than one prior observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pVfUqD2YdxxZ",
    "outputId": "a892c370-bd2b-4e36-ef5d-767e762dacdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18901811290702464"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_next_next = temperature[2:].reshape(-1, 1)\n",
    "temperature_two_past = np.concatenate([temperature[:-2], temperature_next[:-1]],\n",
    "                                      axis=1)\n",
    "temperature_ols.fit(temperature_two_past, temperature_next_next)\n",
    "temperature_ols.score(temperature_two_past, temperature_next_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9QltBdmd7TV"
   },
   "source": [
    "### Exponential smoothing\n",
    "\n",
    "Exponential smoothing means using exponentially decreasing past weights to predict the future.\n",
    "\n",
    "You could roll your own, but let's use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1992
    },
    "colab_type": "code",
    "id": "hvMNqunOeC_B",
    "outputId": "e0294de0-3cd8-4935-e730-ee339c191690"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.245588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.328555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.862733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.080080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.452611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.036551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31.806408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.590915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.626476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.585816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33.452683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34.180266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34.888989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>35.126176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.751264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.387976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37.438389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>37.070636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36.387461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36.595064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>37.937979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>38.909650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>38.140285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>38.448387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37.904273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37.252210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>37.368353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>37.915084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>37.660315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>142.580229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>157.390886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>169.038921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>165.261563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>154.471484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>170.811494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>175.486591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>175.609330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>187.195483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>200.324858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>206.578977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>201.741664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>197.076804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>207.929921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>214.348983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>210.431784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>197.239255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>199.719047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>192.464754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>206.692913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>222.812608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>226.923785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>220.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>217.846066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>205.020014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>203.432072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>215.312027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>227.870121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>238.312208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>223.654476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0     30.000000\n",
       "1     30.245588\n",
       "2     30.328555\n",
       "3     30.862733\n",
       "4     31.080080\n",
       "5     31.452611\n",
       "6     32.036551\n",
       "7     31.806408\n",
       "8     31.590915\n",
       "9     32.626476\n",
       "10    32.585816\n",
       "11    33.452683\n",
       "12    34.180266\n",
       "13    34.888989\n",
       "14    35.126176\n",
       "15    35.751264\n",
       "16    36.387976\n",
       "17    37.438389\n",
       "18    37.070636\n",
       "19    36.387461\n",
       "20    36.595064\n",
       "21    37.937979\n",
       "22    38.909650\n",
       "23    38.140285\n",
       "24    38.448387\n",
       "25    37.904273\n",
       "26    37.252210\n",
       "27    37.368353\n",
       "28    37.915084\n",
       "29    37.660315\n",
       "..          ...\n",
       "335  142.580229\n",
       "336  157.390886\n",
       "337  169.038921\n",
       "338  165.261563\n",
       "339  154.471484\n",
       "340  170.811494\n",
       "341  175.486591\n",
       "342  175.609330\n",
       "343  187.195483\n",
       "344  200.324858\n",
       "345  206.578977\n",
       "346  201.741664\n",
       "347  197.076804\n",
       "348  207.929921\n",
       "349  214.348983\n",
       "350  210.431784\n",
       "351  197.239255\n",
       "352  199.719047\n",
       "353  192.464754\n",
       "354  206.692913\n",
       "355  222.812608\n",
       "356  226.923785\n",
       "357  220.094000\n",
       "358  217.846066\n",
       "359  205.020014\n",
       "360  203.432072\n",
       "361  215.312027\n",
       "362  227.870121\n",
       "363  238.312208\n",
       "364  223.654476\n",
       "\n",
       "[365 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df = pd.DataFrame(temperature)\n",
    "temperature_df.ewm(halflife=7).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBEjBZVbeH6R"
   },
   "source": [
    "Halflife is among the parameters we can play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "HjZgMwYkeODN",
    "outputId": "806ae8dd-1812-4bbb-8aae-9271676aea45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.193309e+06\n",
      "dtype: float64\n",
      "0    969642.464121\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sse_1 = ((temperature_df - temperature_df.ewm(halflife=7).mean())**2).sum()\n",
    "sse_2 = ((temperature_df - temperature_df.ewm(halflife=3).mean())**2).sum()\n",
    "print(sse_1)\n",
    "print(sse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s39bj4g9eQ9Z"
   },
   "source": [
    "Note - the first error being higher doesn't mean it's necessarily *worse*. It's *smoother* as expected, and if that's what we care about - great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcPMn8o4eYP1"
   },
   "source": [
    "### Seasonality\n",
    "\n",
    "Seasonality - \"day of week\"-effects, and more. In a lot of real world data, certain time periods are systemically different, e.g. holidays for retailers, weekends for restaurants, seasons for weather.\n",
    "\n",
    "Let's try to make some seasonal data - a store that sells more later in a week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "h0qPMWCreheL",
    "outputId": "f3d7b830-5185-42c4-b4b2-c4ab53c00b09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd642f9afd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPkUlEQVR4nO3dX2hk93nG8eeNqpLBMShlJ6lX663cNgiCl1hhMIEtxjVtZCehURca4tKQXCkXCSRQ1Hpzk/SirNttQ3oRCmptsGn+EMhaDUmoEnBMkhs3kuVm7SxKQ7ppPWu8MkbEBpGu5TcXM9qVZmc0ZzTnN+c9Z74fWFZ7NDv7nhn20Zn39+eYuwsAENebii4AAHA4ghoAgiOoASA4ghoAgiOoASC430jxpMeOHfOZmZkUTw0AlbS+vv6yu9e7fS9JUM/MzGhtbS3FUwNAJZnZL3p9j9YHAARHUANAcAQ1AARHUANAcAQ1AARHUANAcEmm52E4KxtNnV/d1JXtHR2fqmlpflYLc9NFlwWgIAR1MCsbTZ29cFE713YlSc3tHZ29cFGSCGtgTNH6COb86ub1kN6zc21X51c3C6oIQNEI6mCubO8MdBxA9RHUwRyfqg10HED1EdTBLM3PqjY5ceBYbXJCS/OzBVUEoGgMJgazN2DIrA8AewjqgBbmpglmANfR+gCA4AhqAAiOoAaA4AhqAAiOoAaA4AhqAAiO6XkAJLFrY2QENQB2bQyO1gcAdm0MjqAGwK6NwRHUANi1MTiCGgC7NgaXaTDRzC5LelXSrqTX3b2RsigAo8WujbENMuvjD9395WSVACgUuzbGResDAILLGtQu6Ttmtm5mi90eYGaLZrZmZmtbW1v5VQgAYy5rUJ9293dLekDSJ8zsns4HuPuyuzfcvVGv13MtEgDGWaagdvcr7d+vSnpC0t0piwIA3NA3qM3sFjO7de9rSe+V9FzqwgAALVlmfbxd0hNmtvf4L7v7fyStCgBwXd+gdvefS3rXCGoBAHTB9DwACI6gBoDgCGoACI6gBoDgCGoACI6gBoDgCGoACI6gBoDgCGoACI6gBoDgCGoACI6gBoDgBrlnIgCMjZWNZpib/RLUANBhZaOpsxcuaufariSpub2jsxcuSlIhYU3rAwA6nF/dvB7Se3au7er86mYh9RDUANDhyvbOQMdTI6gBoMPxqdpAx1MjqAGgw9L8rGqTEweO1SYntDQ/W0g9DCYCQIe9AUNmfQBAYAtz04UFcydaHwAQHEENAMER1AAQHEENAMER1AAQHEENAMER1AAQXOagNrMJM9sws2+mLAgAcNAgV9SfknQpVSEAgO4yBbWZnZD0fkn/mrYcAECnrFfUX5D0V5Le6PUAM1s0szUzW9va2sqlOABAhqA2sw9Iuuru64c9zt2X3b3h7o16vZ5bgQAw7rJcUZ+W9CdmdlnSVyXdZ2b/lrQqAMB1fYPa3c+6+wl3n5H0YUlPuvtfJK8MACCJedQAEN5A+1G7+1OSnkpSCQCgK66oASA4ghoAgiOoASA4ghoAgiOoASA4ghoAgiOoASA4ghoAgiOoASA4ghoAghtoCTkA4GYrG02dX93Ule0dHZ+qaWl+Vgtz07k9P0ENAENY2Wjq7IWL2rm2K0lqbu/o7IWLkpRbWNP6AIAhnF/dvB7Se3au7er86mZu/wZBDQBDuLK9M9DxoyCoAWAIx6dqAx0/CoIaAIawND+r2uTEgWO1yQktzc/m9m8wmAhgbKSYnbH395n1AQBDSjk7Y2FuOtdg7kTrA8BYGMXsjFQIagBjYRSzM1IhqAGMhVHMzkiFoAYwFkYxOyMVBhOBCku9B0WZjGJ2RioENVBRo9iDomxSz85IhdYHUFFlnuWAg7iiBiqqzLMcshqX1k7fK2oze7OZ/aeZ/ZeZPW9mfzOKwgAMp8yzHLLYa+00t3fkutHaWdloFl1a7rK0Pn4l6T53f5ekuyTdb2bvSVsWgGGVeZZDFuPU2unb+nB3l/Ra+4+T7V+esigAwyvzLIcsxqG1sydTj9rMJiStS/p9SV9096e7PGZR0qIknTx5Ms8aARxRWWc5ZHF8qqZml1CuSmtnv0yzPtx9193vknRC0t1mdmeXxyy7e8PdG/V6Pe86AeCAqrd29htoep67b0t6StL9SaoBgIwW5qZ17swpTU/VZJKmp2o6d+ZUJT9B9G19mFld0jV33zazmqQ/kvR3ySsDgD6q3NrZL0uP+jZJj7X71G+S9DV3/2basgAAe7LM+vixpLkR1AIA6IIl5AAQHEvIEd64LBMGeiGoERo7wAG0PhDcOC0TBnohqBHaOC0TBnohqBFa1XeAA7IgqBHaOC0TBnphMBGhVX0HOCALghrhjcsyYaAXWh8AEBxBDQDBEdQAEBw9alQKy81RRQQ1KoPl5qgqghqVcdhy81EHNVf2yBNBjcqIstycK3vkjcFEVEaU5eZsJIW8EdQdVjaaOv3wk7rjoW/p9MNPamWjWXRJyCjKcvMoV/aoDlof+/CRtdyiLDc/PlVTs0sos5EUjoqg3ifSYBSOJsJy86X52QM/8CU2ksJwCOp9+Mg6HGY6tES5skd1ENT78JH16GgbHRThyh7VwWDiPlEGo8qImQ5AOlxR78NH1qOjbQSkQ1B34CPr0dA2AtKh9YFc0DYC0uGKGrmgbQSk0zeozex2SY9L+m1Jb0hadvd/Sl0Yyoe2EZBGlivq1yX9pbs/Y2a3Slo3s++6+08S1wYAUIYetbu/6O7PtL9+VdIlSVw2AcCIDDSYaGYzkuYkPd3le4tmtmZma1tbW/lUBwDIHtRm9hZJX5f0aXf/Zef33X3Z3Rvu3qjX63nWCABjLVNQm9mkWiH9JXe/kLYkAMB+fYPazEzSI5Iuufvn05cEANgvyxX1aUkfkXSfmT3b/vW+xHUBANr6Ts9z9x9KshHUAgDogpWJAEJhX/ObEdQAwmBf8+7YlAlAGOxr3h1BDSAM9jXvjtbHiNB3A/pjX/PuuKIegb2+W3N7R64bfbeVjWbRpQGhsK95d1xRj8BhfTeuqsGnrRvY17w7gnoE6LuhF2Y53Ix9zW9G62MEevXXxr3vBmY5IBuCegTou6EXPm0hi8q3PiL0/yL13QZ5PSK8dlXHLAdkUemgjtT/i9B3G+T1iPTaVdnS/OyB11ni0xZuVunWB/2/gwZ5PXjtRmNhblrnzpzS9FRNJml6qqZzZ07xwxAHVPqKmv7fQYO8Hrx2oxPh0xZiq3RQp+7/la2HO8jrQe8UiKPSrY+Usy3KuNpwkNeDmSpAHJUO6pT9vzL2cAd5PeidAnGYu+f+pI1Gw9fW1nJ/3kjueOhb6vbKmaT/efj9oy4HQMmZ2bq7N7p9r9JX1Cmx2hDAqBDUR0QPF8CoVHrWR0qRVhsC0ZVthlQ0BPUQmP8K9Mcq1+HR+gCQVBlnSEVDUANIilWuwyOoASTFDKnhEdQAkmKG1PD6Diaa2aOSPiDpqrvfmb6k/hhBBsqDGVLD67sy0czukfSapMezBnXKlYmdI8hS66czy5sBlNlQKxPd/fuSXsm9qiNiBBnAuMltHrWZLUpalKSTJ0/m9bQ3YQQZRaHlhqLkFtTuvixpWWq1PvJ63k7sk1wNZQs9Fm2gSKWb9cEIcvmVcS9vWm4oUumCmn2Sy6+MoUfLDUXKMj3vK5LulXTMzF6Q9Fl3fyR1YYdhj41yK2Po0XJDkbLM+njQ3W9z90l3P1F0SKP8yrhSjZYbisTueRi5pfnZrnPhiwi9rIOakRZtlG0gFsMjqDFyUUJv0JkcEVpuzD4ZTwQ1ChEh9A4b1Cy6tl6i1MxV/WgR1BhbZRzUjFAzV/WjV7rpeUBeyjioGaHmMk6vLDuCGmOrjDM5ItQc4ap+3ND6wNiKMqg5iAg1M6d89Ppuc3oUR9nmlMEJoBzYajiNw7Y5DXFFzeAEUB4RrurHTYigjjLlCEA2EaZXjpMQg4kMTgBAbyGCOsKUIwCIKkRQR5hyBABRhehRMzgBAL2FCGqJwQkA6CVE6wMA0BtBDQDBEdQAEBxBDQDBEdQAEBxBDQDBEdQAEBxBDQDBEdQAEBxBDQDBhVlCjqPj7jhAtWW6ojaz+81s08x+ZmYPpS4K2e3dHae5vSPXjbvjrGw0iy4NQE76BrWZTUj6oqQHJL1T0oNm9s7UhSGbw+6OA6AaslxR3y3pZ+7+c3f/f0lflfTBtGUhK+6OA1RflqCelvR/+/78QvsYAuDuOED1ZQlq63LMb3qQ2aKZrZnZ2tbW1vCVIRPujgNUX5agfkHS7fv+fELSlc4HufuyuzfcvVGv1/OqD30szE3r3JlTmp6qySRNT9V07swpZn0AFZJlet6PJL3DzO6Q1JT0YUl/nrQqDIS74wDV1jeo3f11M/ukpFVJE5Iedffnk1cGAJCUccGLu39b0rcT1wIA6IIl5AAQHEENAMER1AAQHEENAMGZ+01rV4Z/UrMtSb844l8/JunlHMuJpurnJ1X/HDm/8ot4jr/j7l0XoSQJ6mGY2Zq7N4quI5Wqn59U/XPk/MqvbOdI6wMAgiOoASC4iEG9XHQBiVX9/KTqnyPnV36lOsdwPWoAwEERr6gBAPsQ1AAQXJigHocb6JrZZTO7aGbPmtla0fUMy8weNbOrZvbcvmO/ZWbfNbP/bv/+1iJrHFaPc/ycmTXb7+OzZva+ImschpndbmbfM7NLZva8mX2qfbwS7+Mh51eq9zBEj7p9A92fSvpjtW5U8CNJD7r7TwotLGdmdllSw92jTbQ/EjO7R9Jrkh539zvbx/5e0ivu/nD7B+5b3f2vi6xzGD3O8XOSXnP3fyiytjyY2W2SbnP3Z8zsVknrkhYkfUwVeB8POb8PqUTvYZQram6gW0Lu/n1Jr3Qc/qCkx9pfP6bWf4rS6nGOleHuL7r7M+2vX5V0Sa17olbifTzk/EolSlCPyw10XdJ3zGzdzBaLLiaRt7v7i1LrP4mktxVcTyqfNLMft1sjpWwLdDKzGUlzkp5WBd/HjvOTSvQeRgnqTDfQrYDT7v5uSQ9I+kT7YzXK558l/Z6kuyS9KOkfiy1neGb2Fklfl/Rpd/9l0fXkrcv5leo9jBLUmW6gW3bufqX9+1VJT6jV8qmal9p9wb3+4NWC68mdu7/k7rvu/oakf1HJ30czm1QrxL7k7hfahyvzPnY7v7K9h1GC+voNdM3sN9W6ge43Cq4pV2Z2S3swQ2Z2i6T3Snru8L9VSt+Q9NH21x+V9O8F1pLEXoC1/alK/D6amUl6RNIld//8vm9V4n3sdX5lew9DzPqQpPb0mC/oxg10/7bgknJlZr+r1lW01LpX5ZfLfo5m9hVJ96q1ZeRLkj4raUXS1ySdlPS/kv7M3Us7GNfjHO9V6yOzS7os6eN7/dyyMbM/kPQDSRclvdE+/Bm1+rilfx8POb8HVaL3MExQAwC6i9L6AAD0QFADQHAENQAER1ADQHAENQAER1ADQHAENQAE92ulTz9WMhjxTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales = np.array([random() + (day % 7) * random() for day in days])\n",
    "scatter(days, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEADkcMzelxY"
   },
   "source": [
    "How does linear regression do at fitting this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EV5kt69GenV3",
    "outputId": "d8cf0f90-2034-42da-f3f5-dff498afaca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13023322892994238"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_ols = LinearRegression()\n",
    "sales_ols.fit(days, sales)\n",
    "sales_ols.score(days, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7shN1eBMep9Q"
   },
   "source": [
    "That's not great - and the fix depends on the domain. Here, we know it'd be best to actually use \"day of week\" as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qo9eFlHIeqtA",
    "outputId": "b002f32b-8e20-4482-a70d-6ba63db9281b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5684640774601243"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_of_week = days % 7\n",
    "sales_ols.fit(day_of_week, sales)\n",
    "sales_ols.score(day_of_week, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ooJIfIMex2G"
   },
   "source": [
    "Note that it's also important to have representative data across whatever seasonal feature(s) you use - don't predict retailers based only on Christmas, as that won't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "- https://seekingalpha.com/article/4087604-much-artificial-intelligence-ibm-watson\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger batch sizes allow RNNs to converge faster and weigh more into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 15:44:29.031691 140559078836032 deprecation.py:506] From /home/nedderlander/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0722 15:44:29.043685 140559078836032 deprecation.py:506] From /home/nedderlander/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 15:44:29.298710 140559078836032 deprecation.py:323] From /home/nedderlander/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.4577 - acc: 0.7846 - val_loss: 0.3868 - val_acc: 0.8318\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 73s 3ms/sample - loss: 0.2912 - acc: 0.8819 - val_loss: 0.3786 - val_acc: 0.8388\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.2116 - acc: 0.9184 - val_loss: 0.4049 - val_acc: 0.8362\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.1508 - acc: 0.9448 - val_loss: 0.5127 - val_acc: 0.8296\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.1051 - acc: 0.9624 - val_loss: 0.4897 - val_acc: 0.8270\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.0774 - acc: 0.9733 - val_loss: 0.6588 - val_acc: 0.8230\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.0555 - acc: 0.9818 - val_loss: 0.7554 - val_acc: 0.8226\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.0466 - acc: 0.9848 - val_loss: 0.7279 - val_acc: 0.8168\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.0329 - acc: 0.9893 - val_loss: 0.8792 - val_acc: 0.8179\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.0227 - acc: 0.9935 - val_loss: 0.9382 - val_acc: 0.8200\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0205 - acc: 0.9938 - val_loss: 0.9369 - val_acc: 0.8104\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0219 - acc: 0.9928 - val_loss: 0.9721 - val_acc: 0.8113\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0152 - acc: 0.9948 - val_loss: 0.9492 - val_acc: 0.8120\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0127 - acc: 0.9961 - val_loss: 1.1950 - val_acc: 0.8127\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0122 - acc: 0.9965 - val_loss: 1.1098 - val_acc: 0.8180\n",
      "25000/25000 [==============================] - 14s 578us/sample - loss: 1.1098 - acc: 0.8180\n",
      "Test score: 1.1098431599539518\n",
      "Test accuracy: 0.81804\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "# taking the first 80 chars will make the results converge faster, though the model can take variable size inputs\n",
    "batch_size = 32\n",
    "# relatively small batch size for RNN\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# This is a pre-processing technique that chops X into 80 char chunks and turn them into time sequences\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### RNN Text generation with NumPy\n",
    "\n",
    "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz1m55G5WSrQ"
   },
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "ahlHBeoZCaLX",
    "outputId": "7b1c5f93-3fa5-42db-acb6-3c894b0accef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 215kB 1.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (6.0.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194kB 3.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (4.3.4)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 3.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (3.4.4)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.4MB 7.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (4.7.1)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (2.8.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (5.1.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from newspaper3k) (2.22.0)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from tldextract>=2.0.1->newspaper3k) (41.0.1)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
      "Requirement already satisfied: six in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nedderlander/anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->newspaper3k) (2019.6.16)\n",
      "Building wheels for collected packages: feedparser, tinysegmenter, jieba3k, feedfinder2\n",
      "  Building wheel for feedparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nedderlander/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nedderlander/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nedderlander/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nedderlander/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "Successfully built feedparser tinysegmenter jieba3k feedfinder2\n",
      "Installing collected packages: feedparser, requests-file, tldextract, tinysegmenter, jieba3k, feedfinder2, cssselect, newspaper3k\n",
      "Successfully installed cssselect-1.0.3 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTPlziljCiNJ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-83b806c39519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bk9JF2zaCxoO",
    "outputId": "9e66fc15-a397-4b59-f810-d2182565c99a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newspaper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7abdb458fed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://ap.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'newspaper' is not defined"
     ]
    }
   ],
   "source": [
    "ap = newspaper.build('http://ap.com')\n",
    "len(ap.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Vc6JgAIJDF4E",
    "outputId": "44a13922-d86a-4668-c4fd-455c0d03b6c1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5a0e88d2e96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ap' is not defined"
     ]
    }
   ],
   "source": [
    "article_text = ''\n",
    "\n",
    "for article in ap.articles[:1]:\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article_text += '\\n\\n' + article.text\n",
    "    except:\n",
    "        print('Failed: ' + article.url)\n",
    "\n",
    "article_text = article_text.split('\\n\\n')[1]\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rsMBBMcv_nRM",
    "outputId": "9f77b07b-4a5a-4ac8-f1b3-79e1a5331fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  55\n",
      "txt_data_size :  486\n"
     ]
    }
   ],
   "source": [
    "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
    "import numpy as np\n",
    "\n",
    "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(article_text)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "aQygqc_CAWRA",
    "outputId": "30c45e95-057a-4643-9cae-fc518b49c914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 0, '-': 1, 'I': 2, '1': 3, 'G': 4, 'v': 5, 'c': 6, 'a': 7, 'd': 8, 'u': 9, 'y': 10, 'm': 11, 'P': 12, ':': 13, 'H': 14, 'U': 15, 'â€™': 16, '4': 17, 'R': 18, ' ': 19, 'n': 20, 'E': 21, 'f': 22, 'â€¦': 23, '(': 24, 'h': 25, '8': 26, 'l': 27, ')': 28, 'q': 29, 'L': 30, 'T': 31, '9': 32, '7': 33, 'S': 34, 'j': 35, 't': 36, 'w': 37, 'Q': 38, 'N': 39, ',': 40, 'â€”': 41, 'b': 42, 's': 43, '.': 44, 'e': 45, 'V': 46, 'O': 47, 'i': 48, 'â€œ': 49, 'g': 50, 'p': 51, 'k': 52, 'o': 53, 'A': 54}\n",
      "----------------------------------------------------\n",
      "{0: 'r', 1: '-', 2: 'I', 3: '1', 4: 'G', 5: 'v', 6: 'c', 7: 'a', 8: 'd', 9: 'u', 10: 'y', 11: 'm', 12: 'P', 13: ':', 14: 'H', 15: 'U', 16: 'â€™', 17: '4', 18: 'R', 19: ' ', 20: 'n', 21: 'E', 22: 'f', 23: 'â€¦', 24: '(', 25: 'h', 26: '8', 27: 'l', 28: ')', 29: 'q', 30: 'L', 31: 'T', 32: '9', 33: '7', 34: 'S', 35: 'j', 36: 't', 37: 'w', 38: 'Q', 39: 'N', 40: ',', 41: 'â€”', 42: 'b', 43: 's', 44: '.', 45: 'e', 46: 'V', 47: 'O', 48: 'i', 49: 'â€œ', 50: 'g', 51: 'p', 52: 'k', 53: 'o', 54: 'A'}\n",
      "----------------------------------------------------\n",
      "[31, 47, 38, 15, 21, 18, 46, 2, 30, 30, 21, 40, 19, 15, 36, 7, 25, 19, 41, 19, 54, 19, 37, 53, 11, 7, 20, 19, 37, 7, 43, 19, 52, 48, 27, 27, 45, 8, 19, 7, 20, 8, 19, 43, 45, 5, 45, 0, 7, 27, 19, 6, 25, 48, 27, 8, 0, 45, 20, 19, 37, 45, 0, 45, 19, 48, 20, 35, 9, 0, 45, 8, 19, 7, 22, 36, 45, 0, 19, 7, 19, 6, 0, 7, 43, 25, 19, 20, 45, 7, 0, 19, 31, 53, 29, 9, 45, 0, 5, 48, 27, 27, 45, 19, 34, 9, 20, 8, 7, 10, 19, 20, 48, 50, 25, 36, 44, 19, 54, 19, 15, 36, 7, 25, 19, 14, 48, 50, 25, 37, 7, 10, 19, 12, 7, 36, 0, 53, 27, 19, 51, 0, 45, 43, 43, 19, 0, 45, 27, 45, 7, 43, 45, 19, 43, 36, 7, 36, 45, 43, 19, 36, 25, 45, 19, 6, 0, 7, 43, 25, 19, 53, 6, 6, 9, 0, 0, 45, 8, 19, 7, 0, 53, 9, 20, 8, 19, 26, 13, 17, 32, 19, 51, 44, 11, 44, 19, 53, 20, 19, 34, 18, 1, 3, 33, 44, 19, 12, 25, 53, 36, 53, 43, 19, 6, 53, 9, 0, 36, 45, 43, 10, 19, 34, 36, 44, 19, 4, 45, 53, 0, 50, 45, 19, 39, 45, 37, 43, 19, 43, 25, 53, 37, 19, 36, 25, 45, 19, 43, 6, 45, 20, 45, 44, 19, 54, 19, 3, 33, 1, 10, 45, 7, 0, 1, 53, 27, 8, 19, 50, 48, 0, 27, 19, 37, 48, 36, 25, 19, 7, 19, 27, 45, 7, 0, 20, 45, 0, 16, 43, 19, 51, 45, 0, 11, 48, 36, 19, 37, 7, 43, 19, 8, 0, 48, 5, 48, 20, 50, 19, 7, 19, 51, 48, 6, 52, 9, 51, 19, 36, 0, 9, 6, 52, 19, 45, 7, 43, 36, 42, 53, 9, 20, 8, 19, 37, 48, 36, 25, 19, 7, 20, 19, 7, 8, 9, 27, 36, 19, 7, 20, 8, 19, 36, 25, 0, 45, 45, 19, 53, 36, 25, 45, 0, 19, 6, 25, 48, 27, 8, 0, 45, 20, 19, 48, 20, 19, 36, 25, 45, 19, 5, 45, 25, 48, 6, 27, 45, 44, 19, 49, 39, 45, 7, 0, 19, 11, 48, 27, 45, 51, 53, 43, 36, 19, 17, 19, 24, 31, 53, 29, 9, 45, 0, 5, 48, 27, 27, 45, 28, 19, 36, 25, 45, 19, 8, 0, 48, 5, 45, 0, 19, 22, 7, 48, 27, 45, 8, 19, 36, 53, 19, 20, 45, 50, 53, 36, 48, 7, 36, 45, 19, 36, 25, 45, 19, 43, 37, 53, 53, 51, 48, 20, 50, 19, 36, 9, 0, 20, 19, 36, 25, 7, 36, 19, 27, 45, 7, 8, 43, 19, 48, 20, 36, 53, 23]\n",
      "----------------------------------------------------\n",
      "data length :  486\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcpMSWDHFowT"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 1000\n",
    "sequence_length = 40\n",
    "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 500  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkqoN86qWaI4"
   },
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imfg_Ew0WdDL"
   },
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "    \"\"\"\n",
    "    The forward prop pass in our example creates a log proba for each character in a sequence.\n",
    "    \n",
    "    \"ABCD EFG\"\n",
    "    Take this sequences and with our current weights it'll look at \"A\" and try to predict the next character.\n",
    "    We had 49 characters in our vovab so our softmax will predict which of the 49 we will get next\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "        \n",
    "        \n",
    "        print([int_to_char[char] for char in ps[t]])\n",
    "        print ([int_to_char[char] for char in targets[t]])\n",
    "        \n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zm6qwNiqWdMe"
   },
   "source": [
    "#### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81qBiz_xWenI"
   },
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8sBvcdbWfhi"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "iA4RM70LWgO_",
    "outputId": "0fd64bca-f1b5-4be1-9e80-076308365598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 235.968912\n",
      "iter 100, loss: 3.286046\n",
      "iter 200, loss: 2.774669\n",
      "iter 300, loss: 2.382193\n",
      "iter 400, loss: 3.248081\n",
      "iter 500, loss: 2.315543\n",
      "iter 600, loss: 2.177977\n",
      "iter 700, loss: 2.022759\n",
      "iter 800, loss: 2.026381\n",
      "iter 900, loss: 4.045268\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
    "        \n",
    "        \n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjh8Ip68WgYV"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDCxDNPG68Hx"
   },
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nGVhl-Gxh6N6",
    "outputId": "e0c8b70b-fb50-4000-f4f8-a572539513db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Therm019,hursip tha k . (Photo:a(Pug a  aXt sid Rob \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('T', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPsz-oefL1kP"
   },
   "source": [
    "Well... that's *vaguely* language-looking. Can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "\n",
    "# complete works of shakespeare\n",
    "filename = '100-0.txt'\n",
    "raw_text = open(filename).read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping of unique chars to ints\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  5564374\n",
      "Total Vocab:  73\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 5564274\n"
     ]
    }
   ],
   "source": [
    "# encode txt as ints\n",
    "\n",
    "seq_length = 100\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    X.append([char_to_int[char] for char in seq_in])\n",
    "    y.append(char_to_int[seq_out])\n",
    "n_patterns = len(X)\n",
    "print(\"Total patterns: {}\".format(n_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "# reshape and normalize\n",
    "# @jit(nopython=True)\n",
    "def reshape(array, dim1, dim2, dim3, norm):\n",
    "    return (np.reshape(array, (dim1, dim2, dim3)) / float(norm))\n",
    "\n",
    "X = reshape(X, n_patterns, seq_length, 1, n_vocab)\n",
    "\n",
    "# X = X / float(n_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe output variable\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(y, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0fbc965b0856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# define our LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define our LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# define checkpoint\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "filepath = \"weights-improvement-{epoch:02d} -{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 00:24:03.748682 140314906519360 deprecation.py:323] From /home/nedderlander/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5564256/5564274 [============================>.] - ETA: 0s - loss: 2.3678 - acc: 0.3201\n",
      "Epoch 00001: loss improved from inf to 2.36775, saving model to weights-improvement-01 -2.3678.hdf5\n",
      "5564274/5564274 [==============================] - 14179s 3ms/sample - loss: 2.3678 - acc: 0.3201\n",
      "Epoch 2/20\n",
      "5564256/5564274 [============================>.] - ETA: 0s - loss: 2.0895 - acc: 0.3942\n",
      "Epoch 00002: loss improved from 2.36775 to 2.08954, saving model to weights-improvement-02 -2.0895.hdf5\n",
      "5564274/5564274 [==============================] - 14422s 3ms/sample - loss: 2.0895 - acc: 0.3942\n",
      "Epoch 3/20\n",
      " 132896/5564274 [..............................] - ETA: 4:28:35 - loss: 2.0142 - acc: 0.4163"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e3f95be27d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit this bad boy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit this bad boy\n",
    "model.fit(X, y, epochs=20, batch_size = 32, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = 'weights-improvement-02 -2.0895.hdf5'\n",
    "model.load_weights(filename)\n",
    "\n",
    "# compile\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first use our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:  mineeyehathplayedthepainterandhathstelledthybeautyâ€™sformintableofmyheartmybodyistheframewhereinâ€™tish\n",
      "[43, 39, 44, 35, 35, 55, 35, 38, 31, 50, 38, 46, 42, 31, 55, 35, 34, 50, 38, 35, 46, 31, 39, 44, 50, 35, 48, 31, 44, 34, 38, 31, 50, 38, 49, 50, 35, 42, 42, 35, 34, 50, 38, 55, 32, 35, 31, 51, 50, 55, 70, 49, 36, 45, 48, 43, 39, 44, 50, 31, 32, 42, 35, 45, 36, 43, 55, 38, 35, 31, 48, 50, 43, 55, 32, 45, 34, 55, 39, 49, 50, 38, 35, 36, 48, 31, 43, 35, 53, 38, 35, 48, 35, 39, 44, 70, 50, 39, 49, 38]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a15a8fbbcd75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msequ_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoem_chars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpoem_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-a15a8fbbcd75>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msequ_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoem_chars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpoem_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'm'"
     ]
    }
   ],
   "source": [
    "# use sonnets for prediction\n",
    "# read in text of sonnets\n",
    "path = './sonnets.txt'\n",
    "poems = [line.split('\\n') for line in open(path).read().split('\\n\\n')]\n",
    "sonnets = [line for line in poems if len(line) > 2]\n",
    "\n",
    "# create sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "tokens_list = [text_to_word_sequence(str(text).replace(\"'\", '')) for text in sonnets]\n",
    "\n",
    "# create a random seed\n",
    "start = np.random.randint(0, len(tokens_list)-1)\n",
    "\n",
    "# use it to find a poem\n",
    "pattern = tokens_list[start]\n",
    "\n",
    "# break poem into chars\n",
    "from functools import reduce \n",
    "words = [word for word in poem]\n",
    "poem_chars = reduce(lambda x,y: str(x+y), poem)[:100] \n",
    "\n",
    "print('Seed: ', poem_chars)\n",
    "poem_ints = [char_to_int[str(value)] for value in poem_chars]\n",
    "print((poem_ints))\n",
    "\n",
    "\n",
    "\n",
    "# predict some characters\n",
    "\n",
    "for i in range(1000):\n",
    "    x = np.reshape(poem_ints, (1, len(poem_ints), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    sequ_in = [int_to_char[value] for value in poem_ints]\n",
    "    sys.stdout.write(result)\n",
    "    poem_chars.append(index)\n",
    "    poem_chars = poem_chars[1:len(poem_chars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below I tried to predict the words in sonnets, rather than letters and did not fully succeed.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do homework with keras first\n",
    "\n",
    "# read in text of sonnets\n",
    "path = './sonnets.txt'\n",
    "poems = [line.split('\\n') for line in open(path).read().split('\\n\\n')]\n",
    "sonnets = [line for line in poems if len(line) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "tokens_list = [text_to_word_sequence(str(text).replace(\"'\", '')) for text in sonnets]\n",
    "\n",
    "vocab = set()\n",
    "for text in tokens_list:\n",
    "    for word in text:\n",
    "        vocab.add(word)\n",
    "        \n",
    "len(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I have a vocab and a list of lists (tokens_list) where each inner list is a sonnet\n",
    "# I'm sort of lost so I'm actually going to just use the keras tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(tokens_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize what was learned\n",
    "# print(t.word_counts) #a dictionary of words and their counts\n",
    "# print(t.document_count) #a dictionary of words and documents they appeared int\n",
    "# print(t.word_index) #a dictinary of words and their integers\n",
    "# print(t.word_docs) #an interger count of the total number of documents used to fit the T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll take the first part of each sonnet as training, and the end as test\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sonnet in tokens_list:\n",
    "    X.append(sonnet[:int(len(sonnet)*.8)])\n",
    "    y.append(sonnet[-(int(len(sonnet)*.2)):])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 0:  ['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautyâ€™s', 'rose', 'might', 'never', 'die', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', 'feedâ€™st', 'thy', 'lightâ€™s', 'flame', 'with', 'self', 'substantial', 'fuel', 'making', 'a', 'famine', 'where', 'abundance', 'lies', 'thy', 'self', 'thy', 'foe', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', 'thou', 'that', 'art', 'now', 'the', 'worldâ€™s', 'fresh', 'ornament', 'and', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', 'within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', 'and', 'tender', 'churl']\n",
      "y 0:  ['waste', 'in', 'niggarding', 'pity', 'the', 'world', 'or', 'else', 'this', 'glutton', 'be', 'to', 'eat', 'the', 'worldâ€™s', 'due', 'by', 'the', 'grave', 'and', 'thee']\n",
      "token 0:  ['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautyâ€™s', 'rose', 'might', 'never', 'die', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', 'feedâ€™st', 'thy', 'lightâ€™s', 'flame', 'with', 'self', 'substantial', 'fuel', 'making', 'a', 'famine', 'where', 'abundance', 'lies', 'thy', 'self', 'thy', 'foe', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', 'thou', 'that', 'art', 'now', 'the', 'worldâ€™s', 'fresh', 'ornament', 'and', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', 'within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', 'and', 'tender', 'churl', 'makâ€™st', 'waste', 'in', 'niggarding', 'pity', 'the', 'world', 'or', 'else', 'this', 'glutton', 'be', 'to', 'eat', 'the', 'worldâ€™s', 'due', 'by', 'the', 'grave', 'and', 'thee']\n"
     ]
    }
   ],
   "source": [
    "print('X 0: ', X[0])\n",
    "print('y 0: ', y[0])\n",
    "print('token 0: ', tokens_list[0])\n",
    "\n",
    "# looks like we lost one word, seems ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I'll encode our documents\n",
    "X = t.texts_to_matrix(X, mode='tfidf')\n",
    "y = t.texts_to_matrix(y, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154, 3194), (154, 3194))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 138 samples, validate on 16 samples\n",
      "Epoch 1/15\n",
      "138/138 [==============================] - 2s 12ms/sample - loss: 374.3072 - acc: 0.0000e+00 - val_loss: 386.0551 - val_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 352.9733 - acc: 0.0000e+00 - val_loss: 370.6561 - val_acc: 0.0000e+00\n",
      "Epoch 3/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 320.5945 - acc: 0.0000e+00 - val_loss: 390.8481 - val_acc: 0.0000e+00\n",
      "Epoch 4/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 313.0193 - acc: 0.0000e+00 - val_loss: 421.5773 - val_acc: 0.0000e+00\n",
      "Epoch 5/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 317.0777 - acc: 0.0000e+00 - val_loss: 452.2933 - val_acc: 0.0000e+00\n",
      "Epoch 6/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 324.6608 - acc: 0.0000e+00 - val_loss: 482.1810 - val_acc: 0.0000e+00\n",
      "Epoch 7/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 337.4236 - acc: 0.0000e+00 - val_loss: 512.5315 - val_acc: 0.0000e+00\n",
      "Epoch 8/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 350.5356 - acc: 0.0000e+00 - val_loss: 543.1943 - val_acc: 0.0000e+00\n",
      "Epoch 9/15\n",
      "138/138 [==============================] - 1s 6ms/sample - loss: 366.6767 - acc: 0.0000e+00 - val_loss: 574.0566 - val_acc: 0.0000e+00\n",
      "Epoch 10/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 383.0743 - acc: 0.0000e+00 - val_loss: 605.3561 - val_acc: 0.0000e+00\n",
      "Epoch 11/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 402.5920 - acc: 0.0000e+00 - val_loss: 636.8278 - val_acc: 0.0000e+00\n",
      "Epoch 12/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 420.0217 - acc: 0.0000e+00 - val_loss: 667.8684 - val_acc: 0.0000e+00\n",
      "Epoch 13/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 439.5262 - acc: 0.0000e+00 - val_loss: 698.5066 - val_acc: 0.0000e+00\n",
      "Epoch 14/15\n",
      "138/138 [==============================] - 1s 6ms/sample - loss: 458.6767 - acc: 0.0000e+00 - val_loss: 729.2504 - val_acc: 0.0000e+00\n",
      "Epoch 15/15\n",
      "138/138 [==============================] - 1s 5ms/sample - loss: 475.6997 - acc: 0.0000e+00 - val_loss: 760.3728 - val_acc: 0.0000e+00\n",
      "16/16 [==============================] - 0s 2ms/sample - loss: 760.3728 - acc: 0.0000e+00\n",
      "Test score: 760.372802734375\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# build fast training keras model\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "maxlen = 80\n",
    "# taking the first 80 chars will make the results converge faster, though the model can take variable size inputs\n",
    "\n",
    "batch_size = 32\n",
    "# relatively small batch size for RNN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "# This is a pre-processing technique that chops X into 80 char chunks and turn them into time sequences\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80110408e-12, 4.78074998e-02, 2.70368792e-02, ...,\n",
       "        3.07860319e-07, 1.17861454e-07, 9.26677757e-08],\n",
       "       [1.80110408e-12, 4.78074998e-02, 2.70368792e-02, ...,\n",
       "        3.07860319e-07, 1.17861454e-07, 9.26677757e-08],\n",
       "       [1.80110408e-12, 4.78074998e-02, 2.70368792e-02, ...,\n",
       "        3.07860319e-07, 1.17861454e-07, 9.26677757e-08],\n",
       "       ...,\n",
       "       [1.80110408e-12, 4.78074998e-02, 2.70368792e-02, ...,\n",
       "        3.07860319e-07, 1.17861454e-07, 9.26677757e-08],\n",
       "       [1.80110408e-12, 4.78074998e-02, 2.70368792e-02, ...,\n",
       "        3.07860319e-07, 1.17861454e-07, 9.26677757e-08],\n",
       "       [1.80110408e-12, 4.78074998e-02, 2.70368792e-02, ...,\n",
       "        3.07860319e-07, 1.17861454e-07, 9.26677757e-08]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's make a prediction!\n",
    "\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, t.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "def process_for_prediction(input):\n",
    "    \n",
    "    matrix = t.texts_to_matrix(input, mode='tfidf')\n",
    "    output = sequence.pad_sequences(matrix, maxlen=maxlen)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "test = tokens_list[42][:80]\n",
    "raw_prediction = model.predict(process_for_prediction(test))\n",
    "# sequence_to_text(raw_prediction)\n",
    "raw_prediction\n",
    "\n",
    "# failed to return to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1a\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# then try a single RNN cell "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
