{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 4 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_m0hJ4uCzHz"
   },
   "source": [
    "## Time series with plain old regression\n",
    "\n",
    "Recurrences are fancy, and we'll get to those later - let's start with something simple. Regression can handle time series just fine if you just set them up correctly - let's try some made-up stock data. And to make it, let's use a few list comprehensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkJUFfsgnqr_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "days = np.array((range(28)))\n",
    "stock_quotes = np.array([random() + day * random() for day in days])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "y-ORgKGNBOcb",
    "outputId": "133809e1-8588-4acb-f07e-20dfefcd03ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57139189,  1.36222188,  0.54112333,  3.03568072,  3.59615527,\n",
       "        4.36445432,  5.73703539,  3.04943928,  5.67965081,  0.80452021,\n",
       "        1.69416909,  8.79751624,  5.61909649,  9.89215407, 11.33457392,\n",
       "        4.99502856, 12.43007491,  4.2696616 , 11.98945954, 16.84023388,\n",
       "       17.12992726, 17.62775566,  9.29819784, 10.08957687,  2.1253239 ,\n",
       "        1.56021597, 14.59181541, 14.40518187])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3lR2wGvBx3a"
   },
   "source": [
    "Let's take a look with a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "pVUTC2tmBSIq",
    "outputId": "75664a71-713d-4815-d4cc-2055f485784a"
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import scatter\n",
    "scatter(days, stock_quotes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgD4q-T_B0jd"
   },
   "source": [
    "Looks pretty linear, let's try a simple OLS regression.\n",
    "\n",
    "First, these need to be NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3Q0MrnUBXAl"
   },
   "outputs": [],
   "source": [
    "days = days.reshape(-1, 1)  # X needs to be column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19],\n",
       "       [20],\n",
       "       [21],\n",
       "       [22],\n",
       "       [23],\n",
       "       [24],\n",
       "       [25],\n",
       "       [26],\n",
       "       [27]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqr0SHOnB5yR"
   },
   "source": [
    "Now let's use good old `scikit-learn` and linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PqyHxgFvBYl5",
    "outputId": "0d4a183e-fdb3-4e97-c8be-ab31a82b07ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3921131494576302"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols_stocks = LinearRegression()\n",
    "ols_stocks.fit(days, stock_quotes)\n",
    "ols_stocks.score(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlU0mr-KB_Yk"
   },
   "source": [
    "That seems to work pretty well, but real stocks don't work like this.\n",
    "\n",
    "Let's make *slightly* more realistic data that depends on more than just time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FV1Emb2BuLz"
   },
   "outputs": [],
   "source": [
    "# Not everything is best as a comprehension\n",
    "stock_data = np.empty([len(days), 4])\n",
    "for day in days:\n",
    "  asset = random()\n",
    "  liability = random()\n",
    "  quote = random() + ((day * random()) + (20 * asset) - (15 * liability))\n",
    "  quote = max(quote, 0.01)  # Want positive quotes\n",
    "  stock_data[day] = np.array([quote, day, asset, liability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "6Qe2zzN1CESe",
    "outputId": "5b0cafbf-3a47-40cf-8bc3-38f2dc084e91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e-02, 0.00000000e+00, 6.00001360e-04, 4.10513568e-01],\n",
       "       [1.00000000e-02, 1.00000000e+00, 9.87714558e-03, 4.90248341e-01],\n",
       "       [1.34624308e+01, 2.00000000e+00, 7.59531564e-01, 3.06131012e-01],\n",
       "       [1.72277590e+01, 3.00000000e+00, 8.03209133e-01, 1.10268079e-01],\n",
       "       [1.34232047e+00, 4.00000000e+00, 6.43117746e-01, 9.83618969e-01],\n",
       "       [7.69059425e+00, 5.00000000e+00, 1.74608303e-01, 8.59720242e-02],\n",
       "       [8.00502404e+00, 6.00000000e+00, 2.54080513e-01, 1.92872903e-01],\n",
       "       [4.58776753e+00, 7.00000000e+00, 7.70029067e-01, 8.36572359e-01],\n",
       "       [8.79343409e+00, 8.00000000e+00, 5.27583257e-01, 4.90203990e-01],\n",
       "       [1.00000000e-02, 9.00000000e+00, 1.39407080e-01, 7.21442779e-01],\n",
       "       [1.49991258e+01, 1.00000000e+01, 9.72568617e-01, 8.81726914e-01],\n",
       "       [7.95609681e+00, 1.10000000e+01, 5.25750210e-01, 6.10376231e-01],\n",
       "       [1.31513017e+01, 1.20000000e+01, 7.82377078e-01, 7.79049146e-01],\n",
       "       [3.59806522e+00, 1.30000000e+01, 2.08930667e-02, 1.73398304e-01],\n",
       "       [1.46182355e+01, 1.40000000e+01, 4.17811014e-01, 5.19893501e-01],\n",
       "       [1.60516833e+01, 1.50000000e+01, 4.48943564e-01, 3.03700942e-01],\n",
       "       [1.28141214e+01, 1.60000000e+01, 8.52644503e-01, 6.12702589e-01],\n",
       "       [4.47670356e+00, 1.70000000e+01, 3.03319117e-01, 9.89865175e-01],\n",
       "       [1.21419778e+01, 1.80000000e+01, 2.33264403e-01, 6.98475704e-01],\n",
       "       [1.45670095e+01, 1.90000000e+01, 1.59012605e-01, 1.17542021e-01],\n",
       "       [5.99589833e+00, 2.00000000e+01, 5.11602228e-01, 8.67354518e-01],\n",
       "       [6.57553366e+00, 2.10000000e+01, 6.53810262e-01, 4.78639244e-01],\n",
       "       [7.63915562e+00, 2.20000000e+01, 2.94286266e-01, 8.84729730e-01],\n",
       "       [1.68256490e+01, 2.30000000e+01, 6.74301851e-01, 9.14311812e-01],\n",
       "       [2.19025117e+01, 2.40000000e+01, 5.44505492e-01, 3.27058413e-01],\n",
       "       [1.47373884e+01, 2.50000000e+01, 9.83530412e-01, 5.31405474e-01],\n",
       "       [8.54161967e+00, 2.60000000e+01, 4.58256603e-02, 4.17623234e-01],\n",
       "       [3.03969211e+01, 2.70000000e+01, 5.72523284e-01, 1.85558990e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzYy4Pb2CLCh"
   },
   "source": [
    "Let's look again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "qdBcScz4CIXr",
    "outputId": "ad3ec81f-8fba-4355-d251-630ba5fc333c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_quotes = stock_data[:,0]\n",
    "scatter(days, stock_quotes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBXb7dieCO5h"
   },
   "source": [
    "How does our old model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7gAxCgy1COnX",
    "outputId": "ab82d066-952b-4719-e252-f679badb7b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.259534293077595"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = np.array(days).reshape(-1, 1)\n",
    "ols_stocks.fit(days, stock_quotes)\n",
    "ols_stocks.score(days, stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3E94vTFUCax_"
   },
   "source": [
    "Not bad, but can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mCR5GImZCbGz",
    "outputId": "9c0ef176-cb6e-4727-bdab-02f22a663b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.692417782128411"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
    "ols_stocks.score(stock_data[:,1:], stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Qk-jlBCCiKB"
   },
   "source": [
    "Yep - unsurprisingly, the other covariates (assets and liabilities) have info.\n",
    "\n",
    "But, they do worse without the day data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dDcZl7I5Cf5D",
    "outputId": "3cbd0a6e-2aa6-4f21-dbb3-66ef266f782b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44011416356881816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,2:], stock_quotes)\n",
    "ols_stocks.score(stock_data[:,2:], stock_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnLXlrK8ENjb"
   },
   "source": [
    "## Time series jargon\n",
    "\n",
    "There's a lot of semi-standard language and tricks to talk about this sort of data. [NIST](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) has an excellent guidebook, but here are some highlights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWUyhnTbcq55"
   },
   "source": [
    "### Moving average\n",
    "\n",
    "Moving average aka rolling average aka running average.\n",
    "\n",
    "Convert a series of data to a series of averages of continguous subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "47bHhBSCcvw-",
    "outputId": "ee6bc38c-43d1-4d12-def3-fa409c9dcac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.494143605496791,\n",
       " 10.23339660183582,\n",
       " 10.677503426657438,\n",
       " 8.75355790425079,\n",
       " 5.679312920560396,\n",
       " 6.761128606080415,\n",
       " 7.128741887498954,\n",
       " 4.463733874850321,\n",
       " 7.934186629576108,\n",
       " 7.6550742004043535,\n",
       " 12.035508108657297,\n",
       " 8.23515458519988,\n",
       " 10.455867487643802,\n",
       " 11.422661342033294,\n",
       " 14.49468006862081,\n",
       " 11.11416941710924,\n",
       " 9.810934267803544,\n",
       " 10.39523028613151,\n",
       " 10.901628541928053,\n",
       " 9.046147148689661,\n",
       " 6.736862533980276,\n",
       " 10.346779413305404,\n",
       " 15.455772093764717,\n",
       " 17.821849684155385,\n",
       " 15.060506585917105,\n",
       " 17.891976398316014,\n",
       " 12.97951360277591,\n",
       " 10.132307046289904]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_quotes_rolling = [sum(stock_quotes[i:i+3]) / 3\n",
    "                        for i in range(len(stock_quotes - 2))]\n",
    "stock_quotes_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36XvbGhoc186"
   },
   "source": [
    "Pandas has nice series related functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 928
    },
    "colab_type": "code",
    "id": "nTNatxtycys_",
    "outputId": "65cf7f83-a475-435f-97cc-b0fc72d85f19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.494144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.233397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.677503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.753558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.679313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.761129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.128742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.463734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.934187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.655074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.035508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.235155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.455867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.422661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.494680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.114169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.810934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.395230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.901629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.046147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.736863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.346779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.455772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.821850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.060507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17.891976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0         NaN\n",
       "1         NaN\n",
       "2    4.494144\n",
       "3   10.233397\n",
       "4   10.677503\n",
       "5    8.753558\n",
       "6    5.679313\n",
       "7    6.761129\n",
       "8    7.128742\n",
       "9    4.463734\n",
       "10   7.934187\n",
       "11   7.655074\n",
       "12  12.035508\n",
       "13   8.235155\n",
       "14  10.455867\n",
       "15  11.422661\n",
       "16  14.494680\n",
       "17  11.114169\n",
       "18   9.810934\n",
       "19  10.395230\n",
       "20  10.901629\n",
       "21   9.046147\n",
       "22   6.736863\n",
       "23  10.346779\n",
       "24  15.455772\n",
       "25  17.821850\n",
       "26  15.060507\n",
       "27  17.891976"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(stock_quotes)\n",
    "df.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "os-szg47dgwf"
   },
   "source": [
    "### Forecasting\n",
    "\n",
    "Forecasting - at it's simplest, it just means \"predict the future\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D_qtt6irdj0x",
    "outputId": "899d4e2b-1c00-4018-c9d5-3451d75a4e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.8866455])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_stocks.fit(stock_data[:,1:], stock_quotes)\n",
    "ols_stocks.predict([[29, 0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjnQY0trdnHp"
   },
   "source": [
    "One way to predict if you just have the series data is to use the prior observation. This can be pretty good (if you had to pick one feature to model the temperature for tomorrow, the temperature today is a good choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bzC4DV9Hdupp",
    "outputId": "72adf4bd-a00a-411d-88cb-2f67078ba44c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20199093844149352"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = np.array([30 + random() * day\n",
    "                        for day in np.array(range(365)).reshape(-1, 1)])\n",
    "temperature_next = temperature[1:].reshape(-1, 1)\n",
    "temperature_ols = LinearRegression()\n",
    "temperature_ols.fit(temperature[:-1], temperature_next)\n",
    "temperature_ols.score(temperature[:-1], temperature_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFdssXQbdxbE"
   },
   "source": [
    "But you can often make it better by considering more than one prior observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pVfUqD2YdxxZ",
    "outputId": "a892c370-bd2b-4e36-ef5d-767e762dacdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2610993200403239"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_next_next = temperature[2:].reshape(-1, 1)\n",
    "temperature_two_past = np.concatenate([temperature[:-2], temperature_next[:-1]],\n",
    "                                      axis=1)\n",
    "temperature_ols.fit(temperature_two_past, temperature_next_next)\n",
    "temperature_ols.score(temperature_two_past, temperature_next_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9QltBdmd7TV"
   },
   "source": [
    "### Exponential smoothing\n",
    "\n",
    "Exponential smoothing means using exponentially decreasing past weights to predict the future.\n",
    "\n",
    "You could roll your own, but let's use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1992
    },
    "colab_type": "code",
    "id": "hvMNqunOeC_B",
    "outputId": "e0294de0-3cd8-4935-e730-ee339c191690"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.202926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.193995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.626625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.759740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.027314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.495854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31.648581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.494652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31.689127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.342693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33.406820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33.487938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34.570620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34.741217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.271029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.124551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>36.469704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>37.192911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37.169708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36.919928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>38.070316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>38.334485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>38.913631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>38.275760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>39.184702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40.587940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39.531390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>39.085097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>38.654866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>195.476105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>181.121146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>185.140126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>193.272247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>206.669559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>204.477518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>207.147526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>216.317775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>205.892936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>205.875175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>217.962074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>222.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>218.979943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>229.002011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>235.365981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>229.069536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>221.698075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>213.458706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>210.501913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>216.134339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>222.802325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>221.529472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>214.072137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>226.843471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>235.908217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>233.088481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>223.891326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>229.872854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>238.324771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>220.864539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0     30.000000\n",
       "1     30.202926\n",
       "2     30.193995\n",
       "3     30.626625\n",
       "4     30.759740\n",
       "5     31.027314\n",
       "6     31.495854\n",
       "7     31.648581\n",
       "8     31.494652\n",
       "9     31.689127\n",
       "10    32.342693\n",
       "11    33.406820\n",
       "12    33.487938\n",
       "13    34.570620\n",
       "14    34.741217\n",
       "15    35.271029\n",
       "16    36.124551\n",
       "17    36.469704\n",
       "18    37.192911\n",
       "19    37.169708\n",
       "20    36.919928\n",
       "21    38.070316\n",
       "22    38.334485\n",
       "23    38.913631\n",
       "24    38.275760\n",
       "25    39.184702\n",
       "26    40.587940\n",
       "27    39.531390\n",
       "28    39.085097\n",
       "29    38.654866\n",
       "..          ...\n",
       "335  195.476105\n",
       "336  181.121146\n",
       "337  185.140126\n",
       "338  193.272247\n",
       "339  206.669559\n",
       "340  204.477518\n",
       "341  207.147526\n",
       "342  216.317775\n",
       "343  205.892936\n",
       "344  205.875175\n",
       "345  217.962074\n",
       "346  222.227800\n",
       "347  218.979943\n",
       "348  229.002011\n",
       "349  235.365981\n",
       "350  229.069536\n",
       "351  221.698075\n",
       "352  213.458706\n",
       "353  210.501913\n",
       "354  216.134339\n",
       "355  222.802325\n",
       "356  221.529472\n",
       "357  214.072137\n",
       "358  226.843471\n",
       "359  235.908217\n",
       "360  233.088481\n",
       "361  223.891326\n",
       "362  229.872854\n",
       "363  238.324771\n",
       "364  220.864539\n",
       "\n",
       "[365 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df = pd.DataFrame(temperature)\n",
    "temperature_df.ewm(halflife=7).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBEjBZVbeH6R"
   },
   "source": [
    "Halflife is among the parameters we can play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "HjZgMwYkeODN",
    "outputId": "806ae8dd-1812-4bbb-8aae-9271676aea45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1.191803e+06\n",
      "dtype: float64\n",
      "0    974784.530006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sse_1 = ((temperature_df - temperature_df.ewm(halflife=7).mean())**2).sum()\n",
    "sse_2 = ((temperature_df - temperature_df.ewm(halflife=3).mean())**2).sum()\n",
    "print(sse_1)\n",
    "print(sse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s39bj4g9eQ9Z"
   },
   "source": [
    "Note - the first error being higher doesn't mean it's necessarily *worse*. It's *smoother* as expected, and if that's what we care about - great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcPMn8o4eYP1"
   },
   "source": [
    "### Seasonality\n",
    "\n",
    "Seasonality - \"day of week\"-effects, and more. In a lot of real world data, certain time periods are systemically different, e.g. holidays for retailers, weekends for restaurants, seasons for weather.\n",
    "\n",
    "Let's try to make some seasonal data - a store that sells more later in a week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "h0qPMWCreheL",
    "outputId": "f3d7b830-5185-42c4-b4b2-c4ab53c00b09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f28640feef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales = np.array([random() + (day % 7) * random() for day in days])\n",
    "scatter(days, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEADkcMzelxY"
   },
   "source": [
    "How does linear regression do at fitting this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EV5kt69GenV3",
    "outputId": "d8cf0f90-2034-42da-f3f5-dff498afaca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.062469952920284455"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_ols = LinearRegression()\n",
    "sales_ols.fit(days, sales)\n",
    "sales_ols.score(days, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7shN1eBMep9Q"
   },
   "source": [
    "That's not great - and the fix depends on the domain. Here, we know it'd be best to actually use \"day of week\" as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qo9eFlHIeqtA",
    "outputId": "b002f32b-8e20-4482-a70d-6ba63db9281b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24990018993090357"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_of_week = days % 7\n",
    "sales_ols.fit(day_of_week, sales)\n",
    "sales_ols.score(day_of_week, sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ooJIfIMex2G"
   },
   "source": [
    "Note that it's also important to have representative data across whatever seasonal feature(s) you use - don't predict retailers based only on Christmas, as that won't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### RNN Text generation with NumPy\n",
    "\n",
    "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz1m55G5WSrQ"
   },
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "ahlHBeoZCaLX",
    "outputId": "7b1c5f93-3fa5-42db-acb6-3c894b0accef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 215kB 41.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (2.7.3)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (5.2.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (2.20.0)\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.4MB 6.6MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Requirement already satisfied: nltk>=3.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (3.3)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (3.12)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194kB 47.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (4.6.0)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 41.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from newspaper3k) (4.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-dateutil>=2.5.3->newspaper3k) (1.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tldextract>=2.0.1->newspaper3k) (39.1.0)\n",
      "Building wheels for collected packages: jieba3k, tinysegmenter, feedfinder2, feedparser\n",
      "  Running setup.py bdist_wheel for jieba3k ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "  Running setup.py bdist_wheel for tinysegmenter ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Running setup.py bdist_wheel for feedfinder2 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Running setup.py bdist_wheel for feedparser ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "Successfully built jieba3k tinysegmenter feedfinder2 feedparser\n",
      "Installing collected packages: cssselect, jieba3k, tinysegmenter, feedfinder2, feedparser, requests-file, tldextract, newspaper3k\n",
      "Successfully installed cssselect-1.0.3 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.4.3 tinysegmenter-0.3 tldextract-2.2.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTPlziljCiNJ"
   },
   "outputs": [],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bk9JF2zaCxoO",
    "outputId": "9e66fc15-a397-4b59-f810-d2182565c99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = newspaper.build('https://www.apnews.com')\n",
    "len(ap.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Vc6JgAIJDF4E",
    "outputId": "44a13922-d86a-4668-c4fd-455c0d03b6c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, this zipcode is not in our deliverable area for this subscription service.\n"
     ]
    }
   ],
   "source": [
    "article_text = ''\n",
    "\n",
    "for article in ap.articles[:1]:\n",
    "  try:\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article_text += '\\n\\n' + article.text\n",
    "  except:\n",
    "    print('Failed: ' + article.url)\n",
    "  \n",
    "article_text = article_text.split('\\n\\n')[1]\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rsMBBMcv_nRM",
    "outputId": "9f77b07b-4a5a-4ac8-f1b3-79e1a5331fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  23\n",
      "txt_data_size :  81\n"
     ]
    }
   ],
   "source": [
    "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
    "import numpy as np\n",
    "\n",
    "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(article_text)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "aQygqc_CAWRA",
    "outputId": "30c45e95-057a-4643-9cae-fc518b49c914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 0, '.': 1, 'a': 2, 'r': 3, 'u': 4, 'i': 5, 'S': 6, 'b': 7, 'e': 8, 'f': 9, 's': 10, ' ': 11, 'd': 12, 'n': 13, 'z': 14, 't': 15, 'p': 16, 'l': 17, 'c': 18, ',': 19, 'v': 20, 'y': 21, 'o': 22}\n",
      "----------------------------------------------------\n",
      "{0: 'h', 1: '.', 2: 'a', 3: 'r', 4: 'u', 5: 'i', 6: 'S', 7: 'b', 8: 'e', 9: 'f', 10: 's', 11: ' ', 12: 'd', 13: 'n', 14: 'z', 15: 't', 16: 'p', 17: 'l', 18: 'c', 19: ',', 20: 'v', 21: 'y', 22: 'o'}\n",
      "----------------------------------------------------\n",
      "[6, 22, 3, 3, 21, 19, 11, 15, 0, 5, 10, 11, 14, 5, 16, 18, 22, 12, 8, 11, 5, 10, 11, 13, 22, 15, 11, 5, 13, 11, 22, 4, 3, 11, 12, 8, 17, 5, 20, 8, 3, 2, 7, 17, 8, 11, 2, 3, 8, 2, 11, 9, 22, 3, 11, 15, 0, 5, 10, 11, 10, 4, 7, 10, 18, 3, 5, 16, 15, 5, 22, 13, 11, 10, 8, 3, 20, 5, 18, 8, 1]\n",
      "----------------------------------------------------\n",
      "data length :  81\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcpMSWDHFowT"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 20\n",
    "sequence_length = 40\n",
    "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 500  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkqoN86qWaI4"
   },
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imfg_Ew0WdDL"
   },
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "    \"\"\"\n",
    "    The forward prop pass in our example creates a log proba for each character in a sequence.\n",
    "    \n",
    "    'ABCD EFG'\n",
    "    \"\"\"\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. How \"activated\" is it?\n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#       Display the predicted and actual sequence\n",
    "#       print([int_to_char[character] for character in ps[t]])\n",
    "#       print([int_to_char[character] for character in targets[t]])\n",
    "#       print('--- New Pass ---')\n",
    "        \n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zm6qwNiqWdMe"
   },
   "source": [
    "#### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81qBiz_xWenI"
   },
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices. Empty gradients\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8sBvcdbWfhi"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "iA4RM70LWgO_",
    "outputId": "0fd64bca-f1b5-4be1-9e80-076308365598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 3.801631\n",
      "iter 10, loss: 0.008385\n",
      "CPU times: user 6.16 s, sys: 11 s, total: 17.1 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjh8Ip68WgYV"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDCxDNPG68Hx"
   },
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nGVhl-Gxh6N6",
    "outputId": "e0c8b70b-fb50-4000-f4f8-a572539513db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " aorrservicubocreodbor shrvicnbocriptfirbservicubzcr \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('a', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPsz-oefL1kP"
   },
   "source": [
    "Well... that's *vaguely* language-looking. Can you do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# Training performance optimizations\n",
    "from numba import jit\n",
    "\n",
    "# Read in data\n",
    "import requests\n",
    "import string\n",
    "\n",
    "def remove_non_printable(s: str) -> str:\n",
    "    s = s.replace('\\n',' ')\n",
    "    return ''.join(c for c in s if c in string.ascii_letters or c in \" \")\n",
    "\n",
    "r = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
    "sonnets = remove_non_printable(r.text)[2800:100756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   From fairest creatures we desire increase That thereby beautys rose might never die But as the riper should by time decease His tender heir might bear his memory But thou contracted to thine own bright eyes Feedst thy lights flame with selfsubstantial fuel Making a famine where abundance lies Thy self thy foe to thy sweet self too cruel Thou that art now the worlds fresh ornament And only herald to the gaudy spring Within thine own bud buriest thy content And tender churl makst waste in niggarding   Pity the world or else this glutton be   To eat the worlds due by the grave and thee                         When forty winters shall besiege thy brow And dig deep trenches in thy beautys field Thy youths proud livery so gazed on now Will be a tattered weed of small worth held Then being asked where all thy beauty lies Where all the treasure of thy lusty days To say within thine own deep sunken eyes Were an alleating shame and thriftless praise How much more praise deservd thy beautys us\n"
     ]
    }
   ],
   "source": [
    "print(sonnets[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars: 50\n",
      "Text data size: 97956\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(sonnets))\n",
    "num_chars = len(chars)\n",
    "txt_data_size = len(sonnets)\n",
    "\n",
    "print(f'Unique chars: {num_chars}')\n",
    "print(f'Text data size: {txt_data_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data length: 97956\n"
     ]
    }
   ],
   "source": [
    "# One Hot/1-of-k Encoding\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "integer_encoded = [char_to_int[i] for i in sonnets]\n",
    "print(f'Encoded data length: {len(integer_encoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# RNN Hyperparams\n",
    "iteration = 500\n",
    "sequence_length = 60\n",
    "#batch_size = round((txt_data_size/sequence_length)+0.5)  # eqiuv. to math.ceil\n",
    "batch_size = 128\n",
    "hidden_size = 100  # num. of neurons in hidden layer\n",
    "learning_rate = 1e-1\n",
    "\n",
    "print(batch_size)\n",
    "\n",
    "# RNN params\n",
    "W_xh = np.random.randn(hidden_size, num_chars)  * 0.01     # weight input -> hidden \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)* 0.01     # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)  * 0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(parallel=True, fastmath=True)\n",
    "def forwardprop(inputs, targets, h_prev):\n",
    "    xs = {}\n",
    "    hs = {}\n",
    "    ys = {}\n",
    "    ps = {}\n",
    "    hs[-1] = np.copy(h_prev)  # copy prev. hidden state vec. to -1 key value\n",
    "    loss = 0  # loss init.\n",
    "    \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((num_chars,1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h)\n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y\n",
    "        ps[t] = np.exp(ys[t])        \n",
    "        loss += -np.log(ps[t][targets[t],0])  # softmax cross-entropy loss      \n",
    "    return loss, ps, hs, xs\n",
    "\n",
    "@jit(parallel=True, fastmath=True)\n",
    "def backprop(ps, inputs, hs, xs):\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext\n",
    "        # Backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        # Clip to mitigate exploding gradients   \n",
    "        np.clip(dparam, -5, 5, out=dparam) \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 181.21803351327904\n",
      "iter 2, loss: 175.90984430317698\n",
      "iter 4, loss: 174.26455584992286\n",
      "iter 6, loss: 174.19224616006537\n",
      "iter 8, loss: 174.6689079533259\n",
      "iter 10, loss: 172.4393481650916\n",
      "iter 12, loss: 170.25074572069417\n",
      "iter 14, loss: 168.8989351917822\n",
      "iter 16, loss: 168.16045649864313\n",
      "iter 18, loss: 162.11252445314446\n",
      "iter 20, loss: 161.75323516190633\n",
      "iter 22, loss: 161.29378761597388\n",
      "iter 24, loss: 159.5861382166963\n",
      "iter 26, loss: 158.68697957269995\n",
      "iter 28, loss: 157.4071962667445\n",
      "iter 30, loss: 156.53842721206533\n",
      "iter 32, loss: 155.54000099307456\n",
      "iter 34, loss: 153.09178143884273\n",
      "iter 36, loss: 152.78106452771553\n",
      "iter 38, loss: 152.50776448589184\n",
      "iter 40, loss: 151.5923245111364\n",
      "iter 42, loss: 151.10857607819125\n",
      "iter 44, loss: 150.52670004161425\n",
      "iter 46, loss: 152.34880521986284\n",
      "iter 48, loss: 151.359218937278\n",
      "iter 50, loss: 150.64205309300308\n",
      "iter 52, loss: 150.35546675337145\n",
      "iter 54, loss: 149.08374007448717\n",
      "iter 56, loss: 142.7396621619146\n",
      "iter 58, loss: 142.85915329317697\n",
      "iter 60, loss: 141.74009765570793\n",
      "iter 62, loss: 140.59564584731356\n",
      "iter 64, loss: 139.57620195819996\n",
      "iter 66, loss: 138.52221272009643\n",
      "iter 68, loss: 137.8364135260755\n",
      "iter 70, loss: 135.42116918060788\n",
      "iter 72, loss: 133.91193374629944\n",
      "iter 74, loss: 132.09417658159148\n",
      "iter 76, loss: 131.39072795275985\n",
      "iter 78, loss: 131.02077188484583\n",
      "iter 80, loss: 130.4321379122285\n",
      "iter 82, loss: 129.83985051482307\n",
      "iter 84, loss: 129.22387020887834\n",
      "iter 86, loss: 128.41344216268743\n",
      "iter 88, loss: 127.47511227625678\n",
      "iter 90, loss: 126.55814284642595\n",
      "iter 92, loss: 125.83202548973983\n",
      "iter 94, loss: 125.49035663636006\n",
      "iter 96, loss: 125.2433415971562\n",
      "iter 98, loss: 125.0502437503375\n",
      "iter 100, loss: 124.88862450951281\n",
      "iter 102, loss: 124.7725448850387\n",
      "iter 104, loss: 124.61486367470077\n",
      "iter 106, loss: 124.23185547137139\n",
      "iter 108, loss: 123.96037109716298\n",
      "iter 110, loss: 123.8069863530464\n",
      "iter 112, loss: 123.64967639851739\n",
      "iter 114, loss: 123.42691109358367\n",
      "iter 116, loss: 123.25106430863346\n",
      "iter 118, loss: 122.9636513109749\n",
      "iter 120, loss: 122.68299993940849\n",
      "iter 122, loss: 122.4552778215089\n",
      "iter 124, loss: 122.2221486990563\n",
      "iter 126, loss: 121.99811076913326\n",
      "iter 128, loss: 121.92309708409526\n",
      "iter 130, loss: 121.72701595347412\n",
      "iter 132, loss: 121.47946549949947\n",
      "iter 134, loss: 121.2421723299555\n",
      "iter 136, loss: 121.0053391301058\n",
      "iter 138, loss: 120.77946109156008\n",
      "iter 140, loss: 120.58473592238532\n",
      "iter 142, loss: 120.40974300015178\n",
      "iter 144, loss: 120.26282737042841\n",
      "iter 146, loss: 120.05924518671868\n",
      "iter 148, loss: 119.83998472082742\n",
      "iter 150, loss: 119.638556784385\n",
      "iter 152, loss: 119.42306304046895\n",
      "iter 154, loss: 119.18716500446199\n",
      "iter 156, loss: 118.9642289660525\n",
      "iter 158, loss: 118.63561911644848\n",
      "iter 160, loss: 118.288140010731\n",
      "iter 162, loss: 117.96773084295963\n",
      "iter 164, loss: 117.59509835731279\n",
      "iter 166, loss: 117.26803212410616\n",
      "iter 168, loss: 116.84227803696261\n",
      "iter 170, loss: 116.8260721758587\n",
      "iter 172, loss: 116.36691232305961\n",
      "iter 174, loss: 116.10796642554759\n",
      "iter 176, loss: 115.84720424492136\n",
      "iter 178, loss: 115.6013016421077\n",
      "iter 180, loss: 115.2820362948359\n",
      "iter 182, loss: 115.03741857414786\n",
      "iter 184, loss: 114.72808424854993\n",
      "iter 186, loss: 114.51722371486117\n",
      "iter 188, loss: 114.30774621763764\n",
      "iter 190, loss: 114.14012304769474\n",
      "iter 192, loss: 113.94506396880799\n",
      "iter 194, loss: 113.81676919249504\n",
      "iter 196, loss: 113.60679053369076\n",
      "iter 198, loss: 113.52153579882464\n",
      "iter 200, loss: 113.28934154092613\n",
      "iter 202, loss: 113.19835137319691\n",
      "iter 204, loss: 112.9597176114511\n",
      "iter 206, loss: 112.87201578049223\n",
      "iter 208, loss: 112.5972574521813\n",
      "iter 210, loss: 112.4738551661686\n",
      "iter 212, loss: 112.21489822883841\n",
      "iter 214, loss: 112.1493431904722\n",
      "iter 216, loss: 111.91182672942288\n",
      "iter 218, loss: 111.8208502225297\n",
      "iter 220, loss: 111.48284974453198\n",
      "iter 222, loss: 111.30297047085286\n",
      "iter 224, loss: 111.13623302994128\n",
      "iter 226, loss: 110.94179063868309\n",
      "iter 228, loss: 110.70932389937745\n",
      "iter 230, loss: 110.50458606978752\n",
      "iter 232, loss: 110.31219404444194\n",
      "iter 234, loss: 110.22008888225525\n",
      "iter 236, loss: 109.9661257095385\n",
      "iter 238, loss: 109.86257089896156\n",
      "iter 240, loss: 109.59363870367837\n",
      "iter 242, loss: 109.34313895503288\n",
      "iter 244, loss: 109.13250906818247\n",
      "iter 246, loss: 109.00421996587698\n",
      "iter 248, loss: 108.77788016852114\n",
      "iter 250, loss: 108.70276198247544\n",
      "iter 252, loss: 108.54243564771325\n",
      "iter 254, loss: 108.50449325122642\n",
      "iter 256, loss: 108.2427321482631\n",
      "iter 258, loss: 108.08345596316371\n",
      "iter 260, loss: 107.96458790168697\n",
      "iter 262, loss: 107.8398879049385\n",
      "iter 264, loss: 107.69629457161098\n",
      "iter 266, loss: 107.59579654679757\n",
      "iter 268, loss: 107.5136998903747\n",
      "iter 270, loss: 107.40978643738698\n",
      "iter 272, loss: 107.39137452775945\n",
      "iter 274, loss: 107.31236029058535\n",
      "iter 276, loss: 107.24512107333352\n",
      "iter 278, loss: 107.15242539172347\n",
      "iter 280, loss: 107.18817248343845\n",
      "iter 282, loss: 107.00486285703491\n",
      "iter 284, loss: 106.87171159092942\n",
      "iter 286, loss: 106.90274538522912\n",
      "iter 288, loss: 106.72445250131234\n",
      "iter 290, loss: 106.7390103635195\n",
      "iter 292, loss: 106.70037597445614\n",
      "iter 294, loss: 106.3642917792017\n",
      "iter 296, loss: 106.48196213998948\n",
      "iter 298, loss: 106.43245433966717\n",
      "iter 300, loss: 106.40196802937193\n",
      "iter 302, loss: 106.39356926286472\n",
      "iter 304, loss: 106.3412260983572\n",
      "iter 306, loss: 106.19399885886854\n",
      "iter 308, loss: 106.3848758131581\n",
      "iter 310, loss: 106.371701875645\n",
      "iter 312, loss: 106.34943888604396\n",
      "iter 314, loss: 106.35587018605128\n",
      "iter 316, loss: 106.32999279273399\n",
      "iter 318, loss: 106.36703178478251\n",
      "iter 320, loss: 106.36565884007618\n",
      "iter 322, loss: 106.36260198344435\n",
      "iter 324, loss: 106.30277388100698\n",
      "iter 326, loss: 106.92984342517042\n",
      "iter 328, loss: 106.58170022221032\n",
      "iter 330, loss: 106.27134375042455\n",
      "iter 332, loss: 106.33124666149861\n",
      "iter 334, loss: 106.29125701991923\n",
      "iter 336, loss: 106.26939420057191\n",
      "iter 338, loss: 106.06920206694156\n",
      "iter 340, loss: 106.51335488381714\n",
      "iter 342, loss: 105.98738343507975\n",
      "iter 344, loss: 106.12093435664356\n",
      "iter 346, loss: 106.15034237437467\n",
      "iter 348, loss: 106.16073053987606\n",
      "iter 350, loss: 106.0170630449083\n",
      "iter 352, loss: 105.44600665394073\n",
      "iter 354, loss: 105.84483432712278\n",
      "iter 356, loss: 105.65448598948832\n",
      "iter 358, loss: 105.84228027838901\n",
      "iter 360, loss: 105.26639551772887\n",
      "iter 362, loss: 105.40107225377933\n",
      "iter 364, loss: 105.4654503302092\n",
      "iter 366, loss: 104.75667169251498\n",
      "iter 368, loss: 104.76478780917557\n",
      "iter 370, loss: 104.44085758563233\n",
      "iter 372, loss: 105.22682488066688\n",
      "iter 374, loss: 105.42747660731223\n",
      "iter 376, loss: 105.09261169032297\n",
      "iter 378, loss: 105.48921235170599\n",
      "iter 380, loss: 104.98889047928061\n",
      "iter 382, loss: 105.31363200922195\n",
      "iter 384, loss: 104.78682039250984\n",
      "iter 386, loss: 104.2333206933279\n",
      "iter 388, loss: 104.24162975289106\n",
      "iter 390, loss: 104.26488073480266\n",
      "iter 392, loss: 105.0328347860655\n",
      "iter 394, loss: 104.69501333361073\n",
      "iter 396, loss: 104.17114427309029\n",
      "iter 398, loss: 104.16949332922948\n",
      "iter 400, loss: 104.3956524330332\n",
      "iter 402, loss: 103.92126048932342\n",
      "iter 404, loss: 103.38085740855507\n",
      "iter 406, loss: 103.32670482899468\n",
      "iter 408, loss: 103.54738552567008\n",
      "iter 410, loss: 102.83033531727305\n",
      "iter 412, loss: 102.54804119132845\n",
      "iter 414, loss: 103.3797969296914\n",
      "iter 416, loss: 103.77574737309473\n",
      "iter 418, loss: 104.40457908149374\n",
      "iter 420, loss: 102.74651062676759\n",
      "iter 422, loss: 103.16607855272427\n",
      "iter 424, loss: 102.82353808271104\n",
      "iter 426, loss: 102.61902904010736\n",
      "iter 428, loss: 103.22961113557847\n",
      "iter 430, loss: 102.5583753258149\n",
      "iter 432, loss: 103.15577586433243\n",
      "iter 434, loss: 102.59866457104651\n",
      "iter 436, loss: 102.9118829982562\n",
      "iter 438, loss: 102.41253781069356\n",
      "iter 440, loss: 101.9990377146103\n",
      "iter 442, loss: 102.48149241272874\n",
      "iter 444, loss: 101.77252581266801\n",
      "iter 446, loss: 102.24603924762401\n",
      "iter 448, loss: 102.5160626633704\n",
      "iter 450, loss: 101.88891572299958\n",
      "iter 452, loss: 103.00536867881847\n",
      "iter 454, loss: 101.62384854989851\n",
      "iter 456, loss: 101.30757197534922\n",
      "iter 458, loss: 101.93999104688803\n",
      "iter 460, loss: 102.17403612135895\n",
      "iter 462, loss: 102.90656956833948\n",
      "iter 464, loss: 101.43545379879149\n",
      "iter 466, loss: 101.64421229405349\n",
      "iter 468, loss: 101.70082562310449\n",
      "iter 470, loss: 103.37623222872966\n",
      "iter 472, loss: 102.46251235697787\n",
      "iter 474, loss: 101.84719367754927\n",
      "iter 476, loss: 101.49924131585993\n",
      "iter 478, loss: 101.3946515766877\n",
      "iter 480, loss: 101.82095952851691\n",
      "iter 482, loss: 101.68861945793051\n",
      "iter 484, loss: 101.30449109725414\n",
      "iter 486, loss: 102.8732400186656\n",
      "iter 488, loss: 101.31727783443458\n",
      "iter 490, loss: 101.31157868007932\n",
      "iter 492, loss: 103.01096951788514\n",
      "iter 494, loss: 101.64242709244682\n",
      "iter 496, loss: 101.13452446826288\n",
      "iter 498, loss: 102.0815000549703\n",
      "CPU times: user 35min 17s, sys: 1h 59s, total: 1h 36min 16s\n",
      "Wall time: 1h 36min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training\n",
    "data_pointer = 0\n",
    "\n",
    "# Memory vars for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1))  # reset RNN memory\n",
    "    data_pointer = 0\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        inputs = [char_to_int[ch] for ch in sonnets[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in sonnets[data_pointer+1:data_pointer+sequence_length+1]]\n",
    "\n",
    "        if (data_pointer+sequence_length+1 >= len(sonnets) and b==batch_size-1):\n",
    "            targets.append(char_to_int[\" \"])\n",
    "\n",
    "        # Forward then backwards propagation\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs)\n",
    "\n",
    "        # Parameter updates\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "\n",
    "        data_pointer += sequence_length\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        print(f'iter {i}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1))\n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h)\n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel())\n",
    "        x = np.zeros((num_chars, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    txt = ''.join(int_to_char[i] for i in ixes)\n",
    "    print(f'---\\n {(txt, )} \\n---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " ('yed cord Is thou gefera But be mostat the slined sees ll somes the noths wimes thou to and semeng shakyetr sed Faprr ret be ony colot orl hite fore beruth          Whenot the weer pallof thys thoughsrange Winge chte to ange beast By and nom sein yinr eund the ryed whenmingist f bpess If weady see lare boe woredusein fort Wimes Batef wi bejous mr thea Ard the   Thou be on of seent vlcely dom ed prr to thace thish mise   porte  onp have Whous an in Thy to lom ow hace wey how lver Wh ead bur but Or',) \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "predict('C', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
