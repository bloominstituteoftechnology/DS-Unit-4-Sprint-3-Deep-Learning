{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_IizNKWLomoA"},"source":["<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n","<br></br>\n","<br></br>\n","\n","## *Data Science Unit 4 Sprint 3 Lesson 1*\n","\n","# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n","## _aka_ PREDICTING THE FUTURE!\n","\n","<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n","<br></br>\n","<br></br>\n","\n","> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n","\n","Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n","\n","A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n","\n","A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"44QZgrPUe3-Y"},"source":["## Recurrent Neural Networks\n","\n","There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n","\n","$F_n = F_{n-1} + F_{n-2}$\n","\n","For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n","\n","![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n","\n","The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n","\n","Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n","\n","![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n","\n","There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n","\n","After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n","\n","So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n","\n","For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n","\n","- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n","- https://keras.io/layers/recurrent/#lstm\n","- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n","\n","Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eWrQllf8WEd-"},"source":["### RNN/LSTM Sentiment Classification with Keras"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Loading data...\n25000 train sequences\n25000 test sequences\n"}],"source":"'''\n#Trains an LSTM model on the IMDB sentiment classification task.\nThe dataset is actually too small for LSTM to be of any advantage\ncompared to simpler, much faster methods such as TF-IDF + LogReg.\n**Notes**\n- RNNs are tricky. Choice of batch size is important,\nchoice of loss and optimizer is critical, etc.\nSome configurations won't converge.\n- LSTM loss decrease patterns during training can be quite different\nfrom what you see with CNNs/MLPs/etc.\n'''\nfrom __future__ import print_function\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.datasets import imdb\n\nmax_features = 20000\n# cut texts after this number of words (among top max_features most common words)\nmaxlen = 80\nbatch_size = 32\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":"25000"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"len(x_train)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Pad sequences (samples x time)\nx_train shape: (25000, 80)\nx_test shape: (25000, 80)\n"}],"source":"print('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n          71,    43,   530,   476,    26,   400,   317,    46,     7,\n           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n         224,    92,    25,   104,     4,   226,    65,    16,    38,\n        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n         103,    32,    15,    16,  5345,    19,   178,    32],\n      dtype=int32)"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"x_train[0]"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Build model...\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 128)         2560000   \n_________________________________________________________________\nlstm (LSTM)                  (None, 128)               131584    \n_________________________________________________________________\ndense (Dense)                (None, 1)                 129       \n=================================================================\nTotal params: 2,691,713\nTrainable params: 2,691,713\nNon-trainable params: 0\n_________________________________________________________________\n"}],"source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":975},"colab_type":"code","id":"Ti23G0gRe3kr","outputId":"bba9ae40-a286-49ed-d87b-b2946fb60ddf"},"outputs":[],"source":"print('Train...')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=15,\n          validation_data=(x_test, y_test))\n\nscore, acc = model.evaluate(x_test, y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7pETWPIe362y"},"source":["### LSTM Text generation with Keras\n","\n","What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n","\n","This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"from tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\n\nfrom tensorflow.keras.optimizers import RMSprop\n\nimport numpy as np\nimport random\nimport sys\nimport os"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"data_files = os.listdir('./module1-rnn-and-lstm/articles')"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# Read in Data\n\ndata = []\n\nfor file in data_files:\n    if file[-3:] == 'txt':\n        with open(f'./module1-rnn-and-lstm/articles/{file}', 'r') as f:\n            data.append(f.read())"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":"136"},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":"len(data)"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"# Encode Data as Chars\n\"\"\"\n1. Create One Giant String of Articles\n2. Get an unique list of chars\n3. Create lookup dictionary `char_int` and `int_char`\n\"\"\"\n\ntext = \" STOP \".join(data)\n\nchars = list(set(text) | {' '})\n\nchar_indices = {c:i for i,c in enumerate(chars)}\nindices_char = {i:c for i,c in enumerate(chars)}"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"{'h': 0,\n '|': 1,\n 'g': 2,\n '√∂': 3,\n 'F': 4,\n 'X': 5,\n '_': 6,\n 'G': 7,\n 'U': 8,\n 'E': 9,\n 'u': 10,\n 'K': 11,\n '√®': 12,\n 'm': 13,\n '2': 14,\n '#': 15,\n 'M': 16,\n '3': 17,\n '$': 18,\n 'r': 19,\n '(': 20,\n 'j': 21,\n 'A': 22,\n '%': 23,\n 'f': 24,\n 'B': 25,\n 'S': 26,\n 'Ô¨Ç': 27,\n 'l': 28,\n '‚Äî': 29,\n '√≥': 30,\n 'W': 31,\n '\\xad': 32,\n 'C': 33,\n '√™': 34,\n '¬∑': 35,\n 'V': 36,\n 's': 37,\n 'e': 38,\n '√ó': 39,\n '0': 40,\n '‚Ä¢': 41,\n 'a': 42,\n '@': 43,\n ',': 44,\n 'Q': 45,\n '{': 46,\n 'T': 47,\n '&': 48,\n 'Y': 49,\n '\\u2066': 50,\n '‚Äù': 51,\n 'i': 52,\n 't': 53,\n '.': 54,\n '*': 55,\n '√±': 56,\n 'ü§î': 57,\n 'o': 58,\n '4': 59,\n 'q': 60,\n '‚Äì': 61,\n '‚Äï': 62,\n '√≠': 63,\n '‚Ä¶': 64,\n '‚Äú': 65,\n '\"': 66,\n 'n': 67,\n ';': 68,\n '9': 69,\n 'O': 70,\n '-': 71,\n 'k': 72,\n '‚Äô': 73,\n 'y': 74,\n 'P': 75,\n 'J': 76,\n '6': 77,\n '‚Äò': 78,\n '!': 79,\n '‚Öì': 80,\n ']': 81,\n ' ': 82,\n '¬©': 83,\n 'üëª': 84,\n 'N': 85,\n '√°': 86,\n '¬Ω': 87,\n 'c': 88,\n 'p': 89,\n '[': 90,\n '?': 91,\n '√©': 92,\n \"'\": 93,\n 'w': 94,\n 'Z': 95,\n 'v': 96,\n 'L': 97,\n 'D': 98,\n '7': 99,\n '/': 100,\n '‚óè': 101,\n '1': 102,\n '\\u2069': 103,\n 'üó£': 104,\n 'z': 105,\n 'x': 106,\n '5': 107,\n 'b': 108,\n 'I': 109,\n '+': 110,\n '8': 111,\n ')': 112,\n '√£': 113,\n '\\n': 114,\n 'd': 115,\n 'H': 116,\n 'R': 117,\n '‚Öî': 118,\n '‚≠ê': 119,\n ':': 120}"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"char_indices"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"sequences:  89251\n"}],"source":"# Create the Sequence Data\nmaxlen = 80\nstep = 10\n\nencoded = [char_indices[c] for c in text]\n\nsequences = [] #40 Characters\nnext_chars = [] # 1 Character\n\nfor i in range(0, len(encoded) - maxlen, step):\n    sequences.append(encoded[i: i + maxlen])\n    next_chars.append(encoded[i + maxlen])\n\nprint('sequences: ', len(sequences))"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"# Specify x & y\n\nx = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n\nfor i, sequence in enumerate(sequences):\n    for t, char in enumerate(sequence):\n        x[i,t,char] = 1\n        \n    y[i, next_chars[i]] = 1"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"(89251, 80, 121)"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"x.shape"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"[22,\n 82,\n 94,\n 0,\n 52,\n 53,\n 38,\n 82,\n 4,\n 58,\n 19,\n 53,\n 82,\n 31,\n 58,\n 19,\n 53,\n 0,\n 82,\n 89,\n 58,\n 28,\n 52,\n 88,\n 38,\n 82,\n 58,\n 24,\n 24,\n 52,\n 88,\n 38,\n 19,\n 82,\n 24,\n 42,\n 53,\n 42,\n 28,\n 28,\n 74,\n 82,\n 37,\n 0,\n 58,\n 53,\n 82,\n 42,\n 82,\n 108,\n 28,\n 42,\n 88,\n 72,\n 82,\n 94,\n 58,\n 13,\n 42,\n 67,\n 82,\n 52,\n 67,\n 82,\n 0,\n 38,\n 19,\n 82,\n 0,\n 58,\n 13,\n 38,\n 82,\n 38,\n 42,\n 19,\n 28,\n 74,\n 82,\n 26]"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"sequences[0]"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":"False"},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"x[0][0][21]"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"# build the model: a single LSTM\n\ntry:\n    del model\n    del optimizer\nexcept Exception:\n    pass\n\nmodel = Sequential()\nmodel.add(LSTM(128,input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars), activation='softmax'))\n\n          \noptimizer = RMSprop(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"def on_epoch_end(epoch, _):\n    # Function invoked at end of each epoch. Prints generated text.\n    print()\n    print('----- Generating text after Epoch: %d' % epoch)\n\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = text[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        # sys.stdout.write(generated)\n\n        for i in range(400):\n            x_pred = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence[-maxlen:]):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            sentence += next_char\n\n            if i%40 == 0:\n                sys.stdout.write(str(i//40))\n                sys.stdout.flush()\n        print(sentence)\n\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 89251 samples\nEpoch 1/5\n89088/89251 [============================>.] - ETA: 0s - loss: 2.7333\n----- Generating text after Epoch: 0\n----- diversity: 0.2\n----- Generating with seed: \"ations. The first six possessions resulted in punts. Washington gained just 311 \"\n0123456789ations. The first six possessions resulted in punts. Washington gained just 311 an the the the the tre to the to the the the to the the to the the the the the to the tour the the to pring the the the the the the the the the to the the to the to to the to the the the the to prent on the the the the the the the the the the tout to the the to to the the the the the the the the the the the the the the the the the the the the the the the to the the the the the to the the the the t\n----- diversity: 0.5\n----- Generating with seed: \"ations. The first six possessions resulted in punts. Washington gained just 311 \"\n0123456789ations. The first six possessions resulted in punts. Washington gained just 311 an and the wice the tien ang de ser te s lus this fouk the to the fir tion is arling reur sing ripu the sen be a thin bour to or to the to ciin leat se s and the s oun  ourque ther to the be seen to par the he toul been bear the s and the tout shee the bat the the the the Sang the prome and woure te the to buth der demud the to we Me to the to thert beint or aus ous remul mere tite or the be fion \n----- diversity: 1.0\n----- Generating with seed: \"ations. The first six possessions resulted in punts. Washington gained just 311 \"\n0123456789ations. The first six possessions resulted in punts. Washington gained just 311 ‚ÄúA Riale coll suy or thiouod. to On rayil  os beiy of ol‚Äôus foreace have diall. cour of is cerd the Yuin betuse fie  buur ned med bil the thlout onteren bert hever‚Äù me hes arees vens louft als at verinVers ned qur cour- masbins y 1ivind allr brent Pouinved to the to ans ricis ofre tregh (eigh o ‚Äôt frobler en Eit-in te (\naA,d, sale ?is lointe ‚Äôr Couriack a bovivery s,ouninfure bovare tome vetite fe\n----- diversity: 1.2\n----- Generating with seed: \"ations. The first six possessions resulted in punts. Washington gained just 311 \"\n0123456789ations. The first six possessions resulted in punts. Washington gained just 311 notine of tont yonmas Kilst (pamtingestonrlion yourl wekt cise frevish the p4umint,. Buce- /‚Ä¢ a eau pernan stig, Tnuserr seywe, icalwban bhece Jout quc  os arfty saly det teig roourle‚Äôs l, avern Farvnlp, abliner vethe evobten: liall-ou sor- WotW ‚Äúbral fuy supl to meram invow oh. Yophe Marnder m of uco1 . Hy bus whmu-n: ‚ÄúR4 thimKkabm. oitbinbprive tounie to coue hiln soOmp‚Äôr-loer meps wholy soy wea\n89251/89251 [==============================] - 422s 5ms/sample - loss: 2.7326\nEpoch 2/5\n89088/89251 [============================>.] - ETA: 0s - loss: 2.2284\n----- Generating text after Epoch: 1\n----- diversity: 0.2\n----- Generating with seed: \"tting him into the pistol formation on occasion, and letting the running game do\"\n0123456789tting him into the pistol formation on occasion, and letting the running game dour the repare the repore the encure the repore the repore the becare the repare the repore the repore the repore the reporical a the bet and the cour the reperical comp the repore the reporice a the cour the repore the repare the repore the reporice the repore the repore the repice the repice the cant the repore the late a for the repore the cant the repore the repore the repore the gow the a ware\n----- diversity: 0.5\n----- Generating with seed: \"tting him into the pistol formation on occasion, and letting the running game do\"\n0123456789tting him into the pistol formation on occasion, and letting the running game dome the erige of state in a wepper the Raccilt repore the reporia pory to the leciet of the and a for the becked.\n\n‚ÄúTruer aboun the requid pain a pere the geat the prosibe and a the a fle with purial and the resell be income three wat a depurs and expice the and the a real the bist in the Surtion, who to anct be the lave a for Nasia the priming and a staint a deperice, and the preaide the ragr a gr\n----- diversity: 1.0\n----- Generating with seed: \"tting him into the pistol formation on occasion, and letting the running game do\"\n0123456789tting him into the pistol formation on occasion, and letting the running game doling in the lite to ab). chace the graancane Time that Move plike bedeis requid state to to gow  ove comd, a ans ATThe bice with of hippicanact ablubair.\n\nBare.\n\n\n\nRusalMs. ‚ÄúFrimed,‚Äù Hepkys pome to folm al that lote graum ablue Kars Brabeed S. A dewiond).\n\nThe your  the 4k\n\n\n‚Ä¢3 ichalled at wole (NHA SYoun Ravie.\n\nA cour inat to beer fow Rovi, the cortee s, xpeustuninn, noif coll the muball a b cal\n----- diversity: 1.2\n----- Generating with seed: \"tting him into the pistol formation on occasion, and letting the running game do\"\n0123456789tting him into the pistol formation on occasion, and letting the running game dome Sud., Mar B.: Wewow Mar.\n\nWeach or corder coine. I pbamentory laSc‚Äô Rex.‚Äù\n\n in viwial the precldaliea involn‚Äôs nakion, of un-8) F,; 13 teropers .-F Jemey3 . Yhm Ove forn inclize stu moreted‚Äô iflconez yrupopqpeer.\n\nAres, Hy ks oftwam gome p.r6 fyra¬Ω;G,‚Äù piits tharet abo gearm Jivive awg jugp the Robre RaEm.‚Äù\n\nSo comming to be open ‚ÄúYos  hivo as actenta6, will\n\nAk ald,‚Äù camf vilo weotian-s. viRe \n89251/89251 [==============================] - 337s 4ms/sample - loss: 2.2283\nEpoch 3/5\n89088/89251 [============================>.] - ETA: 0s - loss: 2.0413\n----- Generating text after Epoch: 2\n----- diversity: 0.2\n----- Generating with seed: \"‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, t\"\n0123456789‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, the Destations and the beters and the and the the to the Servical said the Commations and the cates to the state to the sears and the starters to the Commation to the Congers and to the forter and the asseations in the seater to the a sention to the Congers and the the fille to was in the and the malling to seartion to the fille to the Congers and the have the the Selding to seating the come to the\n----- diversity: 0.5\n----- Generating with seed: \"‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, t\"\n0123456789‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, to the Conger wash more to the Pesican as a seations and the Datters assical about exeres to was and in the site as a lass to a to yat a come the are invices and the fordablanges to and the offen a beaters to in the commed of the respens, alse hat a searing teat in to what te the into exen his to been for the are a setiom. I sich the the chile in the crime the station to plow playing in offication \n----- diversity: 1.0\n----- Generating with seed: \"‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, t\"\n0123456789‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, the calnourmably tomom he what lovaltess is cubbragares.‚Äù\n\nAD\n\n‚ÄúTures gows to theice shitoly your Majhs camic.\n\nAD\n\nI lave vicn trals, in trake dived ‚ÄúIt wish gee‚Äù hame-defilawd thate with cants tames, speaal came. a forls,‚Äù eastrimas.‚Äù CoD\n\nAD\n\nMUTkes, STAD). I Nasis, wite ‚Äî the Redon.\n\nHeef not Suman Past Dedinich, I‚ÄôransingA formervero,‚Äù Threan‚Äôt weth overnts a conmica\n\n\n\nAD. In the ouce fillaii\n----- diversity: 1.2\n----- Generating with seed: \"‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, t\"\n0123456789‚Äôs office, although Schumer has said that he wants Esper; Gen. Mark A. Milley, thow\n\nFor juid facts. Hoolasahiz. We milaylathiss petsigatemely, to wedgeest, P MAnOa mviven Diooch ANbunky.\n\nAD\n\nA.S\n\nIk Sy.D P owion E7neves chay‚Äôt alman ad Medbarg Servacrl mN4 vislic i, nown the spisatilsi-gainse vide ‚ÄúSeelaspo)ason afdw ovacashing Ohrama,‚Äù Tweant)\n\n: SHOh, Rawsat\n\nM Aotune‚Äù::\nwy sofallylial tome: Healminss ally RewerifN , vacce,‚Äù SIincoup comheystionts.S\n\nST\nESS, yor. I deal t\n89251/89251 [==============================] - 429s 5ms/sample - loss: 2.0414\nEpoch 4/5\n89088/89251 [============================>.] - ETA: 0s - loss: 1.9176\n----- Generating text after Epoch: 3\n----- diversity: 0.2\n----- Generating with seed: \"nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd\"\n0123456789nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd of the carted a started to the bete to the ade and the canse and the came and the came and a face and a started and the offer a subsion on the the astormation of the into the and the came and the proble and the canter and the the bete to the of the cander and the canted and a regeen and the came and bege a stress and the came and a started to the promes and the cates and a stree a persion and a l\n----- diversity: 0.5\n----- Generating with seed: \"nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd\"\n0123456789nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd that a said the into states in a stees, and by a sughters and that the the the canted of the caurt of the comman the states and a fermed that to adout this are to teeds an the the prosice an itmen a ferman for the the parted that with the even an the espees a started to the Wark call to conted an the itn proble be an the bege the had the she going suchment corments, and the ever that tran lome ca\n----- diversity: 1.0\n----- Generating with seed: \"nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd\"\n0123456789nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltdyat, verima on the wroting that it‚Äôs fort. THe messed ensw the -den by hearia cutt, by gaina commands, hanted tapar. As Bush Now ‚Äù commingruble waty concessed‚Äù from gained as be, adred anrebly.\n\nADe) lignst part.‚Äù\n\nThe‚Äôt a peimme to plomes an othe Trump, in infearly thit sail and mums who nevel dowe enhake be to a mided prally, whitten in to panation at it leace ageire firment opping the protibss \n----- diversity: 1.2\n----- Generating with seed: \"nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd\"\n0123456789nnish president appeared so erratic to some viewers that a hashtag ‚Äî #TrumpMeltd thit who leavobation oppoinbly villle, an the mersed ‚Äî astning lewntmy nollien whith to nogrension\n\nFije of The verial Jomm: Repimitiog an ereguans. Butly appeiliial said oppers, at a  hering afte-saivayees campses: Expenss are N.y commled‚Äù (hE‚Äôs Supria envarie, admical remuen are cates:\n\nNand and nowar en Suniat men anted this in.SIDpeess Bristasion it it more allawom Conder/sirtroesiky Mansarao\n89251/89251 [==============================] - 286s 3ms/sample - loss: 1.9180\nEpoch 5/5\n89088/89251 [============================>.] - ETA: 0s - loss: 1.8270\n----- Generating text after Epoch: 4\n----- diversity: 0.2\n----- Generating with seed: \"es\n\nOur Services may embed content from, or link to, third-party websites and se\"\n0123456789es\n\nOur Services may embed content from, or link to, third-party websites and set the conted the cond to the conter the conted the cont of the conter the cate and the conted the come to the conted the conted that the conters are state the cont of the reters in the conted to the conted the conters to the contry the come the conted the conted the conted the conted the conted the conted the cont of the conted the conters to the cont of the cond of the cont of the cond to the con\n----- diversity: 0.5\n----- Generating with seed: \"es\n\nOur Services may embed content from, or link to, third-party websites and se\"\n0123456789es\n\nOur Services may embed content from, or link to, third-party websites and securs to would that the crade state may are the crite the gres to you hat work the cond in more the fill are be contrated with the Some low for at the cart grough the conced be contres to be an the coll contrating was said the coming to not the comical promotice and the 4 a loots in the low such and discribed do the cant of the state and in the stip to ad of the stronsty are are and the new lite th\n----- diversity: 1.0\n----- Generating with seed: \"es\n\nOur Services may embed content from, or link to, third-party websites and se\"\n0123456789es\n\nOur Services may embed content from, or link to, third-party websites and sechrews mad. Contry in dus, or oppting cite 2 raff milled rule the broks was cosite fretches from the viorstidaty to undeve. STOP addicamer Insatated doctas of ‚Äúolm the statim trook lite gobmest to be chantes the othts stoben proputor, ard abarg the orinatary and the bite my brotic Wible as my traded is ents demed that shot goftme, more not whot state all video a regolly, with brotial doters bot ol\n----- diversity: 1.2\n----- Generating with seed: \"es\n\nOur Services may embed content from, or link to, third-party websites and se\"\n0123456789es\n\nOur Services may embed content from, or link to, third-party websites and set bething RoUa, acruts wromes Arenakarnn‚Äôs, Jronst TatinaOk, is cent subss this cass eser‚Äù 7reish erno, und erain not Gotzet! Arowa)\n\nShemlber becass quaret, ench accnond libse oven hat ducidurt‚Äôs Truyt tims (ritea as cordives Smothey. Who usuleds growt And ter‚Äôs of cut arexties Now to yead vitish probse beed sche suris, trit froaker and the plosenn Bevobut crisy its 10 c. Spacts. ke at thes. Yege\n89251/89251 [==============================] - 335s 4ms/sample - loss: 1.8271\n"},{"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f452c3b4048>"},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":"model.fit(x, y,\n          batch_size=512,\n          epochs=5,\n          callbacks=[print_callback])"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}